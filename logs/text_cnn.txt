Running optimization with 20 trials
Starting TextCNN hyperparameter optimization with Optuna (20 trials)
Using device: mps
System has 16 CPU cores, using 4 workers
Vocab size: 50000

================================================================================
Starting Optuna hyperparameter optimization...
wandb: Currently logged in as: devmarkpro (devmarkpro-org) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_053322-cjmy2hpn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run optuna_study
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/cjmy2hpn
[I 2025-10-06 05:33:22,911] A new study created in memory with name: textcnn_optimization
wandb: wandb.init() called while a run is active and reinit is set to 'default', so returning the previous run.
wandb: Finishing previous runs because reinit is set to True.
wandb: üöÄ View run optuna_study at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/cjmy2hpn
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_053322-cjmy2hpn/logs
wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_053322-dof46kz8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_000
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/dof46kz8

Train Epoch 1 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 1.5819 (avg: 1.6467) | Acc: 0.2812 (avg: 0.2494) | LR: 1.51e-05
  Batch  100/1500 (  6.7%) | Loss: 1.5733 (avg: 1.6209) | Acc: 0.3281 (avg: 0.2575) | LR: 1.52e-05
  Batch  150/1500 ( 10.0%) | Loss: 1.5335 (avg: 1.6166) | Acc: 0.3438 (avg: 0.2578) | LR: 1.53e-05
  Batch  200/1500 ( 13.3%) | Loss: 1.4857 (avg: 1.6079) | Acc: 0.2969 (avg: 0.2610) | LR: 1.55e-05
  Batch  250/1500 ( 16.7%) | Loss: 1.6163 (avg: 1.6009) | Acc: 0.2812 (avg: 0.2619) | LR: 1.58e-05
  Batch  300/1500 ( 20.0%) | Loss: 1.4820 (avg: 1.5943) | Acc: 0.3125 (avg: 0.2639) | LR: 1.61e-05
  Batch  350/1500 ( 23.3%) | Loss: 1.4485 (avg: 1.5823) | Acc: 0.2656 (avg: 0.2698) | LR: 1.64e-05
  Batch  400/1500 ( 26.7%) | Loss: 1.5325 (avg: 1.5776) | Acc: 0.4062 (avg: 0.2725) | LR: 1.68e-05
  Batch  450/1500 ( 30.0%) | Loss: 1.5134 (avg: 1.5723) | Acc: 0.2656 (avg: 0.2742) | LR: 1.73e-05
  Batch  500/1500 ( 33.3%) | Loss: 1.5539 (avg: 1.5654) | Acc: 0.3125 (avg: 0.2768) | LR: 1.78e-05
  Batch  550/1500 ( 36.7%) | Loss: 1.6253 (avg: 1.5567) | Acc: 0.2344 (avg: 0.2804) | LR: 1.84e-05
  Batch  600/1500 ( 40.0%) | Loss: 1.4964 (avg: 1.5497) | Acc: 0.4375 (avg: 0.2842) | LR: 1.90e-05
  Batch  650/1500 ( 43.3%) | Loss: 1.6490 (avg: 1.5429) | Acc: 0.2188 (avg: 0.2871) | LR: 1.97e-05
  Batch  700/1500 ( 46.7%) | Loss: 1.4785 (avg: 1.5355) | Acc: 0.3125 (avg: 0.2900) | LR: 2.04e-05
  Batch  750/1500 ( 50.0%) | Loss: 1.3176 (avg: 1.5280) | Acc: 0.4062 (avg: 0.2938) | LR: 2.12e-05
  Batch  800/1500 ( 53.3%) | Loss: 1.3976 (avg: 1.5206) | Acc: 0.3281 (avg: 0.2972) | LR: 2.21e-05
  Batch  850/1500 ( 56.7%) | Loss: 1.2263 (avg: 1.5141) | Acc: 0.4219 (avg: 0.3003) | LR: 2.30e-05
  Batch  900/1500 ( 60.0%) | Loss: 1.4433 (avg: 1.5070) | Acc: 0.3125 (avg: 0.3040) | LR: 2.39e-05
  Batch  950/1500 ( 63.3%) | Loss: 1.4583 (avg: 1.5012) | Acc: 0.2969 (avg: 0.3073) | LR: 2.49e-05
  Batch 1000/1500 ( 66.7%) | Loss: 1.4737 (avg: 1.4940) | Acc: 0.3125 (avg: 0.3115) | LR: 2.60e-05
  Batch 1050/1500 ( 70.0%) | Loss: 1.1894 (avg: 1.4869) | Acc: 0.5625 (avg: 0.3151) | LR: 2.71e-05
  Batch 1100/1500 ( 73.3%) | Loss: 1.3065 (avg: 1.4802) | Acc: 0.4375 (avg: 0.3190) | LR: 2.83e-05
  Batch 1150/1500 ( 76.7%) | Loss: 1.2785 (avg: 1.4733) | Acc: 0.4375 (avg: 0.3227) | LR: 2.95e-05
  Batch 1200/1500 ( 80.0%) | Loss: 1.2780 (avg: 1.4669) | Acc: 0.4219 (avg: 0.3262) | LR: 3.07e-05
  Batch 1250/1500 ( 83.3%) | Loss: 1.2450 (avg: 1.4592) | Acc: 0.4844 (avg: 0.3306) | LR: 3.20e-05
  Batch 1300/1500 ( 86.7%) | Loss: 1.2967 (avg: 1.4515) | Acc: 0.3594 (avg: 0.3354) | LR: 3.34e-05
  Batch 1350/1500 ( 90.0%) | Loss: 1.3462 (avg: 1.4443) | Acc: 0.4531 (avg: 0.3399) | LR: 3.48e-05
  Batch 1400/1500 ( 93.3%) | Loss: 1.3320 (avg: 1.4365) | Acc: 0.4219 (avg: 0.3447) | LR: 3.63e-05
  Batch 1450/1500 ( 96.7%) | Loss: 1.1928 (avg: 1.4292) | Acc: 0.4688 (avg: 0.3491) | LR: 3.78e-05
  Batch 1500/1500 (100.0%) | Loss: 1.1649 (avg: 1.4207) | Acc: 0.5625 (avg: 0.3543) | LR: 3.93e-05

Train Summary:
  Final Loss: 1.4207 | Final Acc: 0.3543
  Final LR: 3.93e-05
  Loss std: 0.1510 | Acc std: 0.0927

Val Epoch 1 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 1.0182 | Final Acc: 0.7463
  Loss std: 0.0402 | Acc std: 0.0523
Epoch 01/20 | train 1.4207/0.3543 | val 1.0182/0.7463

Train Epoch 2 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 1.1322 (avg: 1.1551) | Acc: 0.5000 (avg: 0.5178) | LR: 4.09e-05
  Batch  100/1500 (  6.7%) | Loss: 1.1224 (avg: 1.1538) | Acc: 0.5469 (avg: 0.5217) | LR: 4.26e-05
  Batch  150/1500 ( 10.0%) | Loss: 1.2239 (avg: 1.1613) | Acc: 0.4844 (avg: 0.5143) | LR: 4.43e-05
  Batch  200/1500 ( 13.3%) | Loss: 1.0587 (avg: 1.1539) | Acc: 0.6094 (avg: 0.5202) | LR: 4.60e-05
  Batch  250/1500 ( 16.7%) | Loss: 1.1366 (avg: 1.1463) | Acc: 0.5156 (avg: 0.5258) | LR: 4.78e-05
  Batch  300/1500 ( 20.0%) | Loss: 1.0877 (avg: 1.1394) | Acc: 0.5312 (avg: 0.5291) | LR: 4.96e-05
  Batch  350/1500 ( 23.3%) | Loss: 0.8946 (avg: 1.1308) | Acc: 0.7031 (avg: 0.5350) | LR: 5.15e-05
  Batch  400/1500 ( 26.7%) | Loss: 1.0567 (avg: 1.1190) | Acc: 0.5781 (avg: 0.5430) | LR: 5.34e-05
  Batch  450/1500 ( 30.0%) | Loss: 1.0280 (avg: 1.1118) | Acc: 0.6250 (avg: 0.5467) | LR: 5.54e-05
  Batch  500/1500 ( 33.3%) | Loss: 0.9612 (avg: 1.1040) | Acc: 0.6562 (avg: 0.5522) | LR: 5.74e-05
  Batch  550/1500 ( 36.7%) | Loss: 1.0801 (avg: 1.0951) | Acc: 0.5625 (avg: 0.5576) | LR: 5.95e-05
  Batch  600/1500 ( 40.0%) | Loss: 1.0486 (avg: 1.0840) | Acc: 0.6094 (avg: 0.5646) | LR: 6.15e-05
  Batch  650/1500 ( 43.3%) | Loss: 0.9357 (avg: 1.0754) | Acc: 0.6562 (avg: 0.5695) | LR: 6.37e-05
  Batch  700/1500 ( 46.7%) | Loss: 1.0015 (avg: 1.0671) | Acc: 0.5312 (avg: 0.5747) | LR: 6.59e-05
  Batch  750/1500 ( 50.0%) | Loss: 0.8485 (avg: 1.0585) | Acc: 0.6719 (avg: 0.5792) | LR: 6.81e-05
  Batch  800/1500 ( 53.3%) | Loss: 0.8961 (avg: 1.0511) | Acc: 0.6562 (avg: 0.5837) | LR: 7.03e-05
  Batch  850/1500 ( 56.7%) | Loss: 0.9391 (avg: 1.0432) | Acc: 0.6875 (avg: 0.5878) | LR: 7.26e-05
  Batch  900/1500 ( 60.0%) | Loss: 0.8530 (avg: 1.0355) | Acc: 0.6875 (avg: 0.5923) | LR: 7.49e-05
  Batch  950/1500 ( 63.3%) | Loss: 0.8318 (avg: 1.0261) | Acc: 0.7188 (avg: 0.5976) | LR: 7.73e-05
  Batch 1000/1500 ( 66.7%) | Loss: 0.7331 (avg: 1.0177) | Acc: 0.7344 (avg: 0.6022) | LR: 7.97e-05
  Batch 1050/1500 ( 70.0%) | Loss: 1.0299 (avg: 1.0098) | Acc: 0.6250 (avg: 0.6076) | LR: 8.21e-05
  Batch 1100/1500 ( 73.3%) | Loss: 0.7335 (avg: 1.0025) | Acc: 0.7500 (avg: 0.6120) | LR: 8.46e-05
  Batch 1150/1500 ( 76.7%) | Loss: 0.9228 (avg: 0.9941) | Acc: 0.6094 (avg: 0.6169) | LR: 8.71e-05
  Batch 1200/1500 ( 80.0%) | Loss: 1.0132 (avg: 0.9875) | Acc: 0.5938 (avg: 0.6209) | LR: 8.97e-05
  Batch 1250/1500 ( 83.3%) | Loss: 0.8294 (avg: 0.9810) | Acc: 0.6875 (avg: 0.6246) | LR: 9.22e-05
  Batch 1300/1500 ( 86.7%) | Loss: 0.8328 (avg: 0.9744) | Acc: 0.7344 (avg: 0.6283) | LR: 9.48e-05
  Batch 1350/1500 ( 90.0%) | Loss: 0.8050 (avg: 0.9678) | Acc: 0.6406 (avg: 0.6321) | LR: 9.75e-05
  Batch 1400/1500 ( 93.3%) | Loss: 0.7018 (avg: 0.9622) | Acc: 0.7812 (avg: 0.6354) | LR: 1.00e-04
  Batch 1450/1500 ( 96.7%) | Loss: 0.9608 (avg: 0.9558) | Acc: 0.6875 (avg: 0.6393) | LR: 1.03e-04
  Batch 1500/1500 (100.0%) | Loss: 0.8119 (avg: 0.9491) | Acc: 0.7344 (avg: 0.6429) | LR: 1.06e-04

Train Summary:
  Final Loss: 0.9491 | Final Acc: 0.6429
  Final LR: 1.05e-04
  Loss std: 0.1500 | Acc std: 0.0934

Val Epoch 2 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.6409 | Final Acc: 0.8307
  Loss std: 0.0689 | Acc std: 0.0460
Epoch 02/20 | train 0.9491/0.6429 | val 0.6409/0.8307

Train Epoch 3 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.8474 (avg: 0.7641) | Acc: 0.7656 (avg: 0.7516) | LR: 1.08e-04
  Batch  100/1500 (  6.7%) | Loss: 0.6570 (avg: 0.7425) | Acc: 0.7656 (avg: 0.7606) | LR: 1.11e-04
  Batch  150/1500 ( 10.0%) | Loss: 0.6766 (avg: 0.7371) | Acc: 0.8125 (avg: 0.7627) | LR: 1.14e-04
  Batch  200/1500 ( 13.3%) | Loss: 0.7028 (avg: 0.7319) | Acc: 0.7500 (avg: 0.7648) | LR: 1.17e-04
  Batch  250/1500 ( 16.7%) | Loss: 0.6651 (avg: 0.7310) | Acc: 0.7969 (avg: 0.7649) | LR: 1.20e-04
  Batch  300/1500 ( 20.0%) | Loss: 0.7263 (avg: 0.7275) | Acc: 0.7969 (avg: 0.7671) | LR: 1.22e-04
  Batch  350/1500 ( 23.3%) | Loss: 0.7396 (avg: 0.7254) | Acc: 0.8125 (avg: 0.7683) | LR: 1.25e-04
  Batch  400/1500 ( 26.7%) | Loss: 0.7869 (avg: 0.7223) | Acc: 0.7344 (avg: 0.7702) | LR: 1.28e-04
  Batch  450/1500 ( 30.0%) | Loss: 0.6512 (avg: 0.7188) | Acc: 0.8438 (avg: 0.7721) | LR: 1.31e-04
  Batch  500/1500 ( 33.3%) | Loss: 0.7005 (avg: 0.7148) | Acc: 0.7500 (avg: 0.7741) | LR: 1.34e-04
  Batch  550/1500 ( 36.7%) | Loss: 0.6640 (avg: 0.7132) | Acc: 0.7500 (avg: 0.7746) | LR: 1.37e-04
  Batch  600/1500 ( 40.0%) | Loss: 0.6657 (avg: 0.7110) | Acc: 0.7188 (avg: 0.7759) | LR: 1.40e-04
  Batch  650/1500 ( 43.3%) | Loss: 0.7181 (avg: 0.7080) | Acc: 0.7656 (avg: 0.7779) | LR: 1.43e-04
  Batch  700/1500 ( 46.7%) | Loss: 0.7283 (avg: 0.7052) | Acc: 0.7500 (avg: 0.7790) | LR: 1.46e-04
  Batch  750/1500 ( 50.0%) | Loss: 0.5635 (avg: 0.7032) | Acc: 0.8906 (avg: 0.7801) | LR: 1.49e-04
  Batch  800/1500 ( 53.3%) | Loss: 0.6633 (avg: 0.7014) | Acc: 0.7969 (avg: 0.7814) | LR: 1.52e-04
  Batch  850/1500 ( 56.7%) | Loss: 0.5517 (avg: 0.6994) | Acc: 0.8750 (avg: 0.7823) | LR: 1.55e-04
  Batch  900/1500 ( 60.0%) | Loss: 0.7307 (avg: 0.6981) | Acc: 0.7969 (avg: 0.7836) | LR: 1.58e-04
  Batch  950/1500 ( 63.3%) | Loss: 0.6571 (avg: 0.6953) | Acc: 0.8438 (avg: 0.7852) | LR: 1.61e-04
  Batch 1000/1500 ( 66.7%) | Loss: 0.6908 (avg: 0.6935) | Acc: 0.7812 (avg: 0.7863) | LR: 1.65e-04
  Batch 1050/1500 ( 70.0%) | Loss: 0.5600 (avg: 0.6902) | Acc: 0.8750 (avg: 0.7877) | LR: 1.68e-04
  Batch 1100/1500 ( 73.3%) | Loss: 0.5891 (avg: 0.6884) | Acc: 0.8438 (avg: 0.7887) | LR: 1.71e-04
  Batch 1150/1500 ( 76.7%) | Loss: 0.6696 (avg: 0.6866) | Acc: 0.7656 (avg: 0.7897) | LR: 1.74e-04
  Batch 1200/1500 ( 80.0%) | Loss: 0.4886 (avg: 0.6842) | Acc: 0.8750 (avg: 0.7911) | LR: 1.77e-04
  Batch 1250/1500 ( 83.3%) | Loss: 0.7207 (avg: 0.6822) | Acc: 0.7969 (avg: 0.7922) | LR: 1.80e-04
  Batch 1300/1500 ( 86.7%) | Loss: 0.6097 (avg: 0.6810) | Acc: 0.8125 (avg: 0.7931) | LR: 1.83e-04
  Batch 1350/1500 ( 90.0%) | Loss: 0.6627 (avg: 0.6787) | Acc: 0.7500 (avg: 0.7944) | LR: 1.87e-04
  Batch 1400/1500 ( 93.3%) | Loss: 0.7113 (avg: 0.6772) | Acc: 0.7812 (avg: 0.7950) | LR: 1.90e-04
  Batch 1450/1500 ( 96.7%) | Loss: 0.6225 (avg: 0.6754) | Acc: 0.8438 (avg: 0.7960) | LR: 1.93e-04
  Batch 1500/1500 (100.0%) | Loss: 0.7647 (avg: 0.6733) | Acc: 0.7500 (avg: 0.7971) | LR: 1.96e-04

Train Summary:
  Final Loss: 0.6733 | Final Acc: 0.7971
  Final LR: 1.96e-04
  Loss std: 0.0968 | Acc std: 0.0545

Val Epoch 3 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5362 | Final Acc: 0.8725
  Loss std: 0.0729 | Acc std: 0.0423
Epoch 03/20 | train 0.6733/0.7971 | val 0.5362/0.8725

Train Epoch 4 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.7225 (avg: 0.5841) | Acc: 0.7969 (avg: 0.8469) | LR: 1.99e-04
  Batch  100/1500 (  6.7%) | Loss: 0.5970 (avg: 0.5822) | Acc: 0.7969 (avg: 0.8431) | LR: 2.02e-04
  Batch  150/1500 ( 10.0%) | Loss: 0.5585 (avg: 0.5795) | Acc: 0.8750 (avg: 0.8450) | LR: 2.05e-04
  Batch  200/1500 ( 13.3%) | Loss: 0.7620 (avg: 0.5803) | Acc: 0.8125 (avg: 0.8439) | LR: 2.09e-04
  Batch  250/1500 ( 16.7%) | Loss: 0.5465 (avg: 0.5810) | Acc: 0.8750 (avg: 0.8440) | LR: 2.12e-04
  Batch  300/1500 ( 20.0%) | Loss: 0.5246 (avg: 0.5806) | Acc: 0.8281 (avg: 0.8447) | LR: 2.15e-04
  Batch  350/1500 ( 23.3%) | Loss: 0.6687 (avg: 0.5804) | Acc: 0.7656 (avg: 0.8443) | LR: 2.18e-04
  Batch  400/1500 ( 26.7%) | Loss: 0.6219 (avg: 0.5790) | Acc: 0.8125 (avg: 0.8445) | LR: 2.21e-04
  Batch  450/1500 ( 30.0%) | Loss: 0.5395 (avg: 0.5798) | Acc: 0.8281 (avg: 0.8450) | LR: 2.24e-04
  Batch  500/1500 ( 33.3%) | Loss: 0.6055 (avg: 0.5796) | Acc: 0.8906 (avg: 0.8448) | LR: 2.27e-04
  Batch  550/1500 ( 36.7%) | Loss: 0.5003 (avg: 0.5787) | Acc: 0.8594 (avg: 0.8454) | LR: 2.31e-04
  Batch  600/1500 ( 40.0%) | Loss: 0.4736 (avg: 0.5769) | Acc: 0.8750 (avg: 0.8467) | LR: 2.34e-04
  Batch  650/1500 ( 43.3%) | Loss: 0.5751 (avg: 0.5763) | Acc: 0.7969 (avg: 0.8461) | LR: 2.37e-04
  Batch  700/1500 ( 46.7%) | Loss: 0.5944 (avg: 0.5756) | Acc: 0.8125 (avg: 0.8465) | LR: 2.40e-04
  Batch  750/1500 ( 50.0%) | Loss: 0.6799 (avg: 0.5751) | Acc: 0.8125 (avg: 0.8470) | LR: 2.43e-04
  Batch  800/1500 ( 53.3%) | Loss: 0.7362 (avg: 0.5747) | Acc: 0.8281 (avg: 0.8474) | LR: 2.46e-04
  Batch  850/1500 ( 56.7%) | Loss: 0.5208 (avg: 0.5740) | Acc: 0.8906 (avg: 0.8480) | LR: 2.49e-04
  Batch  900/1500 ( 60.0%) | Loss: 0.5665 (avg: 0.5736) | Acc: 0.8281 (avg: 0.8480) | LR: 2.52e-04
  Batch  950/1500 ( 63.3%) | Loss: 0.3817 (avg: 0.5731) | Acc: 0.9375 (avg: 0.8485) | LR: 2.55e-04
  Batch 1000/1500 ( 66.7%) | Loss: 0.6180 (avg: 0.5737) | Acc: 0.7812 (avg: 0.8485) | LR: 2.58e-04
  Batch 1050/1500 ( 70.0%) | Loss: 0.5218 (avg: 0.5737) | Acc: 0.8594 (avg: 0.8483) | LR: 2.61e-04
  Batch 1100/1500 ( 73.3%) | Loss: 0.6098 (avg: 0.5730) | Acc: 0.8125 (avg: 0.8485) | LR: 2.64e-04
  Batch 1150/1500 ( 76.7%) | Loss: 0.5974 (avg: 0.5730) | Acc: 0.8125 (avg: 0.8485) | LR: 2.67e-04
  Batch 1200/1500 ( 80.0%) | Loss: 0.5919 (avg: 0.5726) | Acc: 0.8125 (avg: 0.8488) | LR: 2.70e-04
  Batch 1250/1500 ( 83.3%) | Loss: 0.6279 (avg: 0.5719) | Acc: 0.8281 (avg: 0.8496) | LR: 2.72e-04
  Batch 1300/1500 ( 86.7%) | Loss: 0.7151 (avg: 0.5709) | Acc: 0.7812 (avg: 0.8500) | LR: 2.75e-04
  Batch 1350/1500 ( 90.0%) | Loss: 0.7027 (avg: 0.5705) | Acc: 0.8281 (avg: 0.8502) | LR: 2.78e-04
  Batch 1400/1500 ( 93.3%) | Loss: 0.6400 (avg: 0.5705) | Acc: 0.8281 (avg: 0.8502) | LR: 2.81e-04
  Batch 1450/1500 ( 96.7%) | Loss: 0.5036 (avg: 0.5704) | Acc: 0.9375 (avg: 0.8504) | LR: 2.84e-04
  Batch 1500/1500 (100.0%) | Loss: 0.6301 (avg: 0.5698) | Acc: 0.8438 (avg: 0.8508) | LR: 2.86e-04

Train Summary:
  Final Loss: 0.5698 | Final Acc: 0.8508
  Final LR: 2.86e-04
  Loss std: 0.0828 | Acc std: 0.0435

Val Epoch 4 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4908 | Final Acc: 0.8892
  Loss std: 0.0712 | Acc std: 0.0405
Epoch 04/20 | train 0.5698/0.8508 | val 0.4908/0.8892

Train Epoch 5 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.5921 (avg: 0.5210) | Acc: 0.8750 (avg: 0.8734) | LR: 2.89e-04
  Batch  100/1500 (  6.7%) | Loss: 0.6528 (avg: 0.5235) | Acc: 0.7656 (avg: 0.8716) | LR: 2.92e-04
  Batch  150/1500 ( 10.0%) | Loss: 0.5070 (avg: 0.5218) | Acc: 0.9062 (avg: 0.8729) | LR: 2.95e-04
  Batch  200/1500 ( 13.3%) | Loss: 0.4627 (avg: 0.5199) | Acc: 0.9375 (avg: 0.8749) | LR: 2.97e-04
  Batch  250/1500 ( 16.7%) | Loss: 0.5241 (avg: 0.5184) | Acc: 0.8906 (avg: 0.8751) | LR: 3.00e-04
  Batch  300/1500 ( 20.0%) | Loss: 0.5187 (avg: 0.5163) | Acc: 0.9062 (avg: 0.8760) | LR: 3.02e-04
  Batch  350/1500 ( 23.3%) | Loss: 0.4888 (avg: 0.5143) | Acc: 0.9062 (avg: 0.8771) | LR: 3.05e-04
  Batch  400/1500 ( 26.7%) | Loss: 0.4277 (avg: 0.5118) | Acc: 0.9375 (avg: 0.8783) | LR: 3.07e-04
  Batch  450/1500 ( 30.0%) | Loss: 0.5401 (avg: 0.5123) | Acc: 0.8906 (avg: 0.8780) | LR: 3.10e-04
  Batch  500/1500 ( 33.3%) | Loss: 0.4028 (avg: 0.5137) | Acc: 0.9219 (avg: 0.8772) | LR: 3.12e-04
  Batch  550/1500 ( 36.7%) | Loss: 0.4859 (avg: 0.5126) | Acc: 0.8906 (avg: 0.8778) | LR: 3.15e-04
  Batch  600/1500 ( 40.0%) | Loss: 0.5474 (avg: 0.5137) | Acc: 0.8281 (avg: 0.8772) | LR: 3.17e-04
  Batch  650/1500 ( 43.3%) | Loss: 0.5686 (avg: 0.5135) | Acc: 0.8594 (avg: 0.8773) | LR: 3.19e-04
  Batch  700/1500 ( 46.7%) | Loss: 0.3744 (avg: 0.5131) | Acc: 0.9688 (avg: 0.8774) | LR: 3.22e-04
  Batch  750/1500 ( 50.0%) | Loss: 0.5479 (avg: 0.5137) | Acc: 0.8125 (avg: 0.8771) | LR: 3.24e-04
  Batch  800/1500 ( 53.3%) | Loss: 0.4992 (avg: 0.5134) | Acc: 0.8594 (avg: 0.8774) | LR: 3.26e-04
  Batch  850/1500 ( 56.7%) | Loss: 0.5153 (avg: 0.5131) | Acc: 0.8750 (avg: 0.8776) | LR: 3.28e-04
  Batch  900/1500 ( 60.0%) | Loss: 0.5613 (avg: 0.5130) | Acc: 0.8594 (avg: 0.8777) | LR: 3.30e-04
  Batch  950/1500 ( 63.3%) | Loss: 0.3590 (avg: 0.5118) | Acc: 0.9688 (avg: 0.8780) | LR: 3.33e-04
  Batch 1000/1500 ( 66.7%) | Loss: 0.4801 (avg: 0.5112) | Acc: 0.9219 (avg: 0.8786) | LR: 3.35e-04
  Batch 1050/1500 ( 70.0%) | Loss: 0.5850 (avg: 0.5103) | Acc: 0.9062 (avg: 0.8791) | LR: 3.37e-04
  Batch 1100/1500 ( 73.3%) | Loss: 0.4268 (avg: 0.5113) | Acc: 0.9531 (avg: 0.8788) | LR: 3.39e-04
  Batch 1150/1500 ( 76.7%) | Loss: 0.5381 (avg: 0.5112) | Acc: 0.8438 (avg: 0.8788) | LR: 3.40e-04
  Batch 1200/1500 ( 80.0%) | Loss: 0.6870 (avg: 0.5111) | Acc: 0.8281 (avg: 0.8791) | LR: 3.42e-04
  Batch 1250/1500 ( 83.3%) | Loss: 0.5405 (avg: 0.5113) | Acc: 0.8750 (avg: 0.8789) | LR: 3.44e-04
  Batch 1300/1500 ( 86.7%) | Loss: 0.4257 (avg: 0.5117) | Acc: 0.8906 (avg: 0.8791) | LR: 3.46e-04
  Batch 1350/1500 ( 90.0%) | Loss: 0.4867 (avg: 0.5114) | Acc: 0.8750 (avg: 0.8792) | LR: 3.48e-04
  Batch 1400/1500 ( 93.3%) | Loss: 0.5131 (avg: 0.5119) | Acc: 0.8438 (avg: 0.8788) | LR: 3.49e-04
  Batch 1450/1500 ( 96.7%) | Loss: 0.5147 (avg: 0.5117) | Acc: 0.8906 (avg: 0.8787) | LR: 3.51e-04
  Batch 1500/1500 (100.0%) | Loss: 0.4272 (avg: 0.5114) | Acc: 0.9375 (avg: 0.8786) | LR: 3.53e-04

Train Summary:
  Final Loss: 0.5114 | Final Acc: 0.8786
  Final LR: 3.53e-04
  Loss std: 0.0748 | Acc std: 0.0408

Val Epoch 5 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4692 | Final Acc: 0.9003
  Loss std: 0.0734 | Acc std: 0.0378
Epoch 05/20 | train 0.5114/0.8786 | val 0.4692/0.9003

Train Epoch 6 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.3923 (avg: 0.4632) | Acc: 0.9375 (avg: 0.8975) | LR: 3.54e-04
  Batch  100/1500 (  6.7%) | Loss: 0.5530 (avg: 0.4620) | Acc: 0.8281 (avg: 0.8989) | LR: 3.56e-04
  Batch  150/1500 ( 10.0%) | Loss: 0.5168 (avg: 0.4599) | Acc: 0.8750 (avg: 0.9006) | LR: 3.57e-04
  Batch  200/1500 ( 13.3%) | Loss: 0.3602 (avg: 0.4598) | Acc: 0.9531 (avg: 0.9002) | LR: 3.59e-04
  Batch  250/1500 ( 16.7%) | Loss: 0.4612 (avg: 0.4600) | Acc: 0.8906 (avg: 0.8999) | LR: 3.60e-04
  Batch  300/1500 ( 20.0%) | Loss: 0.4012 (avg: 0.4611) | Acc: 0.9062 (avg: 0.9002) | LR: 3.61e-04
  Batch  350/1500 ( 23.3%) | Loss: 0.6409 (avg: 0.4636) | Acc: 0.8125 (avg: 0.8992) | LR: 3.63e-04
  Batch  400/1500 ( 26.7%) | Loss: 0.5135 (avg: 0.4650) | Acc: 0.8594 (avg: 0.8990) | LR: 3.64e-04
  Batch  450/1500 ( 30.0%) | Loss: 0.4774 (avg: 0.4648) | Acc: 0.9062 (avg: 0.8997) | LR: 3.65e-04
  Batch  500/1500 ( 33.3%) | Loss: 0.5347 (avg: 0.4647) | Acc: 0.8750 (avg: 0.8997) | LR: 3.66e-04
  Batch  550/1500 ( 36.7%) | Loss: 0.3968 (avg: 0.4652) | Acc: 0.9219 (avg: 0.8996) | LR: 3.67e-04
  Batch  600/1500 ( 40.0%) | Loss: 0.4414 (avg: 0.4656) | Acc: 0.8750 (avg: 0.8996) | LR: 3.68e-04
  Batch  650/1500 ( 43.3%) | Loss: 0.5050 (avg: 0.4657) | Acc: 0.8906 (avg: 0.8991) | LR: 3.69e-04
  Batch  700/1500 ( 46.7%) | Loss: 0.3752 (avg: 0.4653) | Acc: 0.9375 (avg: 0.8994) | LR: 3.70e-04
  Batch  750/1500 ( 50.0%) | Loss: 0.4149 (avg: 0.4656) | Acc: 0.9375 (avg: 0.8993) | LR: 3.71e-04
  Batch  800/1500 ( 53.3%) | Loss: 0.4422 (avg: 0.4665) | Acc: 0.9062 (avg: 0.8988) | LR: 3.72e-04
  Batch  850/1500 ( 56.7%) | Loss: 0.4832 (avg: 0.4671) | Acc: 0.8906 (avg: 0.8984) | LR: 3.72e-04
  Batch  900/1500 ( 60.0%) | Loss: 0.6344 (avg: 0.4676) | Acc: 0.8281 (avg: 0.8984) | LR: 3.73e-04
  Batch  950/1500 ( 63.3%) | Loss: 0.3960 (avg: 0.4674) | Acc: 0.9375 (avg: 0.8985) | LR: 3.74e-04
  Batch 1000/1500 ( 66.7%) | Loss: 0.4289 (avg: 0.4679) | Acc: 0.9062 (avg: 0.8981) | LR: 3.74e-04
  Batch 1050/1500 ( 70.0%) | Loss: 0.3332 (avg: 0.4686) | Acc: 0.9531 (avg: 0.8979) | LR: 3.75e-04
  Batch 1100/1500 ( 73.3%) | Loss: 0.3604 (avg: 0.4683) | Acc: 0.9688 (avg: 0.8983) | LR: 3.75e-04
  Batch 1150/1500 ( 76.7%) | Loss: 0.4929 (avg: 0.4682) | Acc: 0.8906 (avg: 0.8983) | LR: 3.76e-04
  Batch 1200/1500 ( 80.0%) | Loss: 0.3706 (avg: 0.4678) | Acc: 0.9375 (avg: 0.8985) | LR: 3.76e-04
  Batch 1250/1500 ( 83.3%) | Loss: 0.3728 (avg: 0.4680) | Acc: 0.9375 (avg: 0.8986) | LR: 3.76e-04
  Batch 1300/1500 ( 86.7%) | Loss: 0.3995 (avg: 0.4681) | Acc: 0.9219 (avg: 0.8988) | LR: 3.76e-04
  Batch 1350/1500 ( 90.0%) | Loss: 0.4413 (avg: 0.4686) | Acc: 0.9219 (avg: 0.8987) | LR: 3.77e-04
  Batch 1400/1500 ( 93.3%) | Loss: 0.4292 (avg: 0.4693) | Acc: 0.8906 (avg: 0.8986) | LR: 3.77e-04
  Batch 1450/1500 ( 96.7%) | Loss: 0.4856 (avg: 0.4696) | Acc: 0.9219 (avg: 0.8986) | LR: 3.77e-04
  Batch 1500/1500 (100.0%) | Loss: 0.4981 (avg: 0.4692) | Acc: 0.8750 (avg: 0.8986) | LR: 3.77e-04

Train Summary:
  Final Loss: 0.4692 | Final Acc: 0.8986
  Final LR: 3.77e-04
  Loss std: 0.0734 | Acc std: 0.0389

Val Epoch 6 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4595 | Final Acc: 0.9015
  Loss std: 0.0735 | Acc std: 0.0397
Epoch 06/20 | train 0.4692/0.8986 | val 0.4595/0.9015

Train Epoch 7 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.3537 (avg: 0.4200) | Acc: 0.9688 (avg: 0.9194) | LR: 3.77e-04
  Batch  100/1500 (  6.7%) | Loss: 0.3762 (avg: 0.4219) | Acc: 0.9688 (avg: 0.9183) | LR: 3.77e-04
  Batch  150/1500 ( 10.0%) | Loss: 0.4476 (avg: 0.4265) | Acc: 0.9062 (avg: 0.9166) | LR: 3.77e-04
  Batch  200/1500 ( 13.3%) | Loss: 0.3730 (avg: 0.4243) | Acc: 0.9375 (avg: 0.9167) | LR: 3.77e-04
  Batch  250/1500 ( 16.7%) | Loss: 0.4372 (avg: 0.4256) | Acc: 0.9219 (avg: 0.9179) | LR: 3.77e-04
  Batch  300/1500 ( 20.0%) | Loss: 0.4457 (avg: 0.4240) | Acc: 0.9062 (avg: 0.9192) | LR: 3.77e-04
  Batch  350/1500 ( 23.3%) | Loss: 0.4125 (avg: 0.4240) | Acc: 0.9375 (avg: 0.9196) | LR: 3.77e-04
  Batch  400/1500 ( 26.7%) | Loss: 0.4110 (avg: 0.4243) | Acc: 0.9062 (avg: 0.9191) | LR: 3.77e-04
  Batch  450/1500 ( 30.0%) | Loss: 0.4217 (avg: 0.4258) | Acc: 0.9219 (avg: 0.9182) | LR: 3.76e-04
  Batch  500/1500 ( 33.3%) | Loss: 0.3616 (avg: 0.4263) | Acc: 0.9688 (avg: 0.9179) | LR: 3.76e-04
  Batch  550/1500 ( 36.7%) | Loss: 0.4634 (avg: 0.4257) | Acc: 0.8750 (avg: 0.9180) | LR: 3.76e-04
  Batch  600/1500 ( 40.0%) | Loss: 0.5174 (avg: 0.4262) | Acc: 0.8281 (avg: 0.9179) | LR: 3.76e-04
  Batch  650/1500 ( 43.3%) | Loss: 0.3890 (avg: 0.4273) | Acc: 0.9375 (avg: 0.9174) | LR: 3.76e-04
  Batch  700/1500 ( 46.7%) | Loss: 0.5209 (avg: 0.4281) | Acc: 0.8438 (avg: 0.9170) | LR: 3.76e-04
  Batch  750/1500 ( 50.0%) | Loss: 0.3765 (avg: 0.4288) | Acc: 0.9531 (avg: 0.9169) | LR: 3.76e-04
  Batch  800/1500 ( 53.3%) | Loss: 0.3801 (avg: 0.4291) | Acc: 0.9375 (avg: 0.9169) | LR: 3.76e-04
  Batch  850/1500 ( 56.7%) | Loss: 0.4874 (avg: 0.4298) | Acc: 0.8906 (avg: 0.9165) | LR: 3.75e-04
  Batch  900/1500 ( 60.0%) | Loss: 0.4663 (avg: 0.4305) | Acc: 0.9062 (avg: 0.9163) | LR: 3.75e-04
  Batch  950/1500 ( 63.3%) | Loss: 0.5416 (avg: 0.4309) | Acc: 0.9062 (avg: 0.9161) | LR: 3.75e-04
  Batch 1000/1500 ( 66.7%) | Loss: 0.3784 (avg: 0.4305) | Acc: 0.9531 (avg: 0.9162) | LR: 3.75e-04
  Batch 1050/1500 ( 70.0%) | Loss: 0.4289 (avg: 0.4312) | Acc: 0.9219 (avg: 0.9158) | LR: 3.75e-04
  Batch 1100/1500 ( 73.3%) | Loss: 0.3684 (avg: 0.4311) | Acc: 0.9375 (avg: 0.9158) | LR: 3.74e-04
  Batch 1150/1500 ( 76.7%) | Loss: 0.3636 (avg: 0.4314) | Acc: 0.9531 (avg: 0.9157) | LR: 3.74e-04
  Batch 1200/1500 ( 80.0%) | Loss: 0.4678 (avg: 0.4316) | Acc: 0.9062 (avg: 0.9155) | LR: 3.74e-04
  Batch 1250/1500 ( 83.3%) | Loss: 0.4057 (avg: 0.4317) | Acc: 0.9219 (avg: 0.9154) | LR: 3.74e-04
  Batch 1300/1500 ( 86.7%) | Loss: 0.3265 (avg: 0.4325) | Acc: 0.9844 (avg: 0.9150) | LR: 3.73e-04
  Batch 1350/1500 ( 90.0%) | Loss: 0.3790 (avg: 0.4321) | Acc: 0.9531 (avg: 0.9152) | LR: 3.73e-04
  Batch 1400/1500 ( 93.3%) | Loss: 0.4933 (avg: 0.4325) | Acc: 0.8906 (avg: 0.9149) | LR: 3.73e-04
  Batch 1450/1500 ( 96.7%) | Loss: 0.4101 (avg: 0.4327) | Acc: 0.9219 (avg: 0.9147) | LR: 3.72e-04
  Batch 1500/1500 (100.0%) | Loss: 0.5493 (avg: 0.4332) | Acc: 0.8594 (avg: 0.9144) | LR: 3.72e-04

Train Summary:
  Final Loss: 0.4332 | Final Acc: 0.9144
  Final LR: 3.72e-04
  Loss std: 0.0658 | Acc std: 0.0356

Val Epoch 7 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4442 | Final Acc: 0.9077
  Loss std: 0.0729 | Acc std: 0.0372
Epoch 07/20 | train 0.4332/0.9144 | val 0.4442/0.9077

Train Epoch 8 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.4204 (avg: 0.3825) | Acc: 0.9219 (avg: 0.9353) | LR: 3.72e-04
  Batch  100/1500 (  6.7%) | Loss: 0.3408 (avg: 0.3926) | Acc: 0.9531 (avg: 0.9316) | LR: 3.71e-04
  Batch  150/1500 ( 10.0%) | Loss: 0.3339 (avg: 0.3922) | Acc: 0.9531 (avg: 0.9325) | LR: 3.71e-04
  Batch  200/1500 ( 13.3%) | Loss: 0.3588 (avg: 0.3937) | Acc: 0.9531 (avg: 0.9319) | LR: 3.71e-04
  Batch  250/1500 ( 16.7%) | Loss: 0.4997 (avg: 0.3978) | Acc: 0.9375 (avg: 0.9310) | LR: 3.70e-04
  Batch  300/1500 ( 20.0%) | Loss: 0.3086 (avg: 0.3965) | Acc: 0.9688 (avg: 0.9319) | LR: 3.70e-04
  Batch  350/1500 ( 23.3%) | Loss: 0.3950 (avg: 0.3976) | Acc: 0.9375 (avg: 0.9309) | LR: 3.70e-04
  Batch  400/1500 ( 26.7%) | Loss: 0.4825 (avg: 0.3974) | Acc: 0.8906 (avg: 0.9314) | LR: 3.69e-04
  Batch  450/1500 ( 30.0%) | Loss: 0.3152 (avg: 0.3976) | Acc: 0.9531 (avg: 0.9318) | LR: 3.69e-04
  Batch  500/1500 ( 33.3%) | Loss: 0.4451 (avg: 0.3978) | Acc: 0.9531 (avg: 0.9317) | LR: 3.68e-04
  Batch  550/1500 ( 36.7%) | Loss: 0.4224 (avg: 0.3986) | Acc: 0.9375 (avg: 0.9311) | LR: 3.68e-04
  Batch  600/1500 ( 40.0%) | Loss: 0.4047 (avg: 0.4004) | Acc: 0.9219 (avg: 0.9304) | LR: 3.68e-04
  Batch  650/1500 ( 43.3%) | Loss: 0.4013 (avg: 0.3999) | Acc: 0.9219 (avg: 0.9304) | LR: 3.67e-04
  Batch  700/1500 ( 46.7%) | Loss: 0.3568 (avg: 0.4004) | Acc: 0.9375 (avg: 0.9303) | LR: 3.67e-04
  Batch  750/1500 ( 50.0%) | Loss: 0.3363 (avg: 0.4013) | Acc: 0.9844 (avg: 0.9303) | LR: 3.66e-04
  Batch  800/1500 ( 53.3%) | Loss: 0.4695 (avg: 0.4007) | Acc: 0.8906 (avg: 0.9306) | LR: 3.66e-04
  Batch  850/1500 ( 56.7%) | Loss: 0.4349 (avg: 0.4009) | Acc: 0.9062 (avg: 0.9308) | LR: 3.65e-04
  Batch  900/1500 ( 60.0%) | Loss: 0.5016 (avg: 0.4014) | Acc: 0.9219 (avg: 0.9304) | LR: 3.65e-04
  Batch  950/1500 ( 63.3%) | Loss: 0.3133 (avg: 0.4015) | Acc: 0.9688 (avg: 0.9300) | LR: 3.64e-04
  Batch 1000/1500 ( 66.7%) | Loss: 0.3675 (avg: 0.4019) | Acc: 0.9219 (avg: 0.9296) | LR: 3.64e-04
  Batch 1050/1500 ( 70.0%) | Loss: 0.4485 (avg: 0.4022) | Acc: 0.8906 (avg: 0.9295) | LR: 3.63e-04
  Batch 1100/1500 ( 73.3%) | Loss: 0.4626 (avg: 0.4028) | Acc: 0.9062 (avg: 0.9294) | LR: 3.63e-04
  Batch 1150/1500 ( 76.7%) | Loss: 0.4011 (avg: 0.4032) | Acc: 0.9219 (avg: 0.9291) | LR: 3.62e-04
  Batch 1200/1500 ( 80.0%) | Loss: 0.5174 (avg: 0.4040) | Acc: 0.8750 (avg: 0.9290) | LR: 3.62e-04
  Batch 1250/1500 ( 83.3%) | Loss: 0.3444 (avg: 0.4039) | Acc: 0.9531 (avg: 0.9289) | LR: 3.61e-04
  Batch 1300/1500 ( 86.7%) | Loss: 0.4215 (avg: 0.4041) | Acc: 0.9219 (avg: 0.9287) | LR: 3.61e-04
  Batch 1350/1500 ( 90.0%) | Loss: 0.3686 (avg: 0.4047) | Acc: 0.8906 (avg: 0.9285) | LR: 3.60e-04
  Batch 1400/1500 ( 93.3%) | Loss: 0.4482 (avg: 0.4051) | Acc: 0.9375 (avg: 0.9283) | LR: 3.59e-04
  Batch 1450/1500 ( 96.7%) | Loss: 0.3761 (avg: 0.4055) | Acc: 0.9375 (avg: 0.9281) | LR: 3.59e-04
  Batch 1500/1500 (100.0%) | Loss: 0.3849 (avg: 0.4054) | Acc: 0.9375 (avg: 0.9280) | LR: 3.58e-04

Train Summary:
  Final Loss: 0.4054 | Final Acc: 0.9280
  Final LR: 3.58e-04
  Loss std: 0.0606 | Acc std: 0.0327

Val Epoch 8 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4392 | Final Acc: 0.9130
  Loss std: 0.0755 | Acc std: 0.0362
Epoch 08/20 | train 0.4054/0.9280 | val 0.4392/0.9130

Train Epoch 9 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.3241 (avg: 0.3675) | Acc: 0.9531 (avg: 0.9472) | LR: 3.58e-04
  Batch  100/1500 (  6.7%) | Loss: 0.3285 (avg: 0.3708) | Acc: 0.9688 (avg: 0.9448) | LR: 3.57e-04
  Batch  150/1500 ( 10.0%) | Loss: 0.3632 (avg: 0.3677) | Acc: 0.9531 (avg: 0.9454) | LR: 3.56e-04
  Batch  200/1500 ( 13.3%) | Loss: 0.3509 (avg: 0.3682) | Acc: 0.9531 (avg: 0.9455) | LR: 3.56e-04
  Batch  250/1500 ( 16.7%) | Loss: 0.3315 (avg: 0.3684) | Acc: 0.9688 (avg: 0.9444) | LR: 3.55e-04
  Batch  300/1500 ( 20.0%) | Loss: 0.3752 (avg: 0.3674) | Acc: 0.9219 (avg: 0.9444) | LR: 3.54e-04
  Batch  350/1500 ( 23.3%) | Loss: 0.3201 (avg: 0.3682) | Acc: 1.0000 (avg: 0.9443) | LR: 3.54e-04
  Batch  400/1500 ( 26.7%) | Loss: 0.4470 (avg: 0.3709) | Acc: 0.9062 (avg: 0.9430) | LR: 3.53e-04
  Batch  450/1500 ( 30.0%) | Loss: 0.3869 (avg: 0.3722) | Acc: 0.9688 (avg: 0.9425) | LR: 3.52e-04
  Batch  500/1500 ( 33.3%) | Loss: 0.3787 (avg: 0.3726) | Acc: 0.9375 (avg: 0.9422) | LR: 3.52e-04
  Batch  550/1500 ( 36.7%) | Loss: 0.4171 (avg: 0.3735) | Acc: 0.9062 (avg: 0.9420) | LR: 3.51e-04
  Batch  600/1500 ( 40.0%) | Loss: 0.5033 (avg: 0.3744) | Acc: 0.9062 (avg: 0.9416) | LR: 3.50e-04
  Batch  650/1500 ( 43.3%) | Loss: 0.4306 (avg: 0.3752) | Acc: 0.9219 (avg: 0.9413) | LR: 3.49e-04
  Batch  700/1500 ( 46.7%) | Loss: 0.3734 (avg: 0.3754) | Acc: 0.9219 (avg: 0.9410) | LR: 3.49e-04
  Batch  750/1500 ( 50.0%) | Loss: 0.4465 (avg: 0.3767) | Acc: 0.8906 (avg: 0.9406) | LR: 3.48e-04
  Batch  800/1500 ( 53.3%) | Loss: 0.4750 (avg: 0.3775) | Acc: 0.8906 (avg: 0.9401) | LR: 3.47e-04
  Batch  850/1500 ( 56.7%) | Loss: 0.3430 (avg: 0.3776) | Acc: 0.9531 (avg: 0.9403) | LR: 3.46e-04
  Batch  900/1500 ( 60.0%) | Loss: 0.3963 (avg: 0.3778) | Acc: 0.8906 (avg: 0.9401) | LR: 3.46e-04
  Batch  950/1500 ( 63.3%) | Loss: 0.4311 (avg: 0.3777) | Acc: 0.9062 (avg: 0.9399) | LR: 3.45e-04
  Batch 1000/1500 ( 66.7%) | Loss: 0.4061 (avg: 0.3781) | Acc: 0.9219 (avg: 0.9398) | LR: 3.44e-04
  Batch 1050/1500 ( 70.0%) | Loss: 0.4107 (avg: 0.3786) | Acc: 0.9219 (avg: 0.9394) | LR: 3.43e-04
  Batch 1100/1500 ( 73.3%) | Loss: 0.4779 (avg: 0.3791) | Acc: 0.8750 (avg: 0.9392) | LR: 3.43e-04
  Batch 1150/1500 ( 76.7%) | Loss: 0.3996 (avg: 0.3802) | Acc: 0.9219 (avg: 0.9387) | LR: 3.42e-04
  Batch 1200/1500 ( 80.0%) | Loss: 0.4188 (avg: 0.3799) | Acc: 0.9062 (avg: 0.9388) | LR: 3.41e-04
  Batch 1250/1500 ( 83.3%) | Loss: 0.3258 (avg: 0.3794) | Acc: 0.9531 (avg: 0.9390) | LR: 3.40e-04
  Batch 1300/1500 ( 86.7%) | Loss: 0.3395 (avg: 0.3790) | Acc: 0.9531 (avg: 0.9391) | LR: 3.39e-04
  Batch 1350/1500 ( 90.0%) | Loss: 0.3592 (avg: 0.3796) | Acc: 0.9531 (avg: 0.9387) | LR: 3.38e-04
  Batch 1400/1500 ( 93.3%) | Loss: 0.4008 (avg: 0.3793) | Acc: 0.9375 (avg: 0.9389) | LR: 3.37e-04
  Batch 1450/1500 ( 96.7%) | Loss: 0.3512 (avg: 0.3796) | Acc: 0.9531 (avg: 0.9387) | LR: 3.37e-04
  Batch 1500/1500 (100.0%) | Loss: 0.3844 (avg: 0.3800) | Acc: 0.9219 (avg: 0.9386) | LR: 3.36e-04

Train Summary:
  Final Loss: 0.3800 | Final Acc: 0.9386
  Final LR: 3.36e-04
  Loss std: 0.0542 | Acc std: 0.0308

Val Epoch 9 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4370 | Final Acc: 0.9145
  Loss std: 0.0777 | Acc std: 0.0350
Epoch 09/20 | train 0.3800/0.9386 | val 0.4370/0.9145

Train Epoch 10 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.3617 (avg: 0.3409) | Acc: 0.9531 (avg: 0.9581) | LR: 3.35e-04
  Batch  100/1500 (  6.7%) | Loss: 0.3125 (avg: 0.3492) | Acc: 0.9688 (avg: 0.9547) | LR: 3.34e-04
  Batch  150/1500 ( 10.0%) | Loss: 0.3349 (avg: 0.3487) | Acc: 0.9531 (avg: 0.9544) | LR: 3.33e-04
  Batch  200/1500 ( 13.3%) | Loss: 0.3275 (avg: 0.3488) | Acc: 0.9688 (avg: 0.9538) | LR: 3.32e-04
  Batch  250/1500 ( 16.7%) | Loss: 0.4148 (avg: 0.3502) | Acc: 0.9375 (avg: 0.9530) | LR: 3.31e-04
  Batch  300/1500 ( 20.0%) | Loss: 0.3440 (avg: 0.3500) | Acc: 0.9688 (avg: 0.9534) | LR: 3.30e-04
  Batch  350/1500 ( 23.3%) | Loss: 0.3824 (avg: 0.3504) | Acc: 0.9375 (avg: 0.9530) | LR: 3.29e-04
  Batch  400/1500 ( 26.7%) | Loss: 0.4168 (avg: 0.3504) | Acc: 0.9531 (avg: 0.9530) | LR: 3.28e-04
  Batch  450/1500 ( 30.0%) | Loss: 0.4010 (avg: 0.3509) | Acc: 0.9219 (avg: 0.9528) | LR: 3.28e-04
  Batch  500/1500 ( 33.3%) | Loss: 0.3986 (avg: 0.3530) | Acc: 0.9219 (avg: 0.9518) | LR: 3.27e-04
  Batch  550/1500 ( 36.7%) | Loss: 0.4988 (avg: 0.3544) | Acc: 0.9062 (avg: 0.9514) | LR: 3.26e-04
  Batch  600/1500 ( 40.0%) | Loss: 0.3327 (avg: 0.3539) | Acc: 0.9531 (avg: 0.9514) | LR: 3.25e-04
  Batch  650/1500 ( 43.3%) | Loss: 0.3255 (avg: 0.3543) | Acc: 0.9531 (avg: 0.9508) | LR: 3.24e-04
  Batch  700/1500 ( 46.7%) | Loss: 0.3052 (avg: 0.3545) | Acc: 0.9844 (avg: 0.9506) | LR: 3.23e-04
  Batch  750/1500 ( 50.0%) | Loss: 0.3224 (avg: 0.3540) | Acc: 0.9688 (avg: 0.9510) | LR: 3.22e-04
  Batch  800/1500 ( 53.3%) | Loss: 0.3132 (avg: 0.3546) | Acc: 0.9688 (avg: 0.9505) | LR: 3.21e-04
  Batch  850/1500 ( 56.7%) | Loss: 0.3206 (avg: 0.3554) | Acc: 0.9688 (avg: 0.9501) | LR: 3.20e-04
  Batch  900/1500 ( 60.0%) | Loss: 0.3871 (avg: 0.3556) | Acc: 0.9531 (avg: 0.9501) | LR: 3.19e-04
  Batch  950/1500 ( 63.3%) | Loss: 0.3217 (avg: 0.3553) | Acc: 0.9688 (avg: 0.9502) | LR: 3.18e-04
  Batch 1000/1500 ( 66.7%) | Loss: 0.4011 (avg: 0.3556) | Acc: 0.9219 (avg: 0.9499) | LR: 3.17e-04
  Batch 1050/1500 ( 70.0%) | Loss: 0.3048 (avg: 0.3565) | Acc: 0.9688 (avg: 0.9496) | LR: 3.16e-04
  Batch 1100/1500 ( 73.3%) | Loss: 0.4429 (avg: 0.3568) | Acc: 0.8906 (avg: 0.9494) | LR: 3.15e-04
  Batch 1150/1500 ( 76.7%) | Loss: 0.3650 (avg: 0.3573) | Acc: 0.9219 (avg: 0.9491) | LR: 3.13e-04
  Batch 1200/1500 ( 80.0%) | Loss: 0.3213 (avg: 0.3575) | Acc: 0.9844 (avg: 0.9490) | LR: 3.12e-04
  Batch 1250/1500 ( 83.3%) | Loss: 0.4386 (avg: 0.3578) | Acc: 0.8906 (avg: 0.9490) | LR: 3.11e-04
  Batch 1300/1500 ( 86.7%) | Loss: 0.3931 (avg: 0.3577) | Acc: 0.9219 (avg: 0.9490) | LR: 3.10e-04
  Batch 1350/1500 ( 90.0%) | Loss: 0.3219 (avg: 0.3579) | Acc: 0.9844 (avg: 0.9491) | LR: 3.09e-04
  Batch 1400/1500 ( 93.3%) | Loss: 0.3641 (avg: 0.3577) | Acc: 0.9531 (avg: 0.9492) | LR: 3.08e-04
  Batch 1450/1500 ( 96.7%) | Loss: 0.3594 (avg: 0.3578) | Acc: 0.9531 (avg: 0.9492) | LR: 3.07e-04
  Batch 1500/1500 (100.0%) | Loss: 0.4224 (avg: 0.3585) | Acc: 0.9531 (avg: 0.9490) | LR: 3.06e-04

Train Summary:
  Final Loss: 0.3585 | Final Acc: 0.9490
  Final LR: 3.06e-04
  Loss std: 0.0460 | Acc std: 0.0270

Val Epoch 10 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4360 | Final Acc: 0.9141
  Loss std: 0.0791 | Acc std: 0.0359
Epoch 10/20 | train 0.3585/0.9490 | val 0.4360/0.9141

Train Epoch 11 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.3252 (avg: 0.3298) | Acc: 0.9531 (avg: 0.9613) | LR: 3.05e-04
  Batch  100/1500 (  6.7%) | Loss: 0.3260 (avg: 0.3297) | Acc: 0.9688 (avg: 0.9625) | LR: 3.04e-04
  Batch  150/1500 ( 10.0%) | Loss: 0.3502 (avg: 0.3328) | Acc: 0.9531 (avg: 0.9604) | LR: 3.03e-04
  Batch  200/1500 ( 13.3%) | Loss: 0.3841 (avg: 0.3307) | Acc: 0.9062 (avg: 0.9609) | LR: 3.01e-04
  Batch  250/1500 ( 16.7%) | Loss: 0.3998 (avg: 0.3312) | Acc: 0.9375 (avg: 0.9607) | LR: 3.00e-04
  Batch  300/1500 ( 20.0%) | Loss: 0.3172 (avg: 0.3333) | Acc: 0.9375 (avg: 0.9604) | LR: 2.99e-04
  Batch  350/1500 ( 23.3%) | Loss: 0.3361 (avg: 0.3330) | Acc: 0.9688 (avg: 0.9600) | LR: 2.98e-04
  Batch  400/1500 ( 26.7%) | Loss: 0.3160 (avg: 0.3331) | Acc: 0.9531 (avg: 0.9598) | LR: 2.97e-04
  Batch  450/1500 ( 30.0%) | Loss: 0.3178 (avg: 0.3342) | Acc: 0.9688 (avg: 0.9590) | LR: 2.96e-04
  Batch  500/1500 ( 33.3%) | Loss: 0.3156 (avg: 0.3340) | Acc: 0.9688 (avg: 0.9591) | LR: 2.95e-04
  Batch  550/1500 ( 36.7%) | Loss: 0.3342 (avg: 0.3342) | Acc: 0.9375 (avg: 0.9595) | LR: 2.93e-04
  Batch  600/1500 ( 40.0%) | Loss: 0.3063 (avg: 0.3345) | Acc: 1.0000 (avg: 0.9597) | LR: 2.92e-04
  Batch  650/1500 ( 43.3%) | Loss: 0.3268 (avg: 0.3341) | Acc: 0.9688 (avg: 0.9600) | LR: 2.91e-04
  Batch  700/1500 ( 46.7%) | Loss: 0.3040 (avg: 0.3342) | Acc: 0.9688 (avg: 0.9601) | LR: 2.90e-04
  Batch  750/1500 ( 50.0%) | Loss: 0.3506 (avg: 0.3340) | Acc: 0.9531 (avg: 0.9603) | LR: 2.89e-04
  Batch  800/1500 ( 53.3%) | Loss: 0.2931 (avg: 0.3338) | Acc: 0.9844 (avg: 0.9604) | LR: 2.87e-04
  Batch  850/1500 ( 56.7%) | Loss: 0.3360 (avg: 0.3341) | Acc: 0.9688 (avg: 0.9603) | LR: 2.86e-04
  Batch  900/1500 ( 60.0%) | Loss: 0.3172 (avg: 0.3343) | Acc: 1.0000 (avg: 0.9603) | LR: 2.85e-04
  Batch  950/1500 ( 63.3%) | Loss: 0.3056 (avg: 0.3341) | Acc: 1.0000 (avg: 0.9605) | LR: 2.84e-04
  Batch 1000/1500 ( 66.7%) | Loss: 0.3158 (avg: 0.3342) | Acc: 0.9688 (avg: 0.9603) | LR: 2.83e-04
  Batch 1050/1500 ( 70.0%) | Loss: 0.2913 (avg: 0.3345) | Acc: 1.0000 (avg: 0.9603) | LR: 2.81e-04
  Batch 1100/1500 ( 73.3%) | Loss: 0.3684 (avg: 0.3349) | Acc: 0.9375 (avg: 0.9600) | LR: 2.80e-04
  Batch 1150/1500 ( 76.7%) | Loss: 0.3781 (avg: 0.3352) | Acc: 0.9219 (avg: 0.9597) | LR: 2.79e-04
  Batch 1200/1500 ( 80.0%) | Loss: 0.4014 (avg: 0.3353) | Acc: 0.9531 (avg: 0.9596) | LR: 2.78e-04
  Batch 1250/1500 ( 83.3%) | Loss: 0.3531 (avg: 0.3354) | Acc: 0.9375 (avg: 0.9596) | LR: 2.76e-04
  Batch 1300/1500 ( 86.7%) | Loss: 0.3150 (avg: 0.3354) | Acc: 0.9688 (avg: 0.9596) | LR: 2.75e-04
  Batch 1350/1500 ( 90.0%) | Loss: 0.3343 (avg: 0.3357) | Acc: 0.9531 (avg: 0.9595) | LR: 2.74e-04
  Batch 1400/1500 ( 93.3%) | Loss: 0.3350 (avg: 0.3362) | Acc: 0.9688 (avg: 0.9592) | LR: 2.73e-04
  Batch 1450/1500 ( 96.7%) | Loss: 0.3223 (avg: 0.3363) | Acc: 0.9844 (avg: 0.9592) | LR: 2.71e-04
  Batch 1500/1500 (100.0%) | Loss: 0.3960 (avg: 0.3364) | Acc: 0.9219 (avg: 0.9591) | LR: 2.70e-04

Train Summary:
  Final Loss: 0.3364 | Final Acc: 0.9591
  Final LR: 2.70e-04
  Loss std: 0.0412 | Acc std: 0.0249

Val Epoch 11 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4359 | Final Acc: 0.9131
  Loss std: 0.0771 | Acc std: 0.0349
Epoch 11/20 | train 0.3364/0.9591 | val 0.4359/0.9131

Train Epoch 12 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.3216 (avg: 0.3131) | Acc: 0.9531 (avg: 0.9706) | LR: 2.69e-04
  Batch  100/1500 (  6.7%) | Loss: 0.3841 (avg: 0.3100) | Acc: 0.9062 (avg: 0.9712) | LR: 2.68e-04
  Batch  150/1500 ( 10.0%) | Loss: 0.2726 (avg: 0.3103) | Acc: 1.0000 (avg: 0.9717) | LR: 2.66e-04
  Batch  200/1500 ( 13.3%) | Loss: 0.2758 (avg: 0.3118) | Acc: 1.0000 (avg: 0.9711) | LR: 2.65e-04
  Batch  250/1500 ( 16.7%) | Loss: 0.2994 (avg: 0.3132) | Acc: 0.9844 (avg: 0.9708) | LR: 2.64e-04
  Batch  300/1500 ( 20.0%) | Loss: 0.3109 (avg: 0.3137) | Acc: 0.9688 (avg: 0.9708) | LR: 2.62e-04
  Batch  350/1500 ( 23.3%) | Loss: 0.2954 (avg: 0.3139) | Acc: 1.0000 (avg: 0.9707) | LR: 2.61e-04
  Batch  400/1500 ( 26.7%) | Loss: 0.2767 (avg: 0.3144) | Acc: 1.0000 (avg: 0.9704) | LR: 2.60e-04
  Batch  450/1500 ( 30.0%) | Loss: 0.3153 (avg: 0.3159) | Acc: 0.9375 (avg: 0.9694) | LR: 2.59e-04
  Batch  500/1500 ( 33.3%) | Loss: 0.3251 (avg: 0.3154) | Acc: 0.9688 (avg: 0.9694) | LR: 2.57e-04
  Batch  550/1500 ( 36.7%) | Loss: 0.2809 (avg: 0.3161) | Acc: 0.9844 (avg: 0.9689) | LR: 2.56e-04
  Batch  600/1500 ( 40.0%) | Loss: 0.3590 (avg: 0.3166) | Acc: 0.9219 (avg: 0.9685) | LR: 2.55e-04
  Batch  650/1500 ( 43.3%) | Loss: 0.3221 (avg: 0.3172) | Acc: 0.9531 (avg: 0.9683) | LR: 2.53e-04
  Batch  700/1500 ( 46.7%) | Loss: 0.3354 (avg: 0.3170) | Acc: 0.9688 (avg: 0.9682) | LR: 2.52e-04
  Batch  750/1500 ( 50.0%) | Loss: 0.3857 (avg: 0.3175) | Acc: 0.9375 (avg: 0.9682) | LR: 2.51e-04
  Batch  800/1500 ( 53.3%) | Loss: 0.2583 (avg: 0.3174) | Acc: 1.0000 (avg: 0.9681) | LR: 2.49e-04
  Batch  850/1500 ( 56.7%) | Loss: 0.3102 (avg: 0.3175) | Acc: 0.9844 (avg: 0.9681) | LR: 2.48e-04
  Batch  900/1500 ( 60.0%) | Loss: 0.3048 (avg: 0.3180) | Acc: 0.9844 (avg: 0.9679) | LR: 2.47e-04
  Batch  950/1500 ( 63.3%) | Loss: 0.2795 (avg: 0.3187) | Acc: 1.0000 (avg: 0.9675) | LR: 2.45e-04
  Batch 1000/1500 ( 66.7%) | Loss: 0.3431 (avg: 0.3190) | Acc: 0.9531 (avg: 0.9674) | LR: 2.44e-04
  Batch 1050/1500 ( 70.0%) | Loss: 0.2918 (avg: 0.3193) | Acc: 0.9688 (avg: 0.9673) | LR: 2.43e-04
  Batch 1100/1500 ( 73.3%) | Loss: 0.2792 (avg: 0.3194) | Acc: 1.0000 (avg: 0.9671) | LR: 2.41e-04
  Batch 1150/1500 ( 76.7%) | Loss: 0.3478 (avg: 0.3197) | Acc: 0.9531 (avg: 0.9669) | LR: 2.40e-04
  Batch 1200/1500 ( 80.0%) | Loss: 0.4260 (avg: 0.3196) | Acc: 0.9375 (avg: 0.9670) | LR: 2.39e-04
  Batch 1250/1500 ( 83.3%) | Loss: 0.3413 (avg: 0.3199) | Acc: 0.9531 (avg: 0.9669) | LR: 2.37e-04
  Batch 1300/1500 ( 86.7%) | Loss: 0.3153 (avg: 0.3205) | Acc: 0.9688 (avg: 0.9665) | LR: 2.36e-04
  Batch 1350/1500 ( 90.0%) | Loss: 0.3030 (avg: 0.3207) | Acc: 0.9688 (avg: 0.9664) | LR: 2.34e-04
  Batch 1400/1500 ( 93.3%) | Loss: 0.3088 (avg: 0.3205) | Acc: 0.9844 (avg: 0.9664) | LR: 2.33e-04
  Batch 1450/1500 ( 96.7%) | Loss: 0.2935 (avg: 0.3202) | Acc: 0.9688 (avg: 0.9666) | LR: 2.32e-04
  Batch 1500/1500 (100.0%) | Loss: 0.3297 (avg: 0.3203) | Acc: 0.9531 (avg: 0.9665) | LR: 2.30e-04

Train Summary:
  Final Loss: 0.3203 | Final Acc: 0.9665
  Final LR: 2.30e-04
  Loss std: 0.0354 | Acc std: 0.0222

Val Epoch 12 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4425 | Final Acc: 0.9135
  Loss std: 0.0824 | Acc std: 0.0351
Epoch 12/20 | train 0.3203/0.9665 | val 0.4425/0.9135
Early stopping at epoch 12
wandb:
wandb: Run history:
wandb:               batch ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñá‚ñÅ‚ñÇ‚ñÑ‚ñá‚ñÇ‚ñá‚ñÉ‚ñá‚ñà‚ñà‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñà‚ñÑ‚ñÖ‚ñÅ‚ñÜ‚ñà‚ñÑ‚ñá‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñá
wandb:          best_epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà
wandb:        best_val_acc ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:    final_best_epoch ‚ñÅ
wandb:  final_best_val_acc ‚ñÅ
wandb: final_learning_rate ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÖ
wandb:      final_val_loss ‚ñÅ
wandb:       learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ
wandb:           train_acc ‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                 +15 ...
wandb:
wandb: Run summary:
wandb:               batch 1500
wandb:          best_epoch 9
wandb:        best_val_acc 0.91446
wandb:               epoch 12
wandb:    final_best_epoch 9
wandb:  final_best_val_acc 0.91446
wandb: final_learning_rate 0.00023
wandb:      final_val_loss 0.44248
wandb:       learning_rate 0.00023
wandb:              status completed
wandb:                 +16 ...
wandb:
wandb: üöÄ View run trial_000 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/dof46kz8
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_053322-dof46kz8/logs
[I 2025-10-06 05:49:37,336] Trial 0 finished with value: 0.9144583333333334 and parameters: {'emb_dim': 200, 'channels': 256, 'kernel_sizes': (2, 3, 4, 5), 'dropout': 0.6, 'lr': 0.0003768726742752859, 'weight_decay': 0.0008988041214758503, 'batch_size': 64, 'max_len': 160, 'grad_clip': 2.0, 'optimizer': 'adamw'}. Best is trial 0 with value: 0.9144583333333334.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_054937-p6otox4y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run decent-river-4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/p6otox4y
wandb: Finishing previous runs because reinit is set to True.
wandb:
wandb: Run history:
wandb:   batch_size ‚ñÅ
wandb:     channels ‚ñÅ
wandb:      dropout ‚ñÅ
wandb:      emb_dim ‚ñÅ
wandb:    grad_clip ‚ñÅ
wandb:           lr ‚ñÅ
wandb:      max_len ‚ñÅ
wandb:      val_acc ‚ñÅ
wandb: weight_decay ‚ñÅ
wandb:
wandb: Run summary:
wandb:   batch_size 64
wandb:     channels 256
wandb:      dropout 0.6
wandb:      emb_dim 200
wandb:    grad_clip 2
wandb:           lr 0.00038
wandb:      max_len 160
wandb:    optimizer adamw
wandb:      val_acc 0.91446
wandb: weight_decay 0.0009
wandb:
wandb: üöÄ View run trial/0/decent-river-4 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/p6otox4y
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_054937-p6otox4y/logs
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_054937-ro1apyvf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_001
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/ro1apyvf

Train Epoch 1 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 1.3197 (avg: 1.3983) | Acc: 0.3906 (avg: 0.3231) | LR: 1.03e-04
  Batch  100/750 ( 13.3%) | Loss: 1.1121 (avg: 1.3086) | Acc: 0.5156 (avg: 0.4021) | LR: 1.05e-04
  Batch  150/750 ( 20.0%) | Loss: 1.0475 (avg: 1.2378) | Acc: 0.5938 (avg: 0.4548) | LR: 1.08e-04
  Batch  200/750 ( 26.7%) | Loss: 1.0027 (avg: 1.1750) | Acc: 0.6484 (avg: 0.5003) | LR: 1.14e-04
  Batch  250/750 ( 33.3%) | Loss: 0.8997 (avg: 1.1218) | Acc: 0.6875 (avg: 0.5373) | LR: 1.20e-04
  Batch  300/750 ( 40.0%) | Loss: 0.8304 (avg: 1.0742) | Acc: 0.7266 (avg: 0.5668) | LR: 1.29e-04
  Batch  350/750 ( 46.7%) | Loss: 0.7489 (avg: 1.0339) | Acc: 0.7734 (avg: 0.5910) | LR: 1.38e-04
  Batch  400/750 ( 53.3%) | Loss: 0.7538 (avg: 0.9989) | Acc: 0.7734 (avg: 0.6112) | LR: 1.49e-04
  Batch  450/750 ( 60.0%) | Loss: 0.7597 (avg: 0.9690) | Acc: 0.7578 (avg: 0.6287) | LR: 1.62e-04
  Batch  500/750 ( 66.7%) | Loss: 0.6582 (avg: 0.9417) | Acc: 0.7969 (avg: 0.6446) | LR: 1.75e-04
  Batch  550/750 ( 73.3%) | Loss: 0.7715 (avg: 0.9170) | Acc: 0.7422 (avg: 0.6585) | LR: 1.91e-04
  Batch  600/750 ( 80.0%) | Loss: 0.6425 (avg: 0.8957) | Acc: 0.7969 (avg: 0.6708) | LR: 2.07e-04
  Batch  650/750 ( 86.7%) | Loss: 0.6880 (avg: 0.8767) | Acc: 0.7891 (avg: 0.6815) | LR: 2.25e-04
  Batch  700/750 ( 93.3%) | Loss: 0.6262 (avg: 0.8599) | Acc: 0.8125 (avg: 0.6910) | LR: 2.45e-04
  Batch  750/750 (100.0%) | Loss: 0.6248 (avg: 0.8443) | Acc: 0.8125 (avg: 0.6995) | LR: 2.66e-04

Train Summary:
  Final Loss: 0.8443 | Final Acc: 0.6995
  Final LR: 2.65e-04
  Loss std: 0.2353 | Acc std: 0.1453

Val Epoch 1 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5486 | Final Acc: 0.8654
  Loss std: 0.0480 | Acc std: 0.0273
Epoch 01/20 | train 0.8443/0.6995 | val 0.5486/0.8654

Train Epoch 2 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.7503 (avg: 0.5455) | Acc: 0.7578 (avg: 0.8645) | LR: 2.87e-04
  Batch  100/750 ( 13.3%) | Loss: 0.4905 (avg: 0.5440) | Acc: 0.8594 (avg: 0.8630) | LR: 3.11e-04
  Batch  150/750 ( 20.0%) | Loss: 0.6451 (avg: 0.5406) | Acc: 0.8125 (avg: 0.8643) | LR: 3.35e-04
  Batch  200/750 ( 26.7%) | Loss: 0.5507 (avg: 0.5423) | Acc: 0.8672 (avg: 0.8638) | LR: 3.61e-04
  Batch  250/750 ( 33.3%) | Loss: 0.5328 (avg: 0.5417) | Acc: 0.8516 (avg: 0.8651) | LR: 3.88e-04
  Batch  300/750 ( 40.0%) | Loss: 0.4359 (avg: 0.5390) | Acc: 0.8984 (avg: 0.8661) | LR: 4.16e-04
  Batch  350/750 ( 46.7%) | Loss: 0.4681 (avg: 0.5391) | Acc: 0.8906 (avg: 0.8661) | LR: 4.45e-04
  Batch  400/750 ( 53.3%) | Loss: 0.5210 (avg: 0.5388) | Acc: 0.8359 (avg: 0.8663) | LR: 4.75e-04
  Batch  450/750 ( 60.0%) | Loss: 0.5442 (avg: 0.5396) | Acc: 0.8438 (avg: 0.8659) | LR: 5.06e-04
  Batch  500/750 ( 66.7%) | Loss: 0.4406 (avg: 0.5388) | Acc: 0.9062 (avg: 0.8664) | LR: 5.38e-04
  Batch  550/750 ( 73.3%) | Loss: 0.6165 (avg: 0.5394) | Acc: 0.8750 (avg: 0.8664) | LR: 5.71e-04
  Batch  600/750 ( 80.0%) | Loss: 0.5155 (avg: 0.5384) | Acc: 0.8828 (avg: 0.8667) | LR: 6.06e-04
  Batch  650/750 ( 86.7%) | Loss: 0.4447 (avg: 0.5387) | Acc: 0.8984 (avg: 0.8664) | LR: 6.40e-04
  Batch  700/750 ( 93.3%) | Loss: 0.5118 (avg: 0.5379) | Acc: 0.8984 (avg: 0.8663) | LR: 6.76e-04
  Batch  750/750 (100.0%) | Loss: 0.4991 (avg: 0.5378) | Acc: 0.8828 (avg: 0.8665) | LR: 7.13e-04

Train Summary:
  Final Loss: 0.5378 | Final Acc: 0.8665
  Final LR: 7.12e-04
  Loss std: 0.0575 | Acc std: 0.0306

Val Epoch 2 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4865 | Final Acc: 0.8918
  Loss std: 0.0509 | Acc std: 0.0254
Epoch 02/20 | train 0.5378/0.8665 | val 0.4865/0.8918

Train Epoch 3 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.2964 (avg: 0.4244) | Acc: 0.9766 (avg: 0.9186) | LR: 7.50e-04
  Batch  100/750 ( 13.3%) | Loss: 0.5155 (avg: 0.4345) | Acc: 0.9062 (avg: 0.9130) | LR: 7.88e-04
  Batch  150/750 ( 20.0%) | Loss: 0.3800 (avg: 0.4442) | Acc: 0.9219 (avg: 0.9094) | LR: 8.27e-04
  Batch  200/750 ( 26.7%) | Loss: 0.5052 (avg: 0.4482) | Acc: 0.8906 (avg: 0.9077) | LR: 8.66e-04
  Batch  250/750 ( 33.3%) | Loss: 0.4564 (avg: 0.4539) | Acc: 0.8906 (avg: 0.9054) | LR: 9.06e-04
  Batch  300/750 ( 40.0%) | Loss: 0.5937 (avg: 0.4591) | Acc: 0.8516 (avg: 0.9032) | LR: 9.46e-04
  Batch  350/750 ( 46.7%) | Loss: 0.4579 (avg: 0.4630) | Acc: 0.9141 (avg: 0.9014) | LR: 9.87e-04
  Batch  400/750 ( 53.3%) | Loss: 0.4886 (avg: 0.4661) | Acc: 0.8828 (avg: 0.9002) | LR: 1.03e-03
  Batch  450/750 ( 60.0%) | Loss: 0.6757 (avg: 0.4696) | Acc: 0.8125 (avg: 0.8982) | LR: 1.07e-03
  Batch  500/750 ( 66.7%) | Loss: 0.4956 (avg: 0.4746) | Acc: 0.9062 (avg: 0.8961) | LR: 1.11e-03
  Batch  550/750 ( 73.3%) | Loss: 0.5256 (avg: 0.4780) | Acc: 0.8672 (avg: 0.8951) | LR: 1.15e-03
  Batch  600/750 ( 80.0%) | Loss: 0.4723 (avg: 0.4799) | Acc: 0.8984 (avg: 0.8944) | LR: 1.20e-03
  Batch  650/750 ( 86.7%) | Loss: 0.4229 (avg: 0.4807) | Acc: 0.9062 (avg: 0.8941) | LR: 1.24e-03
  Batch  700/750 ( 93.3%) | Loss: 0.5820 (avg: 0.4842) | Acc: 0.8750 (avg: 0.8924) | LR: 1.28e-03
  Batch  750/750 (100.0%) | Loss: 0.5248 (avg: 0.4862) | Acc: 0.8672 (avg: 0.8916) | LR: 1.32e-03

Train Summary:
  Final Loss: 0.4862 | Final Acc: 0.8916
  Final LR: 1.32e-03
  Loss std: 0.0635 | Acc std: 0.0313

Val Epoch 3 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4707 | Final Acc: 0.9005
  Loss std: 0.0565 | Acc std: 0.0268
Epoch 03/20 | train 0.4862/0.8916 | val 0.4707/0.9005

Train Epoch 4 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.3825 (avg: 0.4060) | Acc: 0.9531 (avg: 0.9259) | LR: 1.37e-03
  Batch  100/750 ( 13.3%) | Loss: 0.3964 (avg: 0.4070) | Acc: 0.9297 (avg: 0.9270) | LR: 1.41e-03
  Batch  150/750 ( 20.0%) | Loss: 0.4633 (avg: 0.4136) | Acc: 0.8906 (avg: 0.9243) | LR: 1.45e-03
  Batch  200/750 ( 26.7%) | Loss: 0.4423 (avg: 0.4213) | Acc: 0.8906 (avg: 0.9209) | LR: 1.49e-03
  Batch  250/750 ( 33.3%) | Loss: 0.4188 (avg: 0.4283) | Acc: 0.9375 (avg: 0.9184) | LR: 1.54e-03
  Batch  300/750 ( 40.0%) | Loss: 0.4552 (avg: 0.4327) | Acc: 0.8984 (avg: 0.9171) | LR: 1.58e-03
  Batch  350/750 ( 46.7%) | Loss: 0.5121 (avg: 0.4378) | Acc: 0.8750 (avg: 0.9147) | LR: 1.62e-03
  Batch  400/750 ( 53.3%) | Loss: 0.5565 (avg: 0.4428) | Acc: 0.8438 (avg: 0.9126) | LR: 1.66e-03
  Batch  450/750 ( 60.0%) | Loss: 0.4319 (avg: 0.4454) | Acc: 0.9141 (avg: 0.9110) | LR: 1.70e-03
  Batch  500/750 ( 66.7%) | Loss: 0.4350 (avg: 0.4464) | Acc: 0.9375 (avg: 0.9107) | LR: 1.74e-03
  Batch  550/750 ( 73.3%) | Loss: 0.4814 (avg: 0.4482) | Acc: 0.9141 (avg: 0.9099) | LR: 1.78e-03
  Batch  600/750 ( 80.0%) | Loss: 0.4657 (avg: 0.4516) | Acc: 0.8906 (avg: 0.9087) | LR: 1.82e-03
  Batch  650/750 ( 86.7%) | Loss: 0.5439 (avg: 0.4547) | Acc: 0.8672 (avg: 0.9073) | LR: 1.86e-03
  Batch  700/750 ( 93.3%) | Loss: 0.4914 (avg: 0.4568) | Acc: 0.9141 (avg: 0.9066) | LR: 1.90e-03
  Batch  750/750 (100.0%) | Loss: 0.4650 (avg: 0.4581) | Acc: 0.9062 (avg: 0.9062) | LR: 1.93e-03

Train Summary:
  Final Loss: 0.4581 | Final Acc: 0.9062
  Final LR: 1.93e-03
  Loss std: 0.0619 | Acc std: 0.0283

Val Epoch 4 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5738 | Final Acc: 0.8607
  Loss std: 0.0667 | Acc std: 0.0289
Epoch 04/20 | train 0.4581/0.9062 | val 0.5738/0.8607

Train Epoch 5 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.4427 (avg: 0.3999) | Acc: 0.9062 (avg: 0.9333) | LR: 1.97e-03
  Batch  100/750 ( 13.3%) | Loss: 0.3738 (avg: 0.4027) | Acc: 0.9453 (avg: 0.9320) | LR: 2.01e-03
  Batch  150/750 ( 20.0%) | Loss: 0.5080 (avg: 0.4084) | Acc: 0.8984 (avg: 0.9306) | LR: 2.04e-03
  Batch  200/750 ( 26.7%) | Loss: 0.4299 (avg: 0.4172) | Acc: 0.8984 (avg: 0.9278) | LR: 2.08e-03
  Batch  250/750 ( 33.3%) | Loss: 0.4991 (avg: 0.4224) | Acc: 0.8594 (avg: 0.9258) | LR: 2.11e-03
  Batch  300/750 ( 40.0%) | Loss: 0.3834 (avg: 0.4257) | Acc: 0.9688 (avg: 0.9248) | LR: 2.14e-03
  Batch  350/750 ( 46.7%) | Loss: 0.4682 (avg: 0.4282) | Acc: 0.9062 (avg: 0.9244) | LR: 2.17e-03
  Batch  400/750 ( 53.3%) | Loss: 0.5210 (avg: 0.4297) | Acc: 0.9062 (avg: 0.9242) | LR: 2.20e-03
  Batch  450/750 ( 60.0%) | Loss: 0.4140 (avg: 0.4335) | Acc: 0.9062 (avg: 0.9224) | LR: 2.23e-03
  Batch  500/750 ( 66.7%) | Loss: 0.5647 (avg: 0.4384) | Acc: 0.8750 (avg: 0.9206) | LR: 2.26e-03
  Batch  550/750 ( 73.3%) | Loss: 0.3693 (avg: 0.4409) | Acc: 0.9453 (avg: 0.9193) | LR: 2.29e-03
  Batch  600/750 ( 80.0%) | Loss: 0.4501 (avg: 0.4454) | Acc: 0.9219 (avg: 0.9178) | LR: 2.31e-03
  Batch  650/750 ( 86.7%) | Loss: 0.4108 (avg: 0.4465) | Acc: 0.9141 (avg: 0.9171) | LR: 2.34e-03
  Batch  700/750 ( 93.3%) | Loss: 0.4805 (avg: 0.4476) | Acc: 0.9219 (avg: 0.9168) | LR: 2.36e-03
  Batch  750/750 (100.0%) | Loss: 0.5008 (avg: 0.4497) | Acc: 0.8828 (avg: 0.9160) | LR: 2.38e-03

Train Summary:
  Final Loss: 0.4497 | Final Acc: 0.9160
  Final LR: 2.38e-03
  Loss std: 0.0683 | Acc std: 0.0299

Val Epoch 5 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4879 | Final Acc: 0.8994
  Loss std: 0.0626 | Acc std: 0.0267
Epoch 05/20 | train 0.4497/0.9160 | val 0.4879/0.8994

Train Epoch 6 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.3355 (avg: 0.3824) | Acc: 0.9531 (avg: 0.9403) | LR: 2.40e-03
  Batch  100/750 ( 13.3%) | Loss: 0.3050 (avg: 0.3912) | Acc: 0.9844 (avg: 0.9384) | LR: 2.42e-03
  Batch  150/750 ( 20.0%) | Loss: 0.3999 (avg: 0.3936) | Acc: 0.9453 (avg: 0.9384) | LR: 2.44e-03
  Batch  200/750 ( 26.7%) | Loss: 0.4106 (avg: 0.3979) | Acc: 0.9297 (avg: 0.9367) | LR: 2.46e-03
  Batch  250/750 ( 33.3%) | Loss: 0.4146 (avg: 0.4031) | Acc: 0.9297 (avg: 0.9345) | LR: 2.47e-03
  Batch  300/750 ( 40.0%) | Loss: 0.3568 (avg: 0.4078) | Acc: 0.9766 (avg: 0.9331) | LR: 2.49e-03
  Batch  350/750 ( 46.7%) | Loss: 0.4618 (avg: 0.4090) | Acc: 0.9062 (avg: 0.9332) | LR: 2.50e-03
  Batch  400/750 ( 53.3%) | Loss: 0.3965 (avg: 0.4127) | Acc: 0.9531 (avg: 0.9321) | LR: 2.51e-03
  Batch  450/750 ( 60.0%) | Loss: 0.4059 (avg: 0.4142) | Acc: 0.9219 (avg: 0.9314) | LR: 2.52e-03
  Batch  500/750 ( 66.7%) | Loss: 0.4867 (avg: 0.4196) | Acc: 0.9141 (avg: 0.9297) | LR: 2.53e-03
  Batch  550/750 ( 73.3%) | Loss: 0.4453 (avg: 0.4238) | Acc: 0.9297 (avg: 0.9276) | LR: 2.53e-03
  Batch  600/750 ( 80.0%) | Loss: 0.4280 (avg: 0.4257) | Acc: 0.9141 (avg: 0.9273) | LR: 2.54e-03
  Batch  650/750 ( 86.7%) | Loss: 0.5472 (avg: 0.4279) | Acc: 0.8984 (avg: 0.9269) | LR: 2.54e-03
  Batch  700/750 ( 93.3%) | Loss: 0.4996 (avg: 0.4307) | Acc: 0.9297 (avg: 0.9258) | LR: 2.54e-03
  Batch  750/750 (100.0%) | Loss: 0.5081 (avg: 0.4330) | Acc: 0.8828 (avg: 0.9251) | LR: 2.54e-03

Train Summary:
  Final Loss: 0.4330 | Final Acc: 0.9251
  Final LR: 2.54e-03
  Loss std: 0.0641 | Acc std: 0.0273

Val Epoch 6 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5707 | Final Acc: 0.8761
  Loss std: 0.0751 | Acc std: 0.0269
Epoch 06/20 | train 0.4330/0.9251 | val 0.5707/0.8761
Early stopping at epoch 6
wandb:
wandb: Run history:
wandb:               batch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñá‚ñà‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñá‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñá
wandb:          best_epoch ‚ñÅ‚ñÖ‚ñà
wandb:        best_val_acc ‚ñÅ‚ñÜ‚ñà
wandb:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:    final_best_epoch ‚ñÅ
wandb:  final_best_val_acc ‚ñÅ
wandb: final_learning_rate ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñà
wandb:      final_val_loss ‚ñÅ
wandb:       learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:           train_acc ‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà
wandb:                 +15 ...
wandb:
wandb: Run summary:
wandb:               batch 750
wandb:          best_epoch 3
wandb:        best_val_acc 0.90054
wandb:               epoch 6
wandb:    final_best_epoch 3
wandb:  final_best_val_acc 0.90054
wandb: final_learning_rate 0.00254
wandb:      final_val_loss 0.5707
wandb:       learning_rate 0.00254
wandb:              status completed
wandb:                 +16 ...
wandb:
wandb: üöÄ View run trial_001 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/ro1apyvf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_054937-ro1apyvf/logs
[I 2025-10-06 05:59:11,002] Trial 1 finished with value: 0.9005416666666667 and parameters: {'emb_dim': 300, 'channels': 256, 'kernel_sizes': (3, 4, 5, 6), 'dropout': 0.4, 'lr': 0.0025448690937199384, 'weight_decay': 0.00017762762197963476, 'batch_size': 128, 'max_len': 200, 'grad_clip': 0.5, 'optimizer': 'adamw'}. Best is trial 0 with value: 0.9144583333333334.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_055911-08z5reil
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hopeful-water-6
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/08z5reil
wandb: Finishing previous runs because reinit is set to True.
wandb:
wandb: Run history:
wandb:   batch_size ‚ñÅ
wandb:     channels ‚ñÅ
wandb:      dropout ‚ñÅ
wandb:      emb_dim ‚ñÅ
wandb:    grad_clip ‚ñÅ
wandb:           lr ‚ñÅ
wandb:      max_len ‚ñÅ
wandb:      val_acc ‚ñÅ
wandb: weight_decay ‚ñÅ
wandb:
wandb: Run summary:
wandb:   batch_size 128
wandb:     channels 256
wandb:      dropout 0.4
wandb:      emb_dim 300
wandb:    grad_clip 0.5
wandb:           lr 0.00254
wandb:      max_len 200
wandb:    optimizer adamw
wandb:      val_acc 0.90054
wandb: weight_decay 0.00018
wandb:
wandb: üöÄ View run trial/1/hopeful-water-6 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/08z5reil
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_055911-08z5reil/logs
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_055911-91t80ols
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_002
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/91t80ols

Train Epoch 1 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 1.2577 (avg: 1.3558) | Acc: 0.4922 (avg: 0.3517) | LR: 1.64e-04
  Batch  100/750 ( 13.3%) | Loss: 1.1160 (avg: 1.2768) | Acc: 0.6484 (avg: 0.4473) | LR: 1.67e-04
  Batch  150/750 ( 20.0%) | Loss: 0.9376 (avg: 1.2029) | Acc: 0.7031 (avg: 0.5162) | LR: 1.73e-04
  Batch  200/750 ( 26.7%) | Loss: 0.8828 (avg: 1.1378) | Acc: 0.7422 (avg: 0.5642) | LR: 1.81e-04
  Batch  250/750 ( 33.3%) | Loss: 0.8035 (avg: 1.0809) | Acc: 0.7656 (avg: 0.6001) | LR: 1.92e-04
  Batch  300/750 ( 40.0%) | Loss: 0.6664 (avg: 1.0315) | Acc: 0.8203 (avg: 0.6265) | LR: 2.05e-04
  Batch  350/750 ( 46.7%) | Loss: 0.7910 (avg: 0.9898) | Acc: 0.7500 (avg: 0.6490) | LR: 2.20e-04
  Batch  400/750 ( 53.3%) | Loss: 0.7343 (avg: 0.9537) | Acc: 0.7656 (avg: 0.6673) | LR: 2.38e-04
  Batch  450/750 ( 60.0%) | Loss: 0.6527 (avg: 0.9207) | Acc: 0.7734 (avg: 0.6835) | LR: 2.58e-04
  Batch  500/750 ( 66.7%) | Loss: 0.6145 (avg: 0.8930) | Acc: 0.8750 (avg: 0.6967) | LR: 2.80e-04
  Batch  550/750 ( 73.3%) | Loss: 0.6070 (avg: 0.8697) | Acc: 0.8438 (avg: 0.7082) | LR: 3.04e-04
  Batch  600/750 ( 80.0%) | Loss: 0.5616 (avg: 0.8474) | Acc: 0.8750 (avg: 0.7189) | LR: 3.31e-04
  Batch  650/750 ( 86.7%) | Loss: 0.6106 (avg: 0.8290) | Acc: 0.8125 (avg: 0.7276) | LR: 3.60e-04
  Batch  700/750 ( 93.3%) | Loss: 0.6057 (avg: 0.8114) | Acc: 0.8281 (avg: 0.7358) | LR: 3.91e-04
  Batch  750/750 (100.0%) | Loss: 0.5357 (avg: 0.7957) | Acc: 0.8906 (avg: 0.7434) | LR: 4.24e-04

Train Summary:
  Final Loss: 0.7957 | Final Acc: 0.7434
  Final LR: 4.23e-04
  Loss std: 0.2389 | Acc std: 0.1382

Val Epoch 1 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5515 | Final Acc: 0.8600
  Loss std: 0.0510 | Acc std: 0.0300
Epoch 01/20 | train 0.7957/0.7434 | val 0.5515/0.8600

Train Epoch 2 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.5173 (avg: 0.5035) | Acc: 0.8594 (avg: 0.8836) | LR: 4.59e-04
  Batch  100/750 ( 13.3%) | Loss: 0.5749 (avg: 0.5019) | Acc: 0.8359 (avg: 0.8848) | LR: 4.96e-04
  Batch  150/750 ( 20.0%) | Loss: 0.5659 (avg: 0.5000) | Acc: 0.8750 (avg: 0.8858) | LR: 5.35e-04
  Batch  200/750 ( 26.7%) | Loss: 0.5180 (avg: 0.4987) | Acc: 0.8516 (avg: 0.8871) | LR: 5.76e-04
  Batch  250/750 ( 33.3%) | Loss: 0.5113 (avg: 0.4992) | Acc: 0.8438 (avg: 0.8868) | LR: 6.19e-04
  Batch  300/750 ( 40.0%) | Loss: 0.3851 (avg: 0.4986) | Acc: 0.9062 (avg: 0.8862) | LR: 6.63e-04
  Batch  350/750 ( 46.7%) | Loss: 0.4465 (avg: 0.4976) | Acc: 0.9219 (avg: 0.8860) | LR: 7.10e-04
  Batch  400/750 ( 53.3%) | Loss: 0.4613 (avg: 0.4968) | Acc: 0.8906 (avg: 0.8863) | LR: 7.58e-04
  Batch  450/750 ( 60.0%) | Loss: 0.5108 (avg: 0.4972) | Acc: 0.8594 (avg: 0.8861) | LR: 8.08e-04
  Batch  500/750 ( 66.7%) | Loss: 0.5727 (avg: 0.4969) | Acc: 0.8125 (avg: 0.8860) | LR: 8.59e-04
  Batch  550/750 ( 73.3%) | Loss: 0.5690 (avg: 0.4978) | Acc: 0.8438 (avg: 0.8857) | LR: 9.12e-04
  Batch  600/750 ( 80.0%) | Loss: 0.4715 (avg: 0.4983) | Acc: 0.8984 (avg: 0.8855) | LR: 9.66e-04
  Batch  650/750 ( 86.7%) | Loss: 0.5148 (avg: 0.4975) | Acc: 0.8906 (avg: 0.8858) | LR: 1.02e-03
  Batch  700/750 ( 93.3%) | Loss: 0.4795 (avg: 0.4978) | Acc: 0.8750 (avg: 0.8855) | LR: 1.08e-03
  Batch  750/750 (100.0%) | Loss: 0.5601 (avg: 0.4988) | Acc: 0.8594 (avg: 0.8848) | LR: 1.14e-03

Train Summary:
  Final Loss: 0.4988 | Final Acc: 0.8848
  Final LR: 1.14e-03
  Loss std: 0.0518 | Acc std: 0.0288

Val Epoch 2 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4921 | Final Acc: 0.8854
  Loss std: 0.0502 | Acc std: 0.0283
Epoch 02/20 | train 0.4988/0.8848 | val 0.4921/0.8854

Train Epoch 3 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.4479 (avg: 0.3959) | Acc: 0.9219 (avg: 0.9334) | LR: 1.20e-03
  Batch  100/750 ( 13.3%) | Loss: 0.3869 (avg: 0.3980) | Acc: 0.9219 (avg: 0.9310) | LR: 1.26e-03
  Batch  150/750 ( 20.0%) | Loss: 0.4345 (avg: 0.3989) | Acc: 0.8984 (avg: 0.9300) | LR: 1.32e-03
  Batch  200/750 ( 26.7%) | Loss: 0.4517 (avg: 0.4043) | Acc: 0.9375 (avg: 0.9272) | LR: 1.38e-03
  Batch  250/750 ( 33.3%) | Loss: 0.4417 (avg: 0.4141) | Acc: 0.8984 (avg: 0.9227) | LR: 1.45e-03
  Batch  300/750 ( 40.0%) | Loss: 0.3863 (avg: 0.4180) | Acc: 0.9141 (avg: 0.9210) | LR: 1.51e-03
  Batch  350/750 ( 46.7%) | Loss: 0.4648 (avg: 0.4223) | Acc: 0.9062 (avg: 0.9192) | LR: 1.57e-03
  Batch  400/750 ( 53.3%) | Loss: 0.5024 (avg: 0.4272) | Acc: 0.8906 (avg: 0.9170) | LR: 1.64e-03
  Batch  450/750 ( 60.0%) | Loss: 0.4020 (avg: 0.4309) | Acc: 0.9375 (avg: 0.9156) | LR: 1.71e-03
  Batch  500/750 ( 66.7%) | Loss: 0.4205 (avg: 0.4339) | Acc: 0.9062 (avg: 0.9140) | LR: 1.77e-03
  Batch  550/750 ( 73.3%) | Loss: 0.4240 (avg: 0.4367) | Acc: 0.9219 (avg: 0.9128) | LR: 1.84e-03
  Batch  600/750 ( 80.0%) | Loss: 0.5451 (avg: 0.4391) | Acc: 0.8438 (avg: 0.9116) | LR: 1.91e-03
  Batch  650/750 ( 86.7%) | Loss: 0.3865 (avg: 0.4407) | Acc: 0.9141 (avg: 0.9111) | LR: 1.98e-03
  Batch  700/750 ( 93.3%) | Loss: 0.4501 (avg: 0.4413) | Acc: 0.8984 (avg: 0.9107) | LR: 2.04e-03
  Batch  750/750 (100.0%) | Loss: 0.4119 (avg: 0.4424) | Acc: 0.9297 (avg: 0.9102) | LR: 2.11e-03

Train Summary:
  Final Loss: 0.4424 | Final Acc: 0.9102
  Final LR: 2.11e-03
  Loss std: 0.0551 | Acc std: 0.0280

Val Epoch 3 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4797 | Final Acc: 0.8977
  Loss std: 0.0579 | Acc std: 0.0263
Epoch 03/20 | train 0.4424/0.9102 | val 0.4797/0.8977

Train Epoch 4 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.3588 (avg: 0.3635) | Acc: 0.9688 (avg: 0.9472) | LR: 2.18e-03
  Batch  100/750 ( 13.3%) | Loss: 0.4076 (avg: 0.3705) | Acc: 0.9297 (avg: 0.9436) | LR: 2.25e-03
  Batch  150/750 ( 20.0%) | Loss: 0.3696 (avg: 0.3708) | Acc: 0.9453 (avg: 0.9432) | LR: 2.32e-03
  Batch  200/750 ( 26.7%) | Loss: 0.4166 (avg: 0.3729) | Acc: 0.8906 (avg: 0.9423) | LR: 2.38e-03
  Batch  250/750 ( 33.3%) | Loss: 0.4017 (avg: 0.3780) | Acc: 0.9375 (avg: 0.9403) | LR: 2.45e-03
  Batch  300/750 ( 40.0%) | Loss: 0.4870 (avg: 0.3824) | Acc: 0.9375 (avg: 0.9385) | LR: 2.52e-03
  Batch  350/750 ( 46.7%) | Loss: 0.3932 (avg: 0.3852) | Acc: 0.9375 (avg: 0.9373) | LR: 2.58e-03
  Batch  400/750 ( 53.3%) | Loss: 0.5028 (avg: 0.3906) | Acc: 0.8672 (avg: 0.9351) | LR: 2.65e-03
  Batch  450/750 ( 60.0%) | Loss: 0.3548 (avg: 0.3939) | Acc: 0.9531 (avg: 0.9338) | LR: 2.71e-03
  Batch  500/750 ( 66.7%) | Loss: 0.4407 (avg: 0.3966) | Acc: 0.9297 (avg: 0.9327) | LR: 2.78e-03
  Batch  550/750 ( 73.3%) | Loss: 0.4434 (avg: 0.3983) | Acc: 0.9219 (avg: 0.9318) | LR: 2.84e-03
  Batch  600/750 ( 80.0%) | Loss: 0.3405 (avg: 0.3995) | Acc: 0.9453 (avg: 0.9313) | LR: 2.90e-03
  Batch  650/750 ( 86.7%) | Loss: 0.4596 (avg: 0.4024) | Acc: 0.8984 (avg: 0.9301) | LR: 2.97e-03
  Batch  700/750 ( 93.3%) | Loss: 0.4827 (avg: 0.4042) | Acc: 0.8906 (avg: 0.9292) | LR: 3.03e-03
  Batch  750/750 (100.0%) | Loss: 0.3507 (avg: 0.4055) | Acc: 0.9609 (avg: 0.9285) | LR: 3.09e-03

Train Summary:
  Final Loss: 0.4055 | Final Acc: 0.9285
  Final LR: 3.09e-03
  Loss std: 0.0516 | Acc std: 0.0266

Val Epoch 4 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4834 | Final Acc: 0.8982
  Loss std: 0.0637 | Acc std: 0.0276
Epoch 04/20 | train 0.4055/0.9285 | val 0.4834/0.8982

Train Epoch 5 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.3060 (avg: 0.3393) | Acc: 0.9766 (avg: 0.9602) | LR: 3.14e-03
  Batch  100/750 ( 13.3%) | Loss: 0.2834 (avg: 0.3456) | Acc: 0.9922 (avg: 0.9574) | LR: 3.20e-03
  Batch  150/750 ( 20.0%) | Loss: 0.3168 (avg: 0.3522) | Acc: 0.9609 (avg: 0.9535) | LR: 3.26e-03
  Batch  200/750 ( 26.7%) | Loss: 0.3837 (avg: 0.3559) | Acc: 0.9297 (avg: 0.9520) | LR: 3.31e-03
  Batch  250/750 ( 33.3%) | Loss: 0.3820 (avg: 0.3590) | Acc: 0.9453 (avg: 0.9506) | LR: 3.36e-03
  Batch  300/750 ( 40.0%) | Loss: 0.3999 (avg: 0.3602) | Acc: 0.8984 (avg: 0.9498) | LR: 3.42e-03
  Batch  350/750 ( 46.7%) | Loss: 0.3469 (avg: 0.3613) | Acc: 0.9609 (avg: 0.9499) | LR: 3.47e-03
  Batch  400/750 ( 53.3%) | Loss: 0.3574 (avg: 0.3635) | Acc: 0.9453 (avg: 0.9487) | LR: 3.51e-03
  Batch  450/750 ( 60.0%) | Loss: 0.4116 (avg: 0.3665) | Acc: 0.9453 (avg: 0.9476) | LR: 3.56e-03
  Batch  500/750 ( 66.7%) | Loss: 0.4185 (avg: 0.3689) | Acc: 0.9453 (avg: 0.9464) | LR: 3.61e-03
  Batch  550/750 ( 73.3%) | Loss: 0.4250 (avg: 0.3715) | Acc: 0.9375 (avg: 0.9452) | LR: 3.65e-03
  Batch  600/750 ( 80.0%) | Loss: 0.4286 (avg: 0.3742) | Acc: 0.9219 (avg: 0.9439) | LR: 3.69e-03
  Batch  650/750 ( 86.7%) | Loss: 0.3938 (avg: 0.3750) | Acc: 0.9375 (avg: 0.9431) | LR: 3.73e-03
  Batch  700/750 ( 93.3%) | Loss: 0.3892 (avg: 0.3780) | Acc: 0.9375 (avg: 0.9420) | LR: 3.76e-03
  Batch  750/750 (100.0%) | Loss: 0.3741 (avg: 0.3801) | Acc: 0.9531 (avg: 0.9410) | LR: 3.80e-03

Train Summary:
  Final Loss: 0.3801 | Final Acc: 0.9410
  Final LR: 3.80e-03
  Loss std: 0.0513 | Acc std: 0.0263

Val Epoch 5 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5742 | Final Acc: 0.8688
  Loss std: 0.0755 | Acc std: 0.0272
Epoch 05/20 | train 0.3801/0.9410 | val 0.5742/0.8688

Train Epoch 6 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.3311 (avg: 0.3343) | Acc: 0.9609 (avg: 0.9641) | LR: 3.83e-03
  Batch  100/750 ( 13.3%) | Loss: 0.2981 (avg: 0.3279) | Acc: 0.9766 (avg: 0.9660) | LR: 3.86e-03
  Batch  150/750 ( 20.0%) | Loss: 0.3346 (avg: 0.3280) | Acc: 0.9609 (avg: 0.9650) | LR: 3.89e-03
  Batch  200/750 ( 26.7%) | Loss: 0.4689 (avg: 0.3319) | Acc: 0.9141 (avg: 0.9635) | LR: 3.92e-03
  Batch  250/750 ( 33.3%) | Loss: 0.3086 (avg: 0.3381) | Acc: 0.9844 (avg: 0.9615) | LR: 3.94e-03
  Batch  300/750 ( 40.0%) | Loss: 0.4079 (avg: 0.3416) | Acc: 0.9375 (avg: 0.9596) | LR: 3.97e-03
  Batch  350/750 ( 46.7%) | Loss: 0.3194 (avg: 0.3441) | Acc: 0.9844 (avg: 0.9585) | LR: 3.99e-03
  Batch  400/750 ( 53.3%) | Loss: 0.5654 (avg: 0.3490) | Acc: 0.8750 (avg: 0.9564) | LR: 4.00e-03
  Batch  450/750 ( 60.0%) | Loss: 0.3583 (avg: 0.3526) | Acc: 0.9531 (avg: 0.9548) | LR: 4.02e-03
  Batch  500/750 ( 66.7%) | Loss: 0.3316 (avg: 0.3526) | Acc: 0.9688 (avg: 0.9549) | LR: 4.03e-03
  Batch  550/750 ( 73.3%) | Loss: 0.4052 (avg: 0.3537) | Acc: 0.9219 (avg: 0.9542) | LR: 4.04e-03
  Batch  600/750 ( 80.0%) | Loss: 0.4287 (avg: 0.3555) | Acc: 0.9297 (avg: 0.9534) | LR: 4.05e-03
  Batch  650/750 ( 86.7%) | Loss: 0.3788 (avg: 0.3579) | Acc: 0.9609 (avg: 0.9525) | LR: 4.06e-03
  Batch  700/750 ( 93.3%) | Loss: 0.3517 (avg: 0.3596) | Acc: 0.9453 (avg: 0.9518) | LR: 4.06e-03
  Batch  750/750 (100.0%) | Loss: 0.4356 (avg: 0.3610) | Acc: 0.9375 (avg: 0.9511) | LR: 4.06e-03

Train Summary:
  Final Loss: 0.3610 | Final Acc: 0.9511
  Final LR: 4.06e-03
  Loss std: 0.0566 | Acc std: 0.0272

Val Epoch 6 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5701 | Final Acc: 0.8650
  Loss std: 0.0665 | Acc std: 0.0297
Epoch 06/20 | train 0.3610/0.9511 | val 0.5701/0.8650

Train Epoch 7 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.3546 (avg: 0.3244) | Acc: 0.9531 (avg: 0.9658) | LR: 4.06e-03
  Batch  100/750 ( 13.3%) | Loss: 0.2745 (avg: 0.3224) | Acc: 0.9922 (avg: 0.9685) | LR: 4.06e-03
  Batch  150/750 ( 20.0%) | Loss: 0.3286 (avg: 0.3235) | Acc: 0.9531 (avg: 0.9690) | LR: 4.06e-03
  Batch  200/750 ( 26.7%) | Loss: 0.3272 (avg: 0.3264) | Acc: 0.9609 (avg: 0.9676) | LR: 4.06e-03
  Batch  250/750 ( 33.3%) | Loss: 0.3072 (avg: 0.3295) | Acc: 0.9766 (avg: 0.9663) | LR: 4.05e-03
  Batch  300/750 ( 40.0%) | Loss: 0.4796 (avg: 0.3293) | Acc: 0.8750 (avg: 0.9666) | LR: 4.05e-03
  Batch  350/750 ( 46.7%) | Loss: 0.2742 (avg: 0.3299) | Acc: 1.0000 (avg: 0.9664) | LR: 4.05e-03
  Batch  400/750 ( 53.3%) | Loss: 0.2942 (avg: 0.3337) | Acc: 0.9766 (avg: 0.9649) | LR: 4.05e-03
  Batch  450/750 ( 60.0%) | Loss: 0.3261 (avg: 0.3344) | Acc: 0.9688 (avg: 0.9642) | LR: 4.04e-03
  Batch  500/750 ( 66.7%) | Loss: 0.3226 (avg: 0.3347) | Acc: 0.9766 (avg: 0.9638) | LR: 4.04e-03
  Batch  550/750 ( 73.3%) | Loss: 0.3045 (avg: 0.3365) | Acc: 0.9844 (avg: 0.9631) | LR: 4.03e-03
  Batch  600/750 ( 80.0%) | Loss: 0.3179 (avg: 0.3374) | Acc: 0.9766 (avg: 0.9628) | LR: 4.03e-03
  Batch  650/750 ( 86.7%) | Loss: 0.3231 (avg: 0.3395) | Acc: 0.9688 (avg: 0.9618) | LR: 4.02e-03
  Batch  700/750 ( 93.3%) | Loss: 0.3106 (avg: 0.3404) | Acc: 0.9688 (avg: 0.9613) | LR: 4.02e-03
  Batch  750/750 (100.0%) | Loss: 0.3528 (avg: 0.3419) | Acc: 0.9688 (avg: 0.9607) | LR: 4.01e-03

Train Summary:
  Final Loss: 0.3419 | Final Acc: 0.9607
  Final LR: 4.01e-03
  Loss std: 0.0493 | Acc std: 0.0231

Val Epoch 7 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4768 | Final Acc: 0.9059
  Loss std: 0.0647 | Acc std: 0.0244
Epoch 07/20 | train 0.3419/0.9607 | val 0.4768/0.9059

Train Epoch 8 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.3300 (avg: 0.2941) | Acc: 0.9688 (avg: 0.9789) | LR: 4.00e-03
  Batch  100/750 ( 13.3%) | Loss: 0.2713 (avg: 0.2977) | Acc: 0.9922 (avg: 0.9785) | LR: 3.99e-03
  Batch  150/750 ( 20.0%) | Loss: 0.2633 (avg: 0.3000) | Acc: 1.0000 (avg: 0.9782) | LR: 3.99e-03
  Batch  200/750 ( 26.7%) | Loss: 0.2964 (avg: 0.3007) | Acc: 0.9766 (avg: 0.9777) | LR: 3.98e-03
  Batch  250/750 ( 33.3%) | Loss: 0.3128 (avg: 0.3004) | Acc: 0.9766 (avg: 0.9781) | LR: 3.97e-03
  Batch  300/750 ( 40.0%) | Loss: 0.3326 (avg: 0.3027) | Acc: 0.9531 (avg: 0.9775) | LR: 3.96e-03
  Batch  350/750 ( 46.7%) | Loss: 0.3649 (avg: 0.3060) | Acc: 0.9453 (avg: 0.9761) | LR: 3.95e-03
  Batch  400/750 ( 53.3%) | Loss: 0.3292 (avg: 0.3077) | Acc: 0.9766 (avg: 0.9755) | LR: 3.94e-03
  Batch  450/750 ( 60.0%) | Loss: 0.3405 (avg: 0.3086) | Acc: 0.9375 (avg: 0.9750) | LR: 3.93e-03
  Batch  500/750 ( 66.7%) | Loss: 0.2830 (avg: 0.3104) | Acc: 0.9844 (avg: 0.9744) | LR: 3.92e-03
  Batch  550/750 ( 73.3%) | Loss: 0.3206 (avg: 0.3110) | Acc: 0.9688 (avg: 0.9740) | LR: 3.91e-03
  Batch  600/750 ( 80.0%) | Loss: 0.3421 (avg: 0.3132) | Acc: 0.9609 (avg: 0.9732) | LR: 3.90e-03
  Batch  650/750 ( 86.7%) | Loss: 0.3713 (avg: 0.3144) | Acc: 0.9609 (avg: 0.9727) | LR: 3.88e-03
  Batch  700/750 ( 93.3%) | Loss: 0.4178 (avg: 0.3163) | Acc: 0.9219 (avg: 0.9720) | LR: 3.87e-03
  Batch  750/750 (100.0%) | Loss: 0.3444 (avg: 0.3183) | Acc: 0.9609 (avg: 0.9711) | LR: 3.86e-03

Train Summary:
  Final Loss: 0.3183 | Final Acc: 0.9711
  Final LR: 3.86e-03
  Loss std: 0.0388 | Acc std: 0.0175

Val Epoch 8 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4820 | Final Acc: 0.9042
  Loss std: 0.0673 | Acc std: 0.0264
Epoch 08/20 | train 0.3183/0.9711 | val 0.4820/0.9042

Train Epoch 9 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.2818 (avg: 0.2842) | Acc: 0.9922 (avg: 0.9858) | LR: 3.85e-03
  Batch  100/750 ( 13.3%) | Loss: 0.2908 (avg: 0.2871) | Acc: 0.9766 (avg: 0.9848) | LR: 3.83e-03
  Batch  150/750 ( 20.0%) | Loss: 0.2794 (avg: 0.2866) | Acc: 0.9766 (avg: 0.9848) | LR: 3.82e-03
  Batch  200/750 ( 26.7%) | Loss: 0.2754 (avg: 0.2854) | Acc: 1.0000 (avg: 0.9851) | LR: 3.80e-03
  Batch  250/750 ( 33.3%) | Loss: 0.3238 (avg: 0.2885) | Acc: 0.9688 (avg: 0.9839) | LR: 3.79e-03
  Batch  300/750 ( 40.0%) | Loss: 0.2834 (avg: 0.2903) | Acc: 0.9766 (avg: 0.9837) | LR: 3.77e-03
  Batch  350/750 ( 46.7%) | Loss: 0.2770 (avg: 0.2917) | Acc: 0.9922 (avg: 0.9832) | LR: 3.76e-03
  Batch  400/750 ( 53.3%) | Loss: 0.3237 (avg: 0.2942) | Acc: 0.9766 (avg: 0.9819) | LR: 3.74e-03
  Batch  450/750 ( 60.0%) | Loss: 0.3090 (avg: 0.2955) | Acc: 0.9844 (avg: 0.9814) | LR: 3.72e-03
  Batch  500/750 ( 66.7%) | Loss: 0.3174 (avg: 0.2963) | Acc: 0.9688 (avg: 0.9810) | LR: 3.71e-03
  Batch  550/750 ( 73.3%) | Loss: 0.2683 (avg: 0.2969) | Acc: 0.9922 (avg: 0.9808) | LR: 3.69e-03
  Batch  600/750 ( 80.0%) | Loss: 0.2991 (avg: 0.2978) | Acc: 0.9844 (avg: 0.9804) | LR: 3.67e-03
  Batch  650/750 ( 86.7%) | Loss: 0.3592 (avg: 0.2990) | Acc: 0.9609 (avg: 0.9799) | LR: 3.65e-03
  Batch  700/750 ( 93.3%) | Loss: 0.2931 (avg: 0.2999) | Acc: 0.9844 (avg: 0.9795) | LR: 3.64e-03
  Batch  750/750 (100.0%) | Loss: 0.2888 (avg: 0.3013) | Acc: 0.9766 (avg: 0.9788) | LR: 3.62e-03

Train Summary:
  Final Loss: 0.3013 | Final Acc: 0.9788
  Final LR: 3.62e-03
  Loss std: 0.0298 | Acc std: 0.0142

Val Epoch 9 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5310 | Final Acc: 0.8843
  Loss std: 0.0687 | Acc std: 0.0271
Epoch 09/20 | train 0.3013/0.9788 | val 0.5310/0.8843

Train Epoch 10 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.2604 (avg: 0.2753) | Acc: 0.9922 (avg: 0.9897) | LR: 3.60e-03
  Batch  100/750 ( 13.3%) | Loss: 0.2552 (avg: 0.2735) | Acc: 1.0000 (avg: 0.9902) | LR: 3.58e-03
  Batch  150/750 ( 20.0%) | Loss: 0.2720 (avg: 0.2714) | Acc: 0.9844 (avg: 0.9905) | LR: 3.56e-03
  Batch  200/750 ( 26.7%) | Loss: 0.2724 (avg: 0.2728) | Acc: 0.9922 (avg: 0.9897) | LR: 3.54e-03
  Batch  250/750 ( 33.3%) | Loss: 0.2958 (avg: 0.2742) | Acc: 0.9844 (avg: 0.9886) | LR: 3.52e-03
  Batch  300/750 ( 40.0%) | Loss: 0.2769 (avg: 0.2759) | Acc: 0.9922 (avg: 0.9879) | LR: 3.50e-03
  Batch  350/750 ( 46.7%) | Loss: 0.2581 (avg: 0.2769) | Acc: 0.9922 (avg: 0.9877) | LR: 3.48e-03
  Batch  400/750 ( 53.3%) | Loss: 0.2902 (avg: 0.2775) | Acc: 0.9844 (avg: 0.9877) | LR: 3.45e-03
  Batch  450/750 ( 60.0%) | Loss: 0.2653 (avg: 0.2784) | Acc: 1.0000 (avg: 0.9872) | LR: 3.43e-03
  Batch  500/750 ( 66.7%) | Loss: 0.2797 (avg: 0.2794) | Acc: 0.9844 (avg: 0.9868) | LR: 3.41e-03
  Batch  550/750 ( 73.3%) | Loss: 0.3022 (avg: 0.2800) | Acc: 0.9766 (avg: 0.9866) | LR: 3.39e-03
  Batch  600/750 ( 80.0%) | Loss: 0.2899 (avg: 0.2805) | Acc: 0.9766 (avg: 0.9864) | LR: 3.37e-03
  Batch  650/750 ( 86.7%) | Loss: 0.2996 (avg: 0.2813) | Acc: 0.9688 (avg: 0.9860) | LR: 3.34e-03
  Batch  700/750 ( 93.3%) | Loss: 0.3204 (avg: 0.2821) | Acc: 0.9609 (avg: 0.9856) | LR: 3.32e-03
  Batch  750/750 (100.0%) | Loss: 0.2664 (avg: 0.2826) | Acc: 0.9922 (avg: 0.9853) | LR: 3.30e-03

Train Summary:
  Final Loss: 0.2826 | Final Acc: 0.9853
  Final LR: 3.30e-03
  Loss std: 0.0243 | Acc std: 0.0118

Val Epoch 10 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5307 | Final Acc: 0.8910
  Loss std: 0.0756 | Acc std: 0.0272
Epoch 10/20 | train 0.2826/0.9853 | val 0.5307/0.8910
Early stopping at epoch 10
wandb:
wandb: Run history:
wandb:               batch ‚ñÉ‚ñÜ‚ñá‚ñá‚ñà‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñá‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñÉ‚ñÉ‚ñÑ‚ñá‚ñà‚ñÅ‚ñÅ‚ñÖ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñá‚ñÉ‚ñÜ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñà
wandb:          best_epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñà
wandb:        best_val_acc ‚ñÅ‚ñÖ‚ñá‚ñá‚ñà
wandb:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà
wandb:    final_best_epoch ‚ñÅ
wandb:  final_best_val_acc ‚ñÅ
wandb: final_learning_rate ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá
wandb:      final_val_loss ‚ñÅ
wandb:       learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá
wandb:           train_acc ‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:                 +15 ...
wandb:
wandb: Run summary:
wandb:               batch 750
wandb:          best_epoch 7
wandb:        best_val_acc 0.90592
wandb:               epoch 10
wandb:    final_best_epoch 7
wandb:  final_best_val_acc 0.90592
wandb: final_learning_rate 0.0033
wandb:      final_val_loss 0.53068
wandb:       learning_rate 0.0033
wandb:              status completed
wandb:                 +16 ...
wandb:
wandb: üöÄ View run trial_002 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/91t80ols
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_055911-91t80ols/logs
[I 2025-10-06 06:10:31,765] Trial 2 finished with value: 0.9059166666666667 and parameters: {'emb_dim': 200, 'channels': 128, 'kernel_sizes': (3, 4, 5), 'dropout': 0.1, 'lr': 0.004060333825822455, 'weight_decay': 0.00021360863718783297, 'batch_size': 128, 'max_len': 160, 'grad_clip': 0.5, 'optimizer': 'adamw'}. Best is trial 0 with value: 0.9144583333333334.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_061031-3y1kbp9x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lilac-valley-8
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/3y1kbp9x
wandb: Finishing previous runs because reinit is set to True.
wandb:
wandb: Run history:
wandb:   batch_size ‚ñÅ
wandb:     channels ‚ñÅ
wandb:      dropout ‚ñÅ
wandb:      emb_dim ‚ñÅ
wandb:    grad_clip ‚ñÅ
wandb:           lr ‚ñÅ
wandb:      max_len ‚ñÅ
wandb:      val_acc ‚ñÅ
wandb: weight_decay ‚ñÅ
wandb:
wandb: Run summary:
wandb:   batch_size 128
wandb:     channels 128
wandb:      dropout 0.1
wandb:      emb_dim 200
wandb:    grad_clip 0.5
wandb:           lr 0.00406
wandb:      max_len 160
wandb:    optimizer adamw
wandb:      val_acc 0.90592
wandb: weight_decay 0.00021
wandb:
wandb: üöÄ View run trial/2/lilac-valley-8 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/3y1kbp9x
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_061031-3y1kbp9x/logs
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_061032-e6uuy01g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_003
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/e6uuy01g

Train Epoch 1 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 1.3845 (avg: 1.4910) | Acc: 0.3477 (avg: 0.2859) | LR: 8.16e-05
  Batch  100/375 ( 26.7%) | Loss: 1.3676 (avg: 1.4259) | Acc: 0.3750 (avg: 0.3254) | LR: 8.86e-05
  Batch  150/375 ( 40.0%) | Loss: 1.1996 (avg: 1.3687) | Acc: 0.4844 (avg: 0.3641) | LR: 1.00e-04
  Batch  200/375 ( 53.3%) | Loss: 1.1043 (avg: 1.3143) | Acc: 0.5391 (avg: 0.4043) | LR: 1.16e-04
  Batch  250/375 ( 66.7%) | Loss: 1.0554 (avg: 1.2609) | Acc: 0.5820 (avg: 0.4404) | LR: 1.37e-04
  Batch  300/375 ( 80.0%) | Loss: 0.9326 (avg: 1.2111) | Acc: 0.6641 (avg: 0.4731) | LR: 1.62e-04
  Batch  350/375 ( 93.3%) | Loss: 0.7994 (avg: 1.1632) | Acc: 0.7461 (avg: 0.5046) | LR: 1.91e-04

Train Summary:
  Final Loss: 1.1417 | Final Acc: 0.5181
  Final LR: 2.06e-04
  Loss std: 0.2186 | Acc std: 0.1464

Val Epoch 1 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.7098 | Final Acc: 0.8059
  Loss std: 0.0335 | Acc std: 0.0242
Epoch 01/20 | train 1.1417/0.5181 | val 0.7098/0.8059

Train Epoch 2 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.7015 (avg: 0.7806) | Acc: 0.7852 (avg: 0.7411) | LR: 2.42e-04
  Batch  100/375 ( 26.7%) | Loss: 0.7543 (avg: 0.7551) | Acc: 0.7852 (avg: 0.7529) | LR: 2.81e-04
  Batch  150/375 ( 40.0%) | Loss: 0.7166 (avg: 0.7393) | Acc: 0.7852 (avg: 0.7628) | LR: 3.24e-04
  Batch  200/375 ( 53.3%) | Loss: 0.6938 (avg: 0.7244) | Acc: 0.7891 (avg: 0.7717) | LR: 3.70e-04
  Batch  250/375 ( 66.7%) | Loss: 0.5836 (avg: 0.7093) | Acc: 0.8438 (avg: 0.7788) | LR: 4.19e-04
  Batch  300/375 ( 80.0%) | Loss: 0.6103 (avg: 0.6987) | Acc: 0.7852 (avg: 0.7845) | LR: 4.72e-04
  Batch  350/375 ( 93.3%) | Loss: 0.5427 (avg: 0.6893) | Acc: 0.8555 (avg: 0.7893) | LR: 5.27e-04

Train Summary:
  Final Loss: 0.6847 | Final Acc: 0.7912
  Final LR: 5.54e-04
  Loss std: 0.0676 | Acc std: 0.0366

Val Epoch 2 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5369 | Final Acc: 0.8725
  Loss std: 0.0343 | Acc std: 0.0197
Epoch 02/20 | train 0.6847/0.7912 | val 0.5369/0.8725

Train Epoch 3 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.5584 (avg: 0.5541) | Acc: 0.8516 (avg: 0.8585) | LR: 6.14e-04
  Batch  100/375 ( 26.7%) | Loss: 0.6484 (avg: 0.5585) | Acc: 0.8125 (avg: 0.8545) | LR: 6.75e-04
  Batch  150/375 ( 40.0%) | Loss: 0.5487 (avg: 0.5613) | Acc: 0.8672 (avg: 0.8537) | LR: 7.37e-04
  Batch  200/375 ( 53.3%) | Loss: 0.5840 (avg: 0.5623) | Acc: 0.8438 (avg: 0.8536) | LR: 8.01e-04
  Batch  250/375 ( 66.7%) | Loss: 0.5486 (avg: 0.5622) | Acc: 0.8477 (avg: 0.8536) | LR: 8.66e-04
  Batch  300/375 ( 80.0%) | Loss: 0.5007 (avg: 0.5615) | Acc: 0.8828 (avg: 0.8539) | LR: 9.32e-04
  Batch  350/375 ( 93.3%) | Loss: 0.5233 (avg: 0.5604) | Acc: 0.8906 (avg: 0.8548) | LR: 9.98e-04

Train Summary:
  Final Loss: 0.5601 | Final Acc: 0.8546
  Final LR: 1.03e-03
  Loss std: 0.0395 | Acc std: 0.0229

Val Epoch 3 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4923 | Final Acc: 0.8912
  Loss std: 0.0335 | Acc std: 0.0191
Epoch 03/20 | train 0.5601/0.8546 | val 0.4923/0.8912

Train Epoch 4 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.5083 (avg: 0.4912) | Acc: 0.8867 (avg: 0.8844) | LR: 1.10e-03
  Batch  100/375 ( 26.7%) | Loss: 0.5249 (avg: 0.4934) | Acc: 0.8750 (avg: 0.8854) | LR: 1.16e-03
  Batch  150/375 ( 40.0%) | Loss: 0.4511 (avg: 0.4977) | Acc: 0.9180 (avg: 0.8835) | LR: 1.23e-03
  Batch  200/375 ( 53.3%) | Loss: 0.5560 (avg: 0.5001) | Acc: 0.8672 (avg: 0.8834) | LR: 1.29e-03
  Batch  250/375 ( 66.7%) | Loss: 0.5048 (avg: 0.5010) | Acc: 0.8633 (avg: 0.8830) | LR: 1.36e-03
  Batch  300/375 ( 80.0%) | Loss: 0.5332 (avg: 0.5025) | Acc: 0.8672 (avg: 0.8819) | LR: 1.42e-03
  Batch  350/375 ( 93.3%) | Loss: 0.4994 (avg: 0.5022) | Acc: 0.8672 (avg: 0.8820) | LR: 1.48e-03

Train Summary:
  Final Loss: 0.5023 | Final Acc: 0.8818
  Final LR: 1.51e-03
  Loss std: 0.0365 | Acc std: 0.0191

Val Epoch 4 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4732 | Final Acc: 0.8972
  Loss std: 0.0358 | Acc std: 0.0176
Epoch 04/20 | train 0.5023/0.8818 | val 0.4732/0.8972

Train Epoch 5 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.4040 (avg: 0.4455) | Acc: 0.9297 (avg: 0.9106) | LR: 1.56e-03
  Batch  100/375 ( 26.7%) | Loss: 0.4214 (avg: 0.4410) | Acc: 0.9023 (avg: 0.9128) | LR: 1.62e-03
  Batch  150/375 ( 40.0%) | Loss: 0.5131 (avg: 0.4449) | Acc: 0.8906 (avg: 0.9105) | LR: 1.67e-03
  Batch  200/375 ( 53.3%) | Loss: 0.5084 (avg: 0.4460) | Acc: 0.8789 (avg: 0.9099) | LR: 1.72e-03
  Batch  250/375 ( 66.7%) | Loss: 0.4635 (avg: 0.4486) | Acc: 0.9062 (avg: 0.9080) | LR: 1.76e-03
  Batch  300/375 ( 80.0%) | Loss: 0.4566 (avg: 0.4493) | Acc: 0.8906 (avg: 0.9068) | LR: 1.80e-03
  Batch  350/375 ( 93.3%) | Loss: 0.4769 (avg: 0.4500) | Acc: 0.8945 (avg: 0.9063) | LR: 1.84e-03

Train Summary:
  Final Loss: 0.4501 | Final Acc: 0.9063
  Final LR: 1.85e-03
  Loss std: 0.0346 | Acc std: 0.0189

Val Epoch 5 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4417 | Final Acc: 0.9079
  Loss std: 0.0344 | Acc std: 0.0189
Epoch 05/20 | train 0.4501/0.9063 | val 0.4417/0.9079

Train Epoch 6 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.4053 (avg: 0.3845) | Acc: 0.9297 (avg: 0.9365) | LR: 1.89e-03
  Batch  100/375 ( 26.7%) | Loss: 0.3385 (avg: 0.3864) | Acc: 0.9492 (avg: 0.9360) | LR: 1.91e-03
  Batch  150/375 ( 40.0%) | Loss: 0.3759 (avg: 0.3898) | Acc: 0.9336 (avg: 0.9340) | LR: 1.94e-03
  Batch  200/375 ( 53.3%) | Loss: 0.4050 (avg: 0.3936) | Acc: 0.9258 (avg: 0.9314) | LR: 1.95e-03
  Batch  250/375 ( 66.7%) | Loss: 0.4200 (avg: 0.3964) | Acc: 0.9219 (avg: 0.9295) | LR: 1.97e-03
  Batch  300/375 ( 80.0%) | Loss: 0.4288 (avg: 0.3985) | Acc: 0.9180 (avg: 0.9282) | LR: 1.98e-03
  Batch  350/375 ( 93.3%) | Loss: 0.4484 (avg: 0.4009) | Acc: 0.9023 (avg: 0.9275) | LR: 1.98e-03

Train Summary:
  Final Loss: 0.4016 | Final Acc: 0.9272
  Final LR: 1.98e-03
  Loss std: 0.0307 | Acc std: 0.0172

Val Epoch 6 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4287 | Final Acc: 0.9136
  Loss std: 0.0363 | Acc std: 0.0181
Epoch 06/20 | train 0.4016/0.9272 | val 0.4287/0.9136

Train Epoch 7 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.3479 (avg: 0.3435) | Acc: 0.9531 (avg: 0.9557) | LR: 1.98e-03
  Batch  100/375 ( 26.7%) | Loss: 0.3234 (avg: 0.3465) | Acc: 0.9648 (avg: 0.9536) | LR: 1.98e-03
  Batch  150/375 ( 40.0%) | Loss: 0.4178 (avg: 0.3528) | Acc: 0.9258 (avg: 0.9497) | LR: 1.98e-03
  Batch  200/375 ( 53.3%) | Loss: 0.4014 (avg: 0.3571) | Acc: 0.9102 (avg: 0.9472) | LR: 1.98e-03
  Batch  250/375 ( 66.7%) | Loss: 0.3653 (avg: 0.3628) | Acc: 0.9453 (avg: 0.9447) | LR: 1.97e-03
  Batch  300/375 ( 80.0%) | Loss: 0.4189 (avg: 0.3657) | Acc: 0.9297 (avg: 0.9433) | LR: 1.97e-03
  Batch  350/375 ( 93.3%) | Loss: 0.4253 (avg: 0.3696) | Acc: 0.9180 (avg: 0.9413) | LR: 1.96e-03

Train Summary:
  Final Loss: 0.3714 | Final Acc: 0.9405
  Final LR: 1.96e-03
  Loss std: 0.0306 | Acc std: 0.0169

Val Epoch 7 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4261 | Final Acc: 0.9160
  Loss std: 0.0364 | Acc std: 0.0178
Epoch 07/20 | train 0.3714/0.9405 | val 0.4261/0.9160

Train Epoch 8 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.3407 (avg: 0.3234) | Acc: 0.9492 (avg: 0.9637) | LR: 1.95e-03
  Batch  100/375 ( 26.7%) | Loss: 0.3271 (avg: 0.3268) | Acc: 0.9648 (avg: 0.9623) | LR: 1.94e-03
  Batch  150/375 ( 40.0%) | Loss: 0.3123 (avg: 0.3303) | Acc: 0.9609 (avg: 0.9604) | LR: 1.93e-03
  Batch  200/375 ( 53.3%) | Loss: 0.3336 (avg: 0.3339) | Acc: 0.9531 (avg: 0.9585) | LR: 1.92e-03
  Batch  250/375 ( 66.7%) | Loss: 0.3550 (avg: 0.3399) | Acc: 0.9648 (avg: 0.9558) | LR: 1.91e-03
  Batch  300/375 ( 80.0%) | Loss: 0.3574 (avg: 0.3439) | Acc: 0.9375 (avg: 0.9540) | LR: 1.90e-03
  Batch  350/375 ( 93.3%) | Loss: 0.3554 (avg: 0.3482) | Acc: 0.9531 (avg: 0.9519) | LR: 1.89e-03

Train Summary:
  Final Loss: 0.3500 | Final Acc: 0.9509
  Final LR: 1.88e-03
  Loss std: 0.0297 | Acc std: 0.0166

Val Epoch 8 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4268 | Final Acc: 0.9173
  Loss std: 0.0388 | Acc std: 0.0169
Epoch 08/20 | train 0.3500/0.9509 | val 0.4268/0.9173

Train Epoch 9 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.3191 (avg: 0.3158) | Acc: 0.9648 (avg: 0.9679) | LR: 1.87e-03
  Batch  100/375 ( 26.7%) | Loss: 0.3509 (avg: 0.3150) | Acc: 0.9414 (avg: 0.9680) | LR: 1.86e-03
  Batch  150/375 ( 40.0%) | Loss: 0.3160 (avg: 0.3189) | Acc: 0.9688 (avg: 0.9656) | LR: 1.84e-03
  Batch  200/375 ( 53.3%) | Loss: 0.3191 (avg: 0.3211) | Acc: 0.9688 (avg: 0.9643) | LR: 1.83e-03
  Batch  250/375 ( 66.7%) | Loss: 0.3852 (avg: 0.3242) | Acc: 0.9258 (avg: 0.9630) | LR: 1.81e-03
  Batch  300/375 ( 80.0%) | Loss: 0.3423 (avg: 0.3276) | Acc: 0.9336 (avg: 0.9613) | LR: 1.79e-03
  Batch  350/375 ( 93.3%) | Loss: 0.3299 (avg: 0.3311) | Acc: 0.9609 (avg: 0.9596) | LR: 1.77e-03

Train Summary:
  Final Loss: 0.3323 | Final Acc: 0.9590
  Final LR: 1.77e-03
  Loss std: 0.0237 | Acc std: 0.0134

Val Epoch 9 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4221 | Final Acc: 0.9181
  Loss std: 0.0400 | Acc std: 0.0182
Epoch 09/20 | train 0.3323/0.9590 | val 0.4221/0.9181

Train Epoch 10 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.2856 (avg: 0.2912) | Acc: 0.9805 (avg: 0.9778) | LR: 1.75e-03
  Batch  100/375 ( 26.7%) | Loss: 0.3047 (avg: 0.2962) | Acc: 0.9688 (avg: 0.9760) | LR: 1.73e-03
  Batch  150/375 ( 40.0%) | Loss: 0.3068 (avg: 0.2994) | Acc: 0.9805 (avg: 0.9746) | LR: 1.71e-03
  Batch  200/375 ( 53.3%) | Loss: 0.2866 (avg: 0.3021) | Acc: 0.9805 (avg: 0.9734) | LR: 1.69e-03
  Batch  250/375 ( 66.7%) | Loss: 0.3506 (avg: 0.3061) | Acc: 0.9414 (avg: 0.9716) | LR: 1.66e-03
  Batch  300/375 ( 80.0%) | Loss: 0.3017 (avg: 0.3090) | Acc: 0.9766 (avg: 0.9704) | LR: 1.64e-03
  Batch  350/375 ( 93.3%) | Loss: 0.3521 (avg: 0.3121) | Acc: 0.9414 (avg: 0.9684) | LR: 1.62e-03

Train Summary:
  Final Loss: 0.3140 | Final Acc: 0.9677
  Final LR: 1.61e-03
  Loss std: 0.0225 | Acc std: 0.0126

Val Epoch 10 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4264 | Final Acc: 0.9200
  Loss std: 0.0397 | Acc std: 0.0175
Epoch 10/20 | train 0.3140/0.9677 | val 0.4264/0.9200

Train Epoch 11 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.2723 (avg: 0.2811) | Acc: 0.9805 (avg: 0.9816) | LR: 1.59e-03
  Batch  100/375 ( 26.7%) | Loss: 0.2703 (avg: 0.2825) | Acc: 0.9922 (avg: 0.9807) | LR: 1.56e-03
  Batch  150/375 ( 40.0%) | Loss: 0.2999 (avg: 0.2855) | Acc: 0.9688 (avg: 0.9798) | LR: 1.54e-03
  Batch  200/375 ( 53.3%) | Loss: 0.2827 (avg: 0.2874) | Acc: 0.9766 (avg: 0.9795) | LR: 1.51e-03
  Batch  250/375 ( 66.7%) | Loss: 0.3176 (avg: 0.2901) | Acc: 0.9766 (avg: 0.9782) | LR: 1.49e-03
  Batch  300/375 ( 80.0%) | Loss: 0.3260 (avg: 0.2924) | Acc: 0.9688 (avg: 0.9774) | LR: 1.46e-03
  Batch  350/375 ( 93.3%) | Loss: 0.3243 (avg: 0.2948) | Acc: 0.9492 (avg: 0.9764) | LR: 1.43e-03

Train Summary:
  Final Loss: 0.2961 | Final Acc: 0.9757
  Final LR: 1.42e-03
  Loss std: 0.0178 | Acc std: 0.0104

Val Epoch 11 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4205 | Final Acc: 0.9196
  Loss std: 0.0389 | Acc std: 0.0178
Epoch 11/20 | train 0.2961/0.9757 | val 0.4205/0.9196

Train Epoch 12 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.2890 (avg: 0.2704) | Acc: 0.9844 (avg: 0.9879) | LR: 1.39e-03
  Batch  100/375 ( 26.7%) | Loss: 0.2671 (avg: 0.2711) | Acc: 0.9883 (avg: 0.9872) | LR: 1.37e-03
  Batch  150/375 ( 40.0%) | Loss: 0.2984 (avg: 0.2732) | Acc: 0.9648 (avg: 0.9863) | LR: 1.34e-03
  Batch  200/375 ( 53.3%) | Loss: 0.2712 (avg: 0.2743) | Acc: 0.9961 (avg: 0.9862) | LR: 1.31e-03
  Batch  250/375 ( 66.7%) | Loss: 0.2938 (avg: 0.2763) | Acc: 0.9688 (avg: 0.9849) | LR: 1.28e-03
  Batch  300/375 ( 80.0%) | Loss: 0.2705 (avg: 0.2776) | Acc: 0.9883 (avg: 0.9845) | LR: 1.25e-03
  Batch  350/375 ( 93.3%) | Loss: 0.2717 (avg: 0.2791) | Acc: 0.9883 (avg: 0.9838) | LR: 1.23e-03

Train Summary:
  Final Loss: 0.2801 | Final Acc: 0.9832
  Final LR: 1.21e-03
  Loss std: 0.0129 | Acc std: 0.0086

Val Epoch 12 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4194 | Final Acc: 0.9195
  Loss std: 0.0398 | Acc std: 0.0184
Epoch 12/20 | train 0.2801/0.9832 | val 0.4194/0.9195

Train Epoch 13 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.2530 (avg: 0.2561) | Acc: 0.9883 (avg: 0.9940) | LR: 1.18e-03
  Batch  100/375 ( 26.7%) | Loss: 0.2566 (avg: 0.2574) | Acc: 0.9883 (avg: 0.9934) | LR: 1.15e-03
  Batch  150/375 ( 40.0%) | Loss: 0.2671 (avg: 0.2597) | Acc: 0.9883 (avg: 0.9922) | LR: 1.12e-03
  Batch  200/375 ( 53.3%) | Loss: 0.2815 (avg: 0.2611) | Acc: 0.9883 (avg: 0.9916) | LR: 1.09e-03
  Batch  250/375 ( 66.7%) | Loss: 0.2614 (avg: 0.2625) | Acc: 0.9961 (avg: 0.9911) | LR: 1.06e-03
  Batch  300/375 ( 80.0%) | Loss: 0.2806 (avg: 0.2639) | Acc: 0.9844 (avg: 0.9904) | LR: 1.03e-03
  Batch  350/375 ( 93.3%) | Loss: 0.2705 (avg: 0.2653) | Acc: 0.9844 (avg: 0.9899) | LR: 1.01e-03

Train Summary:
  Final Loss: 0.2662 | Final Acc: 0.9896
  Final LR: 9.91e-04
  Loss std: 0.0109 | Acc std: 0.0066

Val Epoch 13 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4203 | Final Acc: 0.9209
  Loss std: 0.0409 | Acc std: 0.0172
Epoch 13/20 | train 0.2662/0.9896 | val 0.4203/0.9209

Train Epoch 14 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.2561 (avg: 0.2493) | Acc: 0.9922 (avg: 0.9944) | LR: 9.61e-04
  Batch  100/375 ( 26.7%) | Loss: 0.2555 (avg: 0.2500) | Acc: 0.9961 (avg: 0.9943) | LR: 9.31e-04
  Batch  150/375 ( 40.0%) | Loss: 0.2469 (avg: 0.2512) | Acc: 0.9922 (avg: 0.9937) | LR: 9.02e-04
  Batch  200/375 ( 53.3%) | Loss: 0.2529 (avg: 0.2524) | Acc: 0.9961 (avg: 0.9936) | LR: 8.72e-04
  Batch  250/375 ( 66.7%) | Loss: 0.2636 (avg: 0.2534) | Acc: 0.9922 (avg: 0.9934) | LR: 8.43e-04
  Batch  300/375 ( 80.0%) | Loss: 0.2665 (avg: 0.2543) | Acc: 0.9805 (avg: 0.9930) | LR: 8.14e-04
  Batch  350/375 ( 93.3%) | Loss: 0.2628 (avg: 0.2553) | Acc: 0.9883 (avg: 0.9928) | LR: 7.84e-04

Train Summary:
  Final Loss: 0.2559 | Final Acc: 0.9927
  Final LR: 7.71e-04
  Loss std: 0.0087 | Acc std: 0.0055

Val Epoch 14 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4184 | Final Acc: 0.9215
  Loss std: 0.0427 | Acc std: 0.0181
Epoch 14/20 | train 0.2559/0.9927 | val 0.4184/0.9215

Train Epoch 15 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.2368 (avg: 0.2420) | Acc: 1.0000 (avg: 0.9966) | LR: 7.41e-04
  Batch  100/375 ( 26.7%) | Loss: 0.2374 (avg: 0.2431) | Acc: 1.0000 (avg: 0.9968) | LR: 7.13e-04
  Batch  150/375 ( 40.0%) | Loss: 0.2449 (avg: 0.2437) | Acc: 0.9922 (avg: 0.9968) | LR: 6.84e-04
  Batch  200/375 ( 53.3%) | Loss: 0.2464 (avg: 0.2442) | Acc: 1.0000 (avg: 0.9966) | LR: 6.56e-04
  Batch  250/375 ( 66.7%) | Loss: 0.2467 (avg: 0.2446) | Acc: 0.9961 (avg: 0.9966) | LR: 6.28e-04
  Batch  300/375 ( 80.0%) | Loss: 0.2574 (avg: 0.2452) | Acc: 0.9961 (avg: 0.9963) | LR: 6.01e-04
  Batch  350/375 ( 93.3%) | Loss: 0.2494 (avg: 0.2462) | Acc: 0.9961 (avg: 0.9958) | LR: 5.74e-04

Train Summary:
  Final Loss: 0.2464 | Final Acc: 0.9957
  Final LR: 5.61e-04
  Loss std: 0.0068 | Acc std: 0.0044

Val Epoch 15 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4151 | Final Acc: 0.9214
  Loss std: 0.0398 | Acc std: 0.0173
Epoch 15/20 | train 0.2464/0.9957 | val 0.4151/0.9214

Train Epoch 16 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.2344 (avg: 0.2363) | Acc: 1.0000 (avg: 0.9978) | LR: 5.34e-04
  Batch  100/375 ( 26.7%) | Loss: 0.2373 (avg: 0.2372) | Acc: 1.0000 (avg: 0.9977) | LR: 5.08e-04
  Batch  150/375 ( 40.0%) | Loss: 0.2416 (avg: 0.2379) | Acc: 0.9961 (avg: 0.9974) | LR: 4.82e-04
  Batch  200/375 ( 53.3%) | Loss: 0.2401 (avg: 0.2381) | Acc: 1.0000 (avg: 0.9975) | LR: 4.57e-04
  Batch  250/375 ( 66.7%) | Loss: 0.2542 (avg: 0.2385) | Acc: 0.9883 (avg: 0.9974) | LR: 4.32e-04
  Batch  300/375 ( 80.0%) | Loss: 0.2461 (avg: 0.2391) | Acc: 1.0000 (avg: 0.9972) | LR: 4.08e-04
  Batch  350/375 ( 93.3%) | Loss: 0.2451 (avg: 0.2397) | Acc: 0.9922 (avg: 0.9970) | LR: 3.84e-04

Train Summary:
  Final Loss: 0.2400 | Final Acc: 0.9969
  Final LR: 3.73e-04
  Loss std: 0.0054 | Acc std: 0.0036

Val Epoch 16 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4144 | Final Acc: 0.9216
  Loss std: 0.0415 | Acc std: 0.0169
Epoch 16/20 | train 0.2400/0.9969 | val 0.4144/0.9216

Train Epoch 17 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.2327 (avg: 0.2328) | Acc: 1.0000 (avg: 0.9989) | LR: 3.50e-04
  Batch  100/375 ( 26.7%) | Loss: 0.2331 (avg: 0.2336) | Acc: 0.9961 (avg: 0.9988) | LR: 3.27e-04
  Batch  150/375 ( 40.0%) | Loss: 0.2294 (avg: 0.2337) | Acc: 1.0000 (avg: 0.9988) | LR: 3.06e-04
  Batch  200/375 ( 53.3%) | Loss: 0.2354 (avg: 0.2343) | Acc: 1.0000 (avg: 0.9985) | LR: 2.85e-04
  Batch  250/375 ( 66.7%) | Loss: 0.2375 (avg: 0.2344) | Acc: 0.9961 (avg: 0.9985) | LR: 2.64e-04
  Batch  300/375 ( 80.0%) | Loss: 0.2381 (avg: 0.2346) | Acc: 1.0000 (avg: 0.9984) | LR: 2.44e-04
  Batch  350/375 ( 93.3%) | Loss: 0.2328 (avg: 0.2349) | Acc: 1.0000 (avg: 0.9983) | LR: 2.25e-04

Train Summary:
  Final Loss: 0.2351 | Final Acc: 0.9983
  Final LR: 2.16e-04
  Loss std: 0.0045 | Acc std: 0.0027

Val Epoch 17 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4117 | Final Acc: 0.9228
  Loss std: 0.0409 | Acc std: 0.0173
Epoch 17/20 | train 0.2351/0.9983 | val 0.4117/0.9228

Train Epoch 18 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.2302 (avg: 0.2299) | Acc: 1.0000 (avg: 0.9991) | LR: 1.98e-04
  Batch  100/375 ( 26.7%) | Loss: 0.2365 (avg: 0.2304) | Acc: 0.9961 (avg: 0.9992) | LR: 1.80e-04
  Batch  150/375 ( 40.0%) | Loss: 0.2431 (avg: 0.2308) | Acc: 0.9961 (avg: 0.9992) | LR: 1.64e-04
  Batch  200/375 ( 53.3%) | Loss: 0.2287 (avg: 0.2309) | Acc: 1.0000 (avg: 0.9991) | LR: 1.48e-04
  Batch  250/375 ( 66.7%) | Loss: 0.2334 (avg: 0.2312) | Acc: 1.0000 (avg: 0.9990) | LR: 1.32e-04
  Batch  300/375 ( 80.0%) | Loss: 0.2405 (avg: 0.2314) | Acc: 0.9922 (avg: 0.9990) | LR: 1.18e-04
  Batch  350/375 ( 93.3%) | Loss: 0.2367 (avg: 0.2316) | Acc: 0.9961 (avg: 0.9989) | LR: 1.04e-04

Train Summary:
  Final Loss: 0.2317 | Final Acc: 0.9989
  Final LR: 9.82e-05
  Loss std: 0.0037 | Acc std: 0.0021

Val Epoch 18 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4112 | Final Acc: 0.9230
  Loss std: 0.0408 | Acc std: 0.0161
Epoch 18/20 | train 0.2317/0.9989 | val 0.4112/0.9230

Train Epoch 19 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.2244 (avg: 0.2287) | Acc: 1.0000 (avg: 0.9995) | LR: 8.55e-05
  Batch  100/375 ( 26.7%) | Loss: 0.2360 (avg: 0.2294) | Acc: 0.9961 (avg: 0.9993) | LR: 7.38e-05
  Batch  150/375 ( 40.0%) | Loss: 0.2280 (avg: 0.2294) | Acc: 1.0000 (avg: 0.9993) | LR: 6.30e-05
  Batch  200/375 ( 53.3%) | Loss: 0.2267 (avg: 0.2294) | Acc: 1.0000 (avg: 0.9991) | LR: 5.30e-05
  Batch  250/375 ( 66.7%) | Loss: 0.2325 (avg: 0.2297) | Acc: 1.0000 (avg: 0.9991) | LR: 4.39e-05
  Batch  300/375 ( 80.0%) | Loss: 0.2293 (avg: 0.2298) | Acc: 1.0000 (avg: 0.9990) | LR: 3.56e-05
  Batch  350/375 ( 93.3%) | Loss: 0.2273 (avg: 0.2298) | Acc: 1.0000 (avg: 0.9991) | LR: 2.81e-05

Train Summary:
  Final Loss: 0.2298 | Final Acc: 0.9991
  Final LR: 2.49e-05
  Loss std: 0.0033 | Acc std: 0.0019

Val Epoch 19 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4111 | Final Acc: 0.9231
  Loss std: 0.0410 | Acc std: 0.0165
Epoch 19/20 | train 0.2298/0.9991 | val 0.4111/0.9231

Train Epoch 20 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.2261 (avg: 0.2285) | Acc: 1.0000 (avg: 0.9991) | LR: 1.86e-05
  Batch  100/375 ( 26.7%) | Loss: 0.2310 (avg: 0.2287) | Acc: 0.9961 (avg: 0.9991) | LR: 1.33e-05
  Batch  150/375 ( 40.0%) | Loss: 0.2275 (avg: 0.2287) | Acc: 1.0000 (avg: 0.9992) | LR: 8.90e-06
  Batch  200/375 ( 53.3%) | Loss: 0.2258 (avg: 0.2285) | Acc: 1.0000 (avg: 0.9993) | LR: 5.38e-06
  Batch  250/375 ( 66.7%) | Loss: 0.2310 (avg: 0.2285) | Acc: 1.0000 (avg: 0.9993) | LR: 2.74e-06
  Batch  300/375 ( 80.0%) | Loss: 0.2286 (avg: 0.2285) | Acc: 1.0000 (avg: 0.9993) | LR: 9.79e-07
  Batch  350/375 ( 93.3%) | Loss: 0.2268 (avg: 0.2286) | Acc: 1.0000 (avg: 0.9993) | LR: 1.10e-07

Train Summary:
  Final Loss: 0.2287 | Final Acc: 0.9993
  Final LR: 7.93e-09
  Loss std: 0.0029 | Acc std: 0.0016

Val Epoch 20 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4111 | Final Acc: 0.9231
  Loss std: 0.0411 | Acc std: 0.0164
Epoch 20/20 | train 0.2287/0.9993 | val 0.4111/0.9231
wandb:
wandb: Run history:
wandb:               batch ‚ñÜ‚ñÅ‚ñÉ‚ñá‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñÖ‚ñÉ‚ñÜ‚ñÇ‚ñÜ‚ñá‚ñÉ‚ñá‚ñÅ‚ñÇ‚ñà‚ñÇ‚ñÜ‚ñÅ‚ñÇ‚ñÜ‚ñà‚ñÉ‚ñá‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñà‚ñÜ‚ñá‚ñÖ‚ñÉ‚ñá‚ñà‚ñà
wandb:          best_epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:        best_val_acc ‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà
wandb:    final_best_epoch ‚ñÅ
wandb:  final_best_val_acc ‚ñÅ
wandb: final_learning_rate ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:      final_val_loss ‚ñÅ
wandb:       learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:           train_acc ‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                 +15 ...
wandb:
wandb: Run summary:
wandb:               batch 350
wandb:          best_epoch 20
wandb:        best_val_acc 0.92312
wandb:               epoch 20
wandb:    final_best_epoch 20
wandb:  final_best_val_acc 0.92312
wandb: final_learning_rate 0.0
wandb:      final_val_loss 0.41108
wandb:       learning_rate 0.0
wandb:              status completed
wandb:                 +16 ...
wandb:
wandb: üöÄ View run trial_003 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/e6uuy01g
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_061032-e6uuy01g/logs
[I 2025-10-06 06:36:39,976] Trial 3 finished with value: 0.923125 and parameters: {'emb_dim': 200, 'channels': 256, 'kernel_sizes': (2, 3, 4, 5), 'dropout': 0.5, 'lr': 0.001982169925018568, 'weight_decay': 1.9378094205564048e-05, 'batch_size': 256, 'max_len': 200, 'grad_clip': 1.5, 'optimizer': 'adam'}. Best is trial 3 with value: 0.923125.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_063639-jn0lppnd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-durian-10
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/jn0lppnd
wandb: Finishing previous runs because reinit is set to True.
wandb:
wandb: Run history:
wandb:   batch_size ‚ñÅ
wandb:     channels ‚ñÅ
wandb:      dropout ‚ñÅ
wandb:      emb_dim ‚ñÅ
wandb:    grad_clip ‚ñÅ
wandb:           lr ‚ñÅ
wandb:      max_len ‚ñÅ
wandb:      val_acc ‚ñÅ
wandb: weight_decay ‚ñÅ
wandb:
wandb: Run summary:
wandb:   batch_size 256
wandb:     channels 256
wandb:      dropout 0.5
wandb:      emb_dim 200
wandb:    grad_clip 1.5
wandb:           lr 0.00198
wandb:      max_len 200
wandb:    optimizer adam
wandb:      val_acc 0.92312
wandb: weight_decay 2e-05
wandb:
wandb: üöÄ View run trial/3/tough-durian-10 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/jn0lppnd
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_063639-jn0lppnd/logs
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_063640-mw8u9whp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_004
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/mw8u9whp

Train Epoch 1 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 1.5144 (avg: 1.5030) | Acc: 0.2266 (avg: 0.2542) | LR: 7.36e-05
  Batch  100/750 ( 13.3%) | Loss: 1.4370 (avg: 1.4770) | Acc: 0.2812 (avg: 0.2677) | LR: 7.52e-05
  Batch  150/750 ( 20.0%) | Loss: 1.3656 (avg: 1.4575) | Acc: 0.3828 (avg: 0.2795) | LR: 7.78e-05
  Batch  200/750 ( 26.7%) | Loss: 1.4005 (avg: 1.4404) | Acc: 0.3359 (avg: 0.2907) | LR: 8.16e-05
  Batch  250/750 ( 33.3%) | Loss: 1.3847 (avg: 1.4261) | Acc: 0.3438 (avg: 0.3027) | LR: 8.64e-05
  Batch  300/750 ( 40.0%) | Loss: 1.3706 (avg: 1.4108) | Acc: 0.3359 (avg: 0.3147) | LR: 9.22e-05
  Batch  350/750 ( 46.7%) | Loss: 1.2706 (avg: 1.3963) | Acc: 0.4375 (avg: 0.3268) | LR: 9.91e-05
  Batch  400/750 ( 53.3%) | Loss: 1.2146 (avg: 1.3806) | Acc: 0.4219 (avg: 0.3393) | LR: 1.07e-04
  Batch  450/750 ( 60.0%) | Loss: 1.1785 (avg: 1.3658) | Acc: 0.4609 (avg: 0.3519) | LR: 1.16e-04
  Batch  500/750 ( 66.7%) | Loss: 1.1192 (avg: 1.3479) | Acc: 0.5547 (avg: 0.3663) | LR: 1.26e-04
  Batch  550/750 ( 73.3%) | Loss: 1.1426 (avg: 1.3312) | Acc: 0.5078 (avg: 0.3799) | LR: 1.37e-04
  Batch  600/750 ( 80.0%) | Loss: 1.0489 (avg: 1.3146) | Acc: 0.6016 (avg: 0.3929) | LR: 1.49e-04
  Batch  650/750 ( 86.7%) | Loss: 1.0956 (avg: 1.2966) | Acc: 0.5781 (avg: 0.4066) | LR: 1.62e-04
  Batch  700/750 ( 93.3%) | Loss: 1.0144 (avg: 1.2792) | Acc: 0.6250 (avg: 0.4198) | LR: 1.76e-04
  Batch  750/750 (100.0%) | Loss: 0.9544 (avg: 1.2602) | Acc: 0.6484 (avg: 0.4340) | LR: 1.91e-04

Train Summary:
  Final Loss: 1.2602 | Final Acc: 0.4340
  Final LR: 1.90e-04
  Loss std: 0.1553 | Acc std: 0.1233

Val Epoch 1 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.8952 | Final Acc: 0.7326
  Loss std: 0.0380 | Acc std: 0.0366
Epoch 01/20 | train 1.2602/0.4340 | val 0.8952/0.7326

Train Epoch 2 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.9622 (avg: 0.9386) | Acc: 0.6328 (avg: 0.6594) | LR: 2.06e-04
  Batch  100/750 ( 13.3%) | Loss: 0.8898 (avg: 0.9266) | Acc: 0.6406 (avg: 0.6639) | LR: 2.23e-04
  Batch  150/750 ( 20.0%) | Loss: 0.8752 (avg: 0.9101) | Acc: 0.6875 (avg: 0.6737) | LR: 2.40e-04
  Batch  200/750 ( 26.7%) | Loss: 0.8370 (avg: 0.8931) | Acc: 0.7109 (avg: 0.6821) | LR: 2.59e-04
  Batch  250/750 ( 33.3%) | Loss: 0.8516 (avg: 0.8805) | Acc: 0.6953 (avg: 0.6885) | LR: 2.78e-04
  Batch  300/750 ( 40.0%) | Loss: 0.8143 (avg: 0.8694) | Acc: 0.7266 (avg: 0.6930) | LR: 2.98e-04
  Batch  350/750 ( 46.7%) | Loss: 0.7584 (avg: 0.8564) | Acc: 0.7266 (avg: 0.6997) | LR: 3.19e-04
  Batch  400/750 ( 53.3%) | Loss: 0.8268 (avg: 0.8437) | Acc: 0.7266 (avg: 0.7069) | LR: 3.41e-04
  Batch  450/750 ( 60.0%) | Loss: 0.6897 (avg: 0.8340) | Acc: 0.7656 (avg: 0.7120) | LR: 3.63e-04
  Batch  500/750 ( 66.7%) | Loss: 0.7418 (avg: 0.8223) | Acc: 0.7266 (avg: 0.7181) | LR: 3.86e-04
  Batch  550/750 ( 73.3%) | Loss: 0.6060 (avg: 0.8106) | Acc: 0.8125 (avg: 0.7241) | LR: 4.10e-04
  Batch  600/750 ( 80.0%) | Loss: 0.7184 (avg: 0.8010) | Acc: 0.8047 (avg: 0.7296) | LR: 4.34e-04
  Batch  650/750 ( 86.7%) | Loss: 0.6750 (avg: 0.7919) | Acc: 0.8125 (avg: 0.7348) | LR: 4.60e-04
  Batch  700/750 ( 93.3%) | Loss: 0.6730 (avg: 0.7826) | Acc: 0.7969 (avg: 0.7399) | LR: 4.85e-04
  Batch  750/750 (100.0%) | Loss: 0.6951 (avg: 0.7750) | Acc: 0.8281 (avg: 0.7441) | LR: 5.11e-04

Train Summary:
  Final Loss: 0.7750 | Final Acc: 0.7441
  Final LR: 5.11e-04
  Loss std: 0.1067 | Acc std: 0.0606

Val Epoch 2 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5776 | Final Acc: 0.8480
  Loss std: 0.0487 | Acc std: 0.0316
Epoch 02/20 | train 0.7750/0.7441 | val 0.5776/0.8480

Train Epoch 3 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.5886 (avg: 0.6225) | Acc: 0.8359 (avg: 0.8275) | LR: 5.38e-04
  Batch  100/750 ( 13.3%) | Loss: 0.5644 (avg: 0.6167) | Acc: 0.8438 (avg: 0.8297) | LR: 5.65e-04
  Batch  150/750 ( 20.0%) | Loss: 0.5401 (avg: 0.6096) | Acc: 0.8594 (avg: 0.8312) | LR: 5.93e-04
  Batch  200/750 ( 26.7%) | Loss: 0.5872 (avg: 0.6045) | Acc: 0.8984 (avg: 0.8340) | LR: 6.21e-04
  Batch  250/750 ( 33.3%) | Loss: 0.6265 (avg: 0.6042) | Acc: 0.8125 (avg: 0.8344) | LR: 6.50e-04
  Batch  300/750 ( 40.0%) | Loss: 0.5434 (avg: 0.6026) | Acc: 0.8516 (avg: 0.8346) | LR: 6.79e-04
  Batch  350/750 ( 46.7%) | Loss: 0.5532 (avg: 0.5998) | Acc: 0.8438 (avg: 0.8367) | LR: 7.08e-04
  Batch  400/750 ( 53.3%) | Loss: 0.6394 (avg: 0.5960) | Acc: 0.8438 (avg: 0.8378) | LR: 7.38e-04
  Batch  450/750 ( 60.0%) | Loss: 0.5079 (avg: 0.5931) | Acc: 0.8828 (avg: 0.8389) | LR: 7.68e-04
  Batch  500/750 ( 66.7%) | Loss: 0.6059 (avg: 0.5912) | Acc: 0.8672 (avg: 0.8399) | LR: 7.98e-04
  Batch  550/750 ( 73.3%) | Loss: 0.6750 (avg: 0.5888) | Acc: 0.8125 (avg: 0.8411) | LR: 8.28e-04
  Batch  600/750 ( 80.0%) | Loss: 0.4880 (avg: 0.5859) | Acc: 0.8984 (avg: 0.8424) | LR: 8.58e-04
  Batch  650/750 ( 86.7%) | Loss: 0.5389 (avg: 0.5831) | Acc: 0.8594 (avg: 0.8441) | LR: 8.89e-04
  Batch  700/750 ( 93.3%) | Loss: 0.5993 (avg: 0.5801) | Acc: 0.8281 (avg: 0.8457) | LR: 9.19e-04
  Batch  750/750 (100.0%) | Loss: 0.4598 (avg: 0.5766) | Acc: 0.8828 (avg: 0.8475) | LR: 9.50e-04

Train Summary:
  Final Loss: 0.5766 | Final Acc: 0.8475
  Final LR: 9.49e-04
  Loss std: 0.0616 | Acc std: 0.0333

Val Epoch 3 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4957 | Final Acc: 0.8834
  Loss std: 0.0508 | Acc std: 0.0283
Epoch 03/20 | train 0.5766/0.8475 | val 0.4957/0.8834

Train Epoch 4 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.4584 (avg: 0.4836) | Acc: 0.9141 (avg: 0.8903) | LR: 9.80e-04
  Batch  100/750 ( 13.3%) | Loss: 0.5412 (avg: 0.4932) | Acc: 0.8516 (avg: 0.8875) | LR: 1.01e-03
  Batch  150/750 ( 20.0%) | Loss: 0.4796 (avg: 0.4919) | Acc: 0.8984 (avg: 0.8890) | LR: 1.04e-03
  Batch  200/750 ( 26.7%) | Loss: 0.5555 (avg: 0.4966) | Acc: 0.8672 (avg: 0.8863) | LR: 1.07e-03
  Batch  250/750 ( 33.3%) | Loss: 0.4838 (avg: 0.4920) | Acc: 0.9062 (avg: 0.8887) | LR: 1.10e-03
  Batch  300/750 ( 40.0%) | Loss: 0.5620 (avg: 0.4924) | Acc: 0.8672 (avg: 0.8882) | LR: 1.13e-03
  Batch  350/750 ( 46.7%) | Loss: 0.4504 (avg: 0.4920) | Acc: 0.9141 (avg: 0.8882) | LR: 1.16e-03
  Batch  400/750 ( 53.3%) | Loss: 0.4139 (avg: 0.4900) | Acc: 0.9297 (avg: 0.8889) | LR: 1.19e-03
  Batch  450/750 ( 60.0%) | Loss: 0.4611 (avg: 0.4888) | Acc: 0.8984 (avg: 0.8892) | LR: 1.22e-03
  Batch  500/750 ( 66.7%) | Loss: 0.4992 (avg: 0.4889) | Acc: 0.8906 (avg: 0.8893) | LR: 1.25e-03
  Batch  550/750 ( 73.3%) | Loss: 0.5060 (avg: 0.4892) | Acc: 0.8828 (avg: 0.8890) | LR: 1.28e-03
  Batch  600/750 ( 80.0%) | Loss: 0.3801 (avg: 0.4892) | Acc: 0.9375 (avg: 0.8889) | LR: 1.31e-03
  Batch  650/750 ( 86.7%) | Loss: 0.5213 (avg: 0.4888) | Acc: 0.8906 (avg: 0.8889) | LR: 1.33e-03
  Batch  700/750 ( 93.3%) | Loss: 0.4600 (avg: 0.4880) | Acc: 0.9062 (avg: 0.8889) | LR: 1.36e-03
  Batch  750/750 (100.0%) | Loss: 0.4688 (avg: 0.4868) | Acc: 0.8906 (avg: 0.8894) | LR: 1.39e-03

Train Summary:
  Final Loss: 0.4868 | Final Acc: 0.8894
  Final LR: 1.39e-03
  Loss std: 0.0532 | Acc std: 0.0288

Val Epoch 4 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4569 | Final Acc: 0.9017
  Loss std: 0.0497 | Acc std: 0.0264
Epoch 04/20 | train 0.4868/0.8894 | val 0.4569/0.9017

Train Epoch 5 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.3720 (avg: 0.4230) | Acc: 0.9297 (avg: 0.9175) | LR: 1.41e-03
  Batch  100/750 ( 13.3%) | Loss: 0.4328 (avg: 0.4265) | Acc: 0.9219 (avg: 0.9155) | LR: 1.44e-03
  Batch  150/750 ( 20.0%) | Loss: 0.4156 (avg: 0.4279) | Acc: 0.9297 (avg: 0.9151) | LR: 1.47e-03
  Batch  200/750 ( 26.7%) | Loss: 0.3931 (avg: 0.4272) | Acc: 0.9375 (avg: 0.9166) | LR: 1.49e-03
  Batch  250/750 ( 33.3%) | Loss: 0.3862 (avg: 0.4266) | Acc: 0.9297 (avg: 0.9171) | LR: 1.51e-03
  Batch  300/750 ( 40.0%) | Loss: 0.4369 (avg: 0.4282) | Acc: 0.9453 (avg: 0.9163) | LR: 1.54e-03
  Batch  350/750 ( 46.7%) | Loss: 0.3516 (avg: 0.4279) | Acc: 0.9375 (avg: 0.9164) | LR: 1.56e-03
  Batch  400/750 ( 53.3%) | Loss: 0.4446 (avg: 0.4279) | Acc: 0.8828 (avg: 0.9165) | LR: 1.58e-03
  Batch  450/750 ( 60.0%) | Loss: 0.4300 (avg: 0.4298) | Acc: 0.9219 (avg: 0.9154) | LR: 1.60e-03
  Batch  500/750 ( 66.7%) | Loss: 0.4942 (avg: 0.4311) | Acc: 0.8984 (avg: 0.9145) | LR: 1.62e-03
  Batch  550/750 ( 73.3%) | Loss: 0.4032 (avg: 0.4317) | Acc: 0.9219 (avg: 0.9142) | LR: 1.64e-03
  Batch  600/750 ( 80.0%) | Loss: 0.4264 (avg: 0.4314) | Acc: 0.9141 (avg: 0.9146) | LR: 1.66e-03
  Batch  650/750 ( 86.7%) | Loss: 0.5459 (avg: 0.4325) | Acc: 0.8359 (avg: 0.9140) | LR: 1.68e-03
  Batch  700/750 ( 93.3%) | Loss: 0.3987 (avg: 0.4321) | Acc: 0.9219 (avg: 0.9142) | LR: 1.69e-03
  Batch  750/750 (100.0%) | Loss: 0.3582 (avg: 0.4324) | Acc: 0.9453 (avg: 0.9139) | LR: 1.71e-03

Train Summary:
  Final Loss: 0.4324 | Final Acc: 0.9139
  Final LR: 1.71e-03
  Loss std: 0.0480 | Acc std: 0.0259

Val Epoch 5 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4316 | Final Acc: 0.9119
  Loss std: 0.0471 | Acc std: 0.0266
Epoch 05/20 | train 0.4324/0.9139 | val 0.4316/0.9119

Train Epoch 6 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.3721 (avg: 0.3803) | Acc: 0.9375 (avg: 0.9361) | LR: 1.72e-03
  Batch  100/750 ( 13.3%) | Loss: 0.3419 (avg: 0.3816) | Acc: 0.9531 (avg: 0.9351) | LR: 1.74e-03
  Batch  150/750 ( 20.0%) | Loss: 0.4408 (avg: 0.3834) | Acc: 0.9062 (avg: 0.9343) | LR: 1.75e-03
  Batch  200/750 ( 26.7%) | Loss: 0.3970 (avg: 0.3866) | Acc: 0.9297 (avg: 0.9340) | LR: 1.76e-03
  Batch  250/750 ( 33.3%) | Loss: 0.3716 (avg: 0.3880) | Acc: 0.9375 (avg: 0.9333) | LR: 1.77e-03
  Batch  300/750 ( 40.0%) | Loss: 0.4026 (avg: 0.3878) | Acc: 0.9141 (avg: 0.9334) | LR: 1.78e-03
  Batch  350/750 ( 46.7%) | Loss: 0.4174 (avg: 0.3886) | Acc: 0.9297 (avg: 0.9336) | LR: 1.79e-03
  Batch  400/750 ( 53.3%) | Loss: 0.3745 (avg: 0.3904) | Acc: 0.9609 (avg: 0.9328) | LR: 1.80e-03
  Batch  450/750 ( 60.0%) | Loss: 0.3336 (avg: 0.3916) | Acc: 0.9609 (avg: 0.9320) | LR: 1.81e-03
  Batch  500/750 ( 66.7%) | Loss: 0.3434 (avg: 0.3925) | Acc: 0.9453 (avg: 0.9315) | LR: 1.81e-03
  Batch  550/750 ( 73.3%) | Loss: 0.4528 (avg: 0.3929) | Acc: 0.8750 (avg: 0.9312) | LR: 1.82e-03
  Batch  600/750 ( 80.0%) | Loss: 0.3959 (avg: 0.3939) | Acc: 0.9297 (avg: 0.9309) | LR: 1.82e-03
  Batch  650/750 ( 86.7%) | Loss: 0.3324 (avg: 0.3947) | Acc: 0.9531 (avg: 0.9307) | LR: 1.82e-03
  Batch  700/750 ( 93.3%) | Loss: 0.4527 (avg: 0.3958) | Acc: 0.9141 (avg: 0.9302) | LR: 1.83e-03
  Batch  750/750 (100.0%) | Loss: 0.4507 (avg: 0.3974) | Acc: 0.8828 (avg: 0.9296) | LR: 1.83e-03

Train Summary:
  Final Loss: 0.3974 | Final Acc: 0.9296
  Final LR: 1.83e-03
  Loss std: 0.0409 | Acc std: 0.0229

Val Epoch 6 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4276 | Final Acc: 0.9137
  Loss std: 0.0491 | Acc std: 0.0255
Epoch 06/20 | train 0.3974/0.9296 | val 0.4276/0.9137

Train Epoch 7 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.3468 (avg: 0.3387) | Acc: 0.9688 (avg: 0.9564) | LR: 1.83e-03
  Batch  100/750 ( 13.3%) | Loss: 0.3383 (avg: 0.3457) | Acc: 0.9609 (avg: 0.9540) | LR: 1.83e-03
  Batch  150/750 ( 20.0%) | Loss: 0.3259 (avg: 0.3471) | Acc: 0.9609 (avg: 0.9542) | LR: 1.83e-03
  Batch  200/750 ( 26.7%) | Loss: 0.3716 (avg: 0.3523) | Acc: 0.9375 (avg: 0.9512) | LR: 1.82e-03
  Batch  250/750 ( 33.3%) | Loss: 0.3543 (avg: 0.3543) | Acc: 0.9453 (avg: 0.9504) | LR: 1.82e-03
  Batch  300/750 ( 40.0%) | Loss: 0.3084 (avg: 0.3566) | Acc: 0.9766 (avg: 0.9492) | LR: 1.82e-03
  Batch  350/750 ( 46.7%) | Loss: 0.3894 (avg: 0.3577) | Acc: 0.9297 (avg: 0.9490) | LR: 1.82e-03
  Batch  400/750 ( 53.3%) | Loss: 0.3731 (avg: 0.3591) | Acc: 0.9375 (avg: 0.9477) | LR: 1.82e-03
  Batch  450/750 ( 60.0%) | Loss: 0.4599 (avg: 0.3610) | Acc: 0.8906 (avg: 0.9463) | LR: 1.82e-03
  Batch  500/750 ( 66.7%) | Loss: 0.4675 (avg: 0.3633) | Acc: 0.9062 (avg: 0.9452) | LR: 1.82e-03
  Batch  550/750 ( 73.3%) | Loss: 0.4234 (avg: 0.3652) | Acc: 0.9297 (avg: 0.9443) | LR: 1.81e-03
  Batch  600/750 ( 80.0%) | Loss: 0.3717 (avg: 0.3664) | Acc: 0.9609 (avg: 0.9439) | LR: 1.81e-03
  Batch  650/750 ( 86.7%) | Loss: 0.3817 (avg: 0.3675) | Acc: 0.9531 (avg: 0.9432) | LR: 1.81e-03
  Batch  700/750 ( 93.3%) | Loss: 0.4836 (avg: 0.3689) | Acc: 0.8984 (avg: 0.9426) | LR: 1.81e-03
  Batch  750/750 (100.0%) | Loss: 0.3615 (avg: 0.3696) | Acc: 0.9531 (avg: 0.9422) | LR: 1.80e-03

Train Summary:
  Final Loss: 0.3696 | Final Acc: 0.9422
  Final LR: 1.80e-03
  Loss std: 0.0383 | Acc std: 0.0213

Val Epoch 7 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4273 | Final Acc: 0.9125
  Loss std: 0.0495 | Acc std: 0.0250
Epoch 07/20 | train 0.3696/0.9422 | val 0.4273/0.9125

Train Epoch 8 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.3186 (avg: 0.3132) | Acc: 0.9609 (avg: 0.9692) | LR: 1.80e-03
  Batch  100/750 ( 13.3%) | Loss: 0.2909 (avg: 0.3197) | Acc: 0.9922 (avg: 0.9657) | LR: 1.80e-03
  Batch  150/750 ( 20.0%) | Loss: 0.3436 (avg: 0.3216) | Acc: 0.9609 (avg: 0.9652) | LR: 1.79e-03
  Batch  200/750 ( 26.7%) | Loss: 0.3203 (avg: 0.3248) | Acc: 0.9688 (avg: 0.9637) | LR: 1.79e-03
  Batch  250/750 ( 33.3%) | Loss: 0.3089 (avg: 0.3292) | Acc: 0.9688 (avg: 0.9617) | LR: 1.79e-03
  Batch  300/750 ( 40.0%) | Loss: 0.3525 (avg: 0.3303) | Acc: 0.9453 (avg: 0.9610) | LR: 1.78e-03
  Batch  350/750 ( 46.7%) | Loss: 0.3768 (avg: 0.3324) | Acc: 0.9297 (avg: 0.9603) | LR: 1.78e-03
  Batch  400/750 ( 53.3%) | Loss: 0.3526 (avg: 0.3342) | Acc: 0.9297 (avg: 0.9591) | LR: 1.77e-03
  Batch  450/750 ( 60.0%) | Loss: 0.3658 (avg: 0.3359) | Acc: 0.9375 (avg: 0.9584) | LR: 1.77e-03
  Batch  500/750 ( 66.7%) | Loss: 0.3677 (avg: 0.3372) | Acc: 0.9453 (avg: 0.9579) | LR: 1.76e-03
  Batch  550/750 ( 73.3%) | Loss: 0.3113 (avg: 0.3383) | Acc: 0.9766 (avg: 0.9573) | LR: 1.76e-03
  Batch  600/750 ( 80.0%) | Loss: 0.4313 (avg: 0.3405) | Acc: 0.9297 (avg: 0.9562) | LR: 1.75e-03
  Batch  650/750 ( 86.7%) | Loss: 0.3536 (avg: 0.3420) | Acc: 0.9453 (avg: 0.9554) | LR: 1.75e-03
  Batch  700/750 ( 93.3%) | Loss: 0.3530 (avg: 0.3437) | Acc: 0.9375 (avg: 0.9545) | LR: 1.74e-03
  Batch  750/750 (100.0%) | Loss: 0.3682 (avg: 0.3456) | Acc: 0.9453 (avg: 0.9536) | LR: 1.74e-03

Train Summary:
  Final Loss: 0.3456 | Final Acc: 0.9536
  Final LR: 1.74e-03
  Loss std: 0.0357 | Acc std: 0.0209

Val Epoch 8 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4258 | Final Acc: 0.9173
  Loss std: 0.0516 | Acc std: 0.0241
Epoch 08/20 | train 0.3456/0.9536 | val 0.4258/0.9173

Train Epoch 9 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.2995 (avg: 0.3104) | Acc: 0.9688 (avg: 0.9697) | LR: 1.73e-03
  Batch  100/750 ( 13.3%) | Loss: 0.3148 (avg: 0.3075) | Acc: 0.9609 (avg: 0.9733) | LR: 1.72e-03
  Batch  150/750 ( 20.0%) | Loss: 0.2828 (avg: 0.3063) | Acc: 0.9844 (avg: 0.9733) | LR: 1.72e-03
  Batch  200/750 ( 26.7%) | Loss: 0.2977 (avg: 0.3082) | Acc: 0.9766 (avg: 0.9719) | LR: 1.71e-03
  Batch  250/750 ( 33.3%) | Loss: 0.3600 (avg: 0.3098) | Acc: 0.9375 (avg: 0.9714) | LR: 1.70e-03
  Batch  300/750 ( 40.0%) | Loss: 0.3427 (avg: 0.3117) | Acc: 0.9531 (avg: 0.9704) | LR: 1.70e-03
  Batch  350/750 ( 46.7%) | Loss: 0.3019 (avg: 0.3139) | Acc: 0.9766 (avg: 0.9694) | LR: 1.69e-03
  Batch  400/750 ( 53.3%) | Loss: 0.3429 (avg: 0.3154) | Acc: 0.9609 (avg: 0.9686) | LR: 1.68e-03
  Batch  450/750 ( 60.0%) | Loss: 0.3845 (avg: 0.3166) | Acc: 0.9531 (avg: 0.9679) | LR: 1.67e-03
  Batch  500/750 ( 66.7%) | Loss: 0.3313 (avg: 0.3181) | Acc: 0.9688 (avg: 0.9672) | LR: 1.67e-03
  Batch  550/750 ( 73.3%) | Loss: 0.3185 (avg: 0.3194) | Acc: 0.9609 (avg: 0.9664) | LR: 1.66e-03
  Batch  600/750 ( 80.0%) | Loss: 0.2998 (avg: 0.3207) | Acc: 0.9766 (avg: 0.9658) | LR: 1.65e-03
  Batch  650/750 ( 86.7%) | Loss: 0.3166 (avg: 0.3221) | Acc: 0.9688 (avg: 0.9651) | LR: 1.64e-03
  Batch  700/750 ( 93.3%) | Loss: 0.3480 (avg: 0.3234) | Acc: 0.9609 (avg: 0.9645) | LR: 1.64e-03
  Batch  750/750 (100.0%) | Loss: 0.3821 (avg: 0.3249) | Acc: 0.9375 (avg: 0.9638) | LR: 1.63e-03

Train Summary:
  Final Loss: 0.3249 | Final Acc: 0.9638
  Final LR: 1.63e-03
  Loss std: 0.0299 | Acc std: 0.0185

Val Epoch 9 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4393 | Final Acc: 0.9093
  Loss std: 0.0539 | Acc std: 0.0254
Epoch 09/20 | train 0.3249/0.9638 | val 0.4393/0.9093

Train Epoch 10 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.2879 (avg: 0.2921) | Acc: 0.9688 (avg: 0.9803) | LR: 1.62e-03
  Batch  100/750 ( 13.3%) | Loss: 0.3078 (avg: 0.2929) | Acc: 0.9766 (avg: 0.9792) | LR: 1.61e-03
  Batch  150/750 ( 20.0%) | Loss: 0.2768 (avg: 0.2928) | Acc: 0.9922 (avg: 0.9792) | LR: 1.60e-03
  Batch  200/750 ( 26.7%) | Loss: 0.3332 (avg: 0.2931) | Acc: 0.9609 (avg: 0.9793) | LR: 1.59e-03
  Batch  250/750 ( 33.3%) | Loss: 0.3158 (avg: 0.2937) | Acc: 0.9766 (avg: 0.9788) | LR: 1.58e-03
  Batch  300/750 ( 40.0%) | Loss: 0.2852 (avg: 0.2949) | Acc: 0.9844 (avg: 0.9784) | LR: 1.57e-03
  Batch  350/750 ( 46.7%) | Loss: 0.2900 (avg: 0.2965) | Acc: 0.9844 (avg: 0.9778) | LR: 1.56e-03
  Batch  400/750 ( 53.3%) | Loss: 0.3749 (avg: 0.2980) | Acc: 0.9453 (avg: 0.9775) | LR: 1.55e-03
  Batch  450/750 ( 60.0%) | Loss: 0.3303 (avg: 0.2994) | Acc: 0.9609 (avg: 0.9769) | LR: 1.54e-03
  Batch  500/750 ( 66.7%) | Loss: 0.3040 (avg: 0.3006) | Acc: 0.9609 (avg: 0.9761) | LR: 1.53e-03
  Batch  550/750 ( 73.3%) | Loss: 0.3194 (avg: 0.3020) | Acc: 0.9609 (avg: 0.9754) | LR: 1.52e-03
  Batch  600/750 ( 80.0%) | Loss: 0.3521 (avg: 0.3034) | Acc: 0.9531 (avg: 0.9745) | LR: 1.51e-03
  Batch  650/750 ( 86.7%) | Loss: 0.3499 (avg: 0.3043) | Acc: 0.9531 (avg: 0.9741) | LR: 1.50e-03
  Batch  700/750 ( 93.3%) | Loss: 0.3905 (avg: 0.3056) | Acc: 0.8984 (avg: 0.9733) | LR: 1.49e-03
  Batch  750/750 (100.0%) | Loss: 0.2940 (avg: 0.3067) | Acc: 0.9688 (avg: 0.9727) | LR: 1.48e-03

Train Summary:
  Final Loss: 0.3067 | Final Acc: 0.9727
  Final LR: 1.48e-03
  Loss std: 0.0242 | Acc std: 0.0153

Val Epoch 10 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4357 | Final Acc: 0.9153
  Loss std: 0.0540 | Acc std: 0.0250
Epoch 10/20 | train 0.3067/0.9727 | val 0.4357/0.9153

Train Epoch 11 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.3214 (avg: 0.2813) | Acc: 0.9766 (avg: 0.9855) | LR: 1.47e-03
  Batch  100/750 ( 13.3%) | Loss: 0.2707 (avg: 0.2796) | Acc: 0.9922 (avg: 0.9857) | LR: 1.46e-03
  Batch  150/750 ( 20.0%) | Loss: 0.2825 (avg: 0.2799) | Acc: 0.9844 (avg: 0.9849) | LR: 1.45e-03
  Batch  200/750 ( 26.7%) | Loss: 0.2965 (avg: 0.2797) | Acc: 0.9844 (avg: 0.9853) | LR: 1.44e-03
  Batch  250/750 ( 33.3%) | Loss: 0.2876 (avg: 0.2801) | Acc: 0.9766 (avg: 0.9850) | LR: 1.43e-03
  Batch  300/750 ( 40.0%) | Loss: 0.2668 (avg: 0.2808) | Acc: 1.0000 (avg: 0.9847) | LR: 1.42e-03
  Batch  350/750 ( 46.7%) | Loss: 0.2993 (avg: 0.2818) | Acc: 0.9766 (avg: 0.9841) | LR: 1.40e-03
  Batch  400/750 ( 53.3%) | Loss: 0.2755 (avg: 0.2830) | Acc: 0.9766 (avg: 0.9834) | LR: 1.39e-03
  Batch  450/750 ( 60.0%) | Loss: 0.2739 (avg: 0.2844) | Acc: 0.9922 (avg: 0.9829) | LR: 1.38e-03
  Batch  500/750 ( 66.7%) | Loss: 0.2984 (avg: 0.2857) | Acc: 0.9688 (avg: 0.9823) | LR: 1.37e-03
  Batch  550/750 ( 73.3%) | Loss: 0.3161 (avg: 0.2871) | Acc: 0.9688 (avg: 0.9814) | LR: 1.36e-03
  Batch  600/750 ( 80.0%) | Loss: 0.2990 (avg: 0.2883) | Acc: 0.9844 (avg: 0.9808) | LR: 1.35e-03
  Batch  650/750 ( 86.7%) | Loss: 0.2714 (avg: 0.2895) | Acc: 0.9922 (avg: 0.9801) | LR: 1.33e-03
  Batch  700/750 ( 93.3%) | Loss: 0.2953 (avg: 0.2905) | Acc: 0.9844 (avg: 0.9797) | LR: 1.32e-03
  Batch  750/750 (100.0%) | Loss: 0.2883 (avg: 0.2917) | Acc: 0.9688 (avg: 0.9792) | LR: 1.31e-03

Train Summary:
  Final Loss: 0.2917 | Final Acc: 0.9792
  Final LR: 1.31e-03
  Loss std: 0.0218 | Acc std: 0.0140

Val Epoch 11 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4375 | Final Acc: 0.9141
  Loss std: 0.0534 | Acc std: 0.0242
Epoch 11/20 | train 0.2917/0.9792 | val 0.4375/0.9141
Early stopping at epoch 11
wandb:
wandb: Run history:
wandb:               batch ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñá‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñá‚ñà‚ñÅ‚ñÖ‚ñá‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÅ‚ñÖ‚ñá‚ñà‚ñÉ‚ñÑ‚ñá
wandb:          best_epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñà
wandb:        best_val_acc ‚ñÅ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:    final_best_epoch ‚ñÅ
wandb:  final_best_val_acc ‚ñÅ
wandb: final_learning_rate ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ
wandb:      final_val_loss ‚ñÅ
wandb:       learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ
wandb:           train_acc ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:                 +15 ...
wandb:
wandb: Run summary:
wandb:               batch 750
wandb:          best_epoch 8
wandb:        best_val_acc 0.91725
wandb:               epoch 11
wandb:    final_best_epoch 8
wandb:  final_best_val_acc 0.91725
wandb: final_learning_rate 0.00131
wandb:      final_val_loss 0.43755
wandb:       learning_rate 0.00131
wandb:              status completed
wandb:                 +16 ...
wandb:
wandb: üöÄ View run trial_004 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/mw8u9whp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_063640-mw8u9whp/logs
[I 2025-10-06 06:48:39,073] Trial 4 finished with value: 0.91725 and parameters: {'emb_dim': 100, 'channels': 128, 'kernel_sizes': (3, 4, 5), 'dropout': 0.4, 'lr': 0.001825946326867454, 'weight_decay': 7.705868534020752e-06, 'batch_size': 128, 'max_len': 200, 'grad_clip': 1.0, 'optimizer': 'adam'}. Best is trial 3 with value: 0.923125.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_064839-3lhoo1jy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run denim-wind-12
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/3lhoo1jy
wandb: Finishing previous runs because reinit is set to True.
wandb:
wandb: Run history:
wandb:   batch_size ‚ñÅ
wandb:     channels ‚ñÅ
wandb:      dropout ‚ñÅ
wandb:      emb_dim ‚ñÅ
wandb:    grad_clip ‚ñÅ
wandb:           lr ‚ñÅ
wandb:      max_len ‚ñÅ
wandb:      val_acc ‚ñÅ
wandb: weight_decay ‚ñÅ
wandb:
wandb: Run summary:
wandb:   batch_size 128
wandb:     channels 128
wandb:      dropout 0.4
wandb:      emb_dim 100
wandb:    grad_clip 1
wandb:           lr 0.00183
wandb:      max_len 200
wandb:    optimizer adam
wandb:      val_acc 0.91725
wandb: weight_decay 1e-05
wandb:
wandb: üöÄ View run trial/4/denim-wind-12 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/3lhoo1jy
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_064839-3lhoo1jy/logs
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_064839-e19r722p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_005
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/e19r722p

Train Epoch 1 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 1.3747 (avg: 1.4860) | Acc: 0.3281 (avg: 0.2612) | LR: 4.09e-05
  Batch  100/1500 (  6.7%) | Loss: 1.3346 (avg: 1.4418) | Acc: 0.2969 (avg: 0.2823) | LR: 4.11e-05
  Batch  150/1500 ( 10.0%) | Loss: 1.3271 (avg: 1.4076) | Acc: 0.4688 (avg: 0.3143) | LR: 4.15e-05
  Batch  200/1500 ( 13.3%) | Loss: 1.1986 (avg: 1.3796) | Acc: 0.5312 (avg: 0.3372) | LR: 4.20e-05
  Batch  250/1500 ( 16.7%) | Loss: 1.2518 (avg: 1.3532) | Acc: 0.4219 (avg: 0.3594) | LR: 4.27e-05
  Batch  300/1500 ( 20.0%) | Loss: 1.2514 (avg: 1.3281) | Acc: 0.4219 (avg: 0.3816) | LR: 4.35e-05
  Batch  350/1500 ( 23.3%) | Loss: 1.1286 (avg: 1.3021) | Acc: 0.5469 (avg: 0.4040) | LR: 4.45e-05
  Batch  400/1500 ( 26.7%) | Loss: 1.0653 (avg: 1.2790) | Acc: 0.5781 (avg: 0.4252) | LR: 4.56e-05
  Batch  450/1500 ( 30.0%) | Loss: 1.0434 (avg: 1.2567) | Acc: 0.6562 (avg: 0.4440) | LR: 4.68e-05
  Batch  500/1500 ( 33.3%) | Loss: 1.0088 (avg: 1.2356) | Acc: 0.5938 (avg: 0.4613) | LR: 4.82e-05
  Batch  550/1500 ( 36.7%) | Loss: 0.9966 (avg: 1.2156) | Acc: 0.6094 (avg: 0.4763) | LR: 4.98e-05
  Batch  600/1500 ( 40.0%) | Loss: 0.9211 (avg: 1.1958) | Acc: 0.7500 (avg: 0.4911) | LR: 5.15e-05
  Batch  650/1500 ( 43.3%) | Loss: 0.8868 (avg: 1.1761) | Acc: 0.6875 (avg: 0.5055) | LR: 5.34e-05
  Batch  700/1500 ( 46.7%) | Loss: 0.8067 (avg: 1.1580) | Acc: 0.7500 (avg: 0.5183) | LR: 5.54e-05
  Batch  750/1500 ( 50.0%) | Loss: 0.8495 (avg: 1.1404) | Acc: 0.7656 (avg: 0.5303) | LR: 5.75e-05
  Batch  800/1500 ( 53.3%) | Loss: 0.7796 (avg: 1.1228) | Acc: 0.7500 (avg: 0.5424) | LR: 5.98e-05
  Batch  850/1500 ( 56.7%) | Loss: 0.8160 (avg: 1.1070) | Acc: 0.7812 (avg: 0.5531) | LR: 6.22e-05
  Batch  900/1500 ( 60.0%) | Loss: 0.8560 (avg: 1.0918) | Acc: 0.7031 (avg: 0.5634) | LR: 6.48e-05
  Batch  950/1500 ( 63.3%) | Loss: 0.8578 (avg: 1.0769) | Acc: 0.7031 (avg: 0.5723) | LR: 6.75e-05
  Batch 1000/1500 ( 66.7%) | Loss: 0.6645 (avg: 1.0627) | Acc: 0.7969 (avg: 0.5812) | LR: 7.03e-05
  Batch 1050/1500 ( 70.0%) | Loss: 0.7700 (avg: 1.0492) | Acc: 0.7344 (avg: 0.5896) | LR: 7.33e-05
  Batch 1100/1500 ( 73.3%) | Loss: 0.6723 (avg: 1.0357) | Acc: 0.7812 (avg: 0.5975) | LR: 7.65e-05
  Batch 1150/1500 ( 76.7%) | Loss: 0.7423 (avg: 1.0227) | Acc: 0.7344 (avg: 0.6052) | LR: 7.97e-05
  Batch 1200/1500 ( 80.0%) | Loss: 0.6275 (avg: 1.0101) | Acc: 0.7969 (avg: 0.6126) | LR: 8.32e-05
  Batch 1250/1500 ( 83.3%) | Loss: 0.6159 (avg: 0.9976) | Acc: 0.8281 (avg: 0.6196) | LR: 8.67e-05
  Batch 1300/1500 ( 86.7%) | Loss: 0.6319 (avg: 0.9855) | Acc: 0.8594 (avg: 0.6262) | LR: 9.04e-05
  Batch 1350/1500 ( 90.0%) | Loss: 0.6449 (avg: 0.9746) | Acc: 0.7812 (avg: 0.6323) | LR: 9.42e-05
  Batch 1400/1500 ( 93.3%) | Loss: 0.5461 (avg: 0.9643) | Acc: 0.8438 (avg: 0.6380) | LR: 9.81e-05
  Batch 1450/1500 ( 96.7%) | Loss: 0.5810 (avg: 0.9533) | Acc: 0.8281 (avg: 0.6442) | LR: 1.02e-04
  Batch 1500/1500 (100.0%) | Loss: 0.6992 (avg: 0.9440) | Acc: 0.7031 (avg: 0.6498) | LR: 1.06e-04

Train Summary:
  Final Loss: 0.9440 | Final Acc: 0.6498
  Final LR: 1.06e-04
  Loss std: 0.2471 | Acc std: 0.1648

Val Epoch 1 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5931 | Final Acc: 0.8523
  Loss std: 0.0707 | Acc std: 0.0443
Epoch 01/20 | train 0.9440/0.6498 | val 0.5931/0.8523

Train Epoch 2 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.7252 (avg: 0.6110) | Acc: 0.7656 (avg: 0.8359) | LR: 1.11e-04
  Batch  100/1500 (  6.7%) | Loss: 0.6938 (avg: 0.6005) | Acc: 0.7812 (avg: 0.8386) | LR: 1.15e-04
  Batch  150/1500 ( 10.0%) | Loss: 0.5513 (avg: 0.5974) | Acc: 0.8594 (avg: 0.8392) | LR: 1.20e-04
  Batch  200/1500 ( 13.3%) | Loss: 0.4818 (avg: 0.5946) | Acc: 0.8906 (avg: 0.8409) | LR: 1.25e-04
  Batch  250/1500 ( 16.7%) | Loss: 0.5621 (avg: 0.5925) | Acc: 0.8281 (avg: 0.8407) | LR: 1.29e-04
  Batch  300/1500 ( 20.0%) | Loss: 0.7307 (avg: 0.5951) | Acc: 0.8281 (avg: 0.8399) | LR: 1.34e-04
  Batch  350/1500 ( 23.3%) | Loss: 0.6000 (avg: 0.5940) | Acc: 0.8438 (avg: 0.8404) | LR: 1.39e-04
  Batch  400/1500 ( 26.7%) | Loss: 0.8115 (avg: 0.5933) | Acc: 0.7812 (avg: 0.8414) | LR: 1.45e-04
  Batch  450/1500 ( 30.0%) | Loss: 0.7336 (avg: 0.5899) | Acc: 0.7812 (avg: 0.8434) | LR: 1.50e-04
  Batch  500/1500 ( 33.3%) | Loss: 0.6830 (avg: 0.5891) | Acc: 0.8438 (avg: 0.8442) | LR: 1.55e-04
  Batch  550/1500 ( 36.7%) | Loss: 0.5383 (avg: 0.5878) | Acc: 0.8438 (avg: 0.8447) | LR: 1.61e-04
  Batch  600/1500 ( 40.0%) | Loss: 0.5486 (avg: 0.5860) | Acc: 0.8594 (avg: 0.8454) | LR: 1.67e-04
  Batch  650/1500 ( 43.3%) | Loss: 0.4377 (avg: 0.5840) | Acc: 0.8750 (avg: 0.8462) | LR: 1.72e-04
  Batch  700/1500 ( 46.7%) | Loss: 0.4643 (avg: 0.5834) | Acc: 0.8906 (avg: 0.8464) | LR: 1.78e-04
  Batch  750/1500 ( 50.0%) | Loss: 0.5633 (avg: 0.5813) | Acc: 0.8750 (avg: 0.8473) | LR: 1.84e-04
  Batch  800/1500 ( 53.3%) | Loss: 0.5518 (avg: 0.5804) | Acc: 0.8750 (avg: 0.8478) | LR: 1.90e-04
  Batch  850/1500 ( 56.7%) | Loss: 0.6247 (avg: 0.5800) | Acc: 0.8594 (avg: 0.8481) | LR: 1.97e-04
  Batch  900/1500 ( 60.0%) | Loss: 0.5493 (avg: 0.5781) | Acc: 0.8438 (avg: 0.8494) | LR: 2.03e-04
  Batch  950/1500 ( 63.3%) | Loss: 0.5868 (avg: 0.5758) | Acc: 0.8125 (avg: 0.8501) | LR: 2.09e-04
  Batch 1000/1500 ( 66.7%) | Loss: 0.5019 (avg: 0.5740) | Acc: 0.9062 (avg: 0.8508) | LR: 2.16e-04
  Batch 1050/1500 ( 70.0%) | Loss: 0.4791 (avg: 0.5729) | Acc: 0.8594 (avg: 0.8515) | LR: 2.22e-04
  Batch 1100/1500 ( 73.3%) | Loss: 0.5817 (avg: 0.5717) | Acc: 0.7969 (avg: 0.8519) | LR: 2.29e-04
  Batch 1150/1500 ( 76.7%) | Loss: 0.5881 (avg: 0.5697) | Acc: 0.8594 (avg: 0.8527) | LR: 2.36e-04
  Batch 1200/1500 ( 80.0%) | Loss: 0.4790 (avg: 0.5678) | Acc: 0.9062 (avg: 0.8530) | LR: 2.43e-04
  Batch 1250/1500 ( 83.3%) | Loss: 0.5180 (avg: 0.5668) | Acc: 0.9062 (avg: 0.8534) | LR: 2.50e-04
  Batch 1300/1500 ( 86.7%) | Loss: 0.4592 (avg: 0.5658) | Acc: 0.9219 (avg: 0.8537) | LR: 2.57e-04
  Batch 1350/1500 ( 90.0%) | Loss: 0.5080 (avg: 0.5641) | Acc: 0.8906 (avg: 0.8544) | LR: 2.64e-04
  Batch 1400/1500 ( 93.3%) | Loss: 0.4347 (avg: 0.5632) | Acc: 0.9062 (avg: 0.8546) | LR: 2.71e-04
  Batch 1450/1500 ( 96.7%) | Loss: 0.5084 (avg: 0.5617) | Acc: 0.8750 (avg: 0.8553) | LR: 2.78e-04
  Batch 1500/1500 (100.0%) | Loss: 0.5341 (avg: 0.5603) | Acc: 0.8281 (avg: 0.8560) | LR: 2.86e-04

Train Summary:
  Final Loss: 0.5603 | Final Acc: 0.8560
  Final LR: 2.86e-04
  Loss std: 0.0833 | Acc std: 0.0445

Val Epoch 2 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4905 | Final Acc: 0.8901
  Loss std: 0.0735 | Acc std: 0.0396
Epoch 02/20 | train 0.5603/0.8560 | val 0.4905/0.8901

Train Epoch 3 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.4994 (avg: 0.4514) | Acc: 0.8438 (avg: 0.9047) | LR: 2.93e-04
  Batch  100/1500 (  6.7%) | Loss: 0.4798 (avg: 0.4496) | Acc: 0.8438 (avg: 0.9038) | LR: 3.01e-04
  Batch  150/1500 ( 10.0%) | Loss: 0.4045 (avg: 0.4532) | Acc: 0.9375 (avg: 0.9053) | LR: 3.08e-04
  Batch  200/1500 ( 13.3%) | Loss: 0.4558 (avg: 0.4508) | Acc: 0.9062 (avg: 0.9059) | LR: 3.16e-04
  Batch  250/1500 ( 16.7%) | Loss: 0.4675 (avg: 0.4529) | Acc: 0.9062 (avg: 0.9055) | LR: 3.24e-04
  Batch  300/1500 ( 20.0%) | Loss: 0.4756 (avg: 0.4556) | Acc: 0.9062 (avg: 0.9039) | LR: 3.31e-04
  Batch  350/1500 ( 23.3%) | Loss: 0.4663 (avg: 0.4578) | Acc: 0.9375 (avg: 0.9032) | LR: 3.39e-04
  Batch  400/1500 ( 26.7%) | Loss: 0.4781 (avg: 0.4595) | Acc: 0.8906 (avg: 0.9032) | LR: 3.47e-04
  Batch  450/1500 ( 30.0%) | Loss: 0.4616 (avg: 0.4603) | Acc: 0.8906 (avg: 0.9028) | LR: 3.55e-04
  Batch  500/1500 ( 33.3%) | Loss: 0.4564 (avg: 0.4599) | Acc: 0.9062 (avg: 0.9029) | LR: 3.63e-04
  Batch  550/1500 ( 36.7%) | Loss: 0.5896 (avg: 0.4603) | Acc: 0.8438 (avg: 0.9027) | LR: 3.71e-04
  Batch  600/1500 ( 40.0%) | Loss: 0.4609 (avg: 0.4608) | Acc: 0.8594 (avg: 0.9028) | LR: 3.79e-04
  Batch  650/1500 ( 43.3%) | Loss: 0.5475 (avg: 0.4627) | Acc: 0.8750 (avg: 0.9020) | LR: 3.87e-04
  Batch  700/1500 ( 46.7%) | Loss: 0.4421 (avg: 0.4644) | Acc: 0.9062 (avg: 0.9011) | LR: 3.96e-04
  Batch  750/1500 ( 50.0%) | Loss: 0.3875 (avg: 0.4656) | Acc: 0.9219 (avg: 0.9003) | LR: 4.04e-04
  Batch  800/1500 ( 53.3%) | Loss: 0.4726 (avg: 0.4670) | Acc: 0.8906 (avg: 0.8999) | LR: 4.12e-04
  Batch  850/1500 ( 56.7%) | Loss: 0.4813 (avg: 0.4671) | Acc: 0.9062 (avg: 0.8998) | LR: 4.20e-04
  Batch  900/1500 ( 60.0%) | Loss: 0.5082 (avg: 0.4675) | Acc: 0.8750 (avg: 0.8994) | LR: 4.29e-04
  Batch  950/1500 ( 63.3%) | Loss: 0.4743 (avg: 0.4681) | Acc: 0.8594 (avg: 0.8989) | LR: 4.37e-04
  Batch 1000/1500 ( 66.7%) | Loss: 0.3999 (avg: 0.4684) | Acc: 0.9375 (avg: 0.8988) | LR: 4.46e-04
  Batch 1050/1500 ( 70.0%) | Loss: 0.4715 (avg: 0.4693) | Acc: 0.8438 (avg: 0.8983) | LR: 4.54e-04
  Batch 1100/1500 ( 73.3%) | Loss: 0.4375 (avg: 0.4708) | Acc: 0.9219 (avg: 0.8974) | LR: 4.62e-04
  Batch 1150/1500 ( 76.7%) | Loss: 0.4280 (avg: 0.4707) | Acc: 0.9219 (avg: 0.8972) | LR: 4.71e-04
  Batch 1200/1500 ( 80.0%) | Loss: 0.4719 (avg: 0.4717) | Acc: 0.8594 (avg: 0.8968) | LR: 4.79e-04
  Batch 1250/1500 ( 83.3%) | Loss: 0.5579 (avg: 0.4724) | Acc: 0.8906 (avg: 0.8966) | LR: 4.88e-04
  Batch 1300/1500 ( 86.7%) | Loss: 0.5535 (avg: 0.4732) | Acc: 0.9062 (avg: 0.8965) | LR: 4.96e-04
  Batch 1350/1500 ( 90.0%) | Loss: 0.3855 (avg: 0.4732) | Acc: 0.9375 (avg: 0.8962) | LR: 5.05e-04
  Batch 1400/1500 ( 93.3%) | Loss: 0.4826 (avg: 0.4737) | Acc: 0.9062 (avg: 0.8960) | LR: 5.13e-04
  Batch 1450/1500 ( 96.7%) | Loss: 0.5805 (avg: 0.4751) | Acc: 0.8438 (avg: 0.8955) | LR: 5.22e-04
  Batch 1500/1500 (100.0%) | Loss: 0.6429 (avg: 0.4760) | Acc: 0.7656 (avg: 0.8950) | LR: 5.31e-04

Train Summary:
  Final Loss: 0.4760 | Final Acc: 0.8950
  Final LR: 5.30e-04
  Loss std: 0.0703 | Acc std: 0.0378

Val Epoch 3 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4578 | Final Acc: 0.9044
  Loss std: 0.0747 | Acc std: 0.0375
Epoch 03/20 | train 0.4760/0.8950 | val 0.4578/0.9044

Train Epoch 4 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.4022 (avg: 0.3848) | Acc: 0.9688 (avg: 0.9384) | LR: 5.39e-04
  Batch  100/1500 (  6.7%) | Loss: 0.3526 (avg: 0.3989) | Acc: 0.9375 (avg: 0.9325) | LR: 5.48e-04
  Batch  150/1500 ( 10.0%) | Loss: 0.4158 (avg: 0.4034) | Acc: 0.9219 (avg: 0.9298) | LR: 5.56e-04
  Batch  200/1500 ( 13.3%) | Loss: 0.4831 (avg: 0.4019) | Acc: 0.9062 (avg: 0.9305) | LR: 5.65e-04
  Batch  250/1500 ( 16.7%) | Loss: 0.4201 (avg: 0.4005) | Acc: 0.8906 (avg: 0.9303) | LR: 5.73e-04
  Batch  300/1500 ( 20.0%) | Loss: 0.4147 (avg: 0.4037) | Acc: 0.9219 (avg: 0.9294) | LR: 5.82e-04
  Batch  350/1500 ( 23.3%) | Loss: 0.4234 (avg: 0.4084) | Acc: 0.9062 (avg: 0.9267) | LR: 5.90e-04
  Batch  400/1500 ( 26.7%) | Loss: 0.4678 (avg: 0.4095) | Acc: 0.9062 (avg: 0.9261) | LR: 5.99e-04
  Batch  450/1500 ( 30.0%) | Loss: 0.3664 (avg: 0.4124) | Acc: 0.9531 (avg: 0.9253) | LR: 6.07e-04
  Batch  500/1500 ( 33.3%) | Loss: 0.3937 (avg: 0.4123) | Acc: 0.9531 (avg: 0.9252) | LR: 6.16e-04
  Batch  550/1500 ( 36.7%) | Loss: 0.3658 (avg: 0.4120) | Acc: 0.9531 (avg: 0.9252) | LR: 6.24e-04
  Batch  600/1500 ( 40.0%) | Loss: 0.4368 (avg: 0.4139) | Acc: 0.8906 (avg: 0.9241) | LR: 6.32e-04
  Batch  650/1500 ( 43.3%) | Loss: 0.5015 (avg: 0.4153) | Acc: 0.8906 (avg: 0.9235) | LR: 6.41e-04
  Batch  700/1500 ( 46.7%) | Loss: 0.5657 (avg: 0.4169) | Acc: 0.8594 (avg: 0.9227) | LR: 6.49e-04
  Batch  750/1500 ( 50.0%) | Loss: 0.3997 (avg: 0.4184) | Acc: 0.9062 (avg: 0.9222) | LR: 6.57e-04
  Batch  800/1500 ( 53.3%) | Loss: 0.5779 (avg: 0.4212) | Acc: 0.8438 (avg: 0.9211) | LR: 6.66e-04
  Batch  850/1500 ( 56.7%) | Loss: 0.4212 (avg: 0.4227) | Acc: 0.8750 (avg: 0.9204) | LR: 6.74e-04
  Batch  900/1500 ( 60.0%) | Loss: 0.4261 (avg: 0.4238) | Acc: 0.9219 (avg: 0.9200) | LR: 6.82e-04
  Batch  950/1500 ( 63.3%) | Loss: 0.3615 (avg: 0.4251) | Acc: 0.9375 (avg: 0.9193) | LR: 6.90e-04
  Batch 1000/1500 ( 66.7%) | Loss: 0.3959 (avg: 0.4274) | Acc: 0.9219 (avg: 0.9183) | LR: 6.98e-04
  Batch 1050/1500 ( 70.0%) | Loss: 0.4224 (avg: 0.4290) | Acc: 0.9219 (avg: 0.9173) | LR: 7.06e-04
  Batch 1100/1500 ( 73.3%) | Loss: 0.4658 (avg: 0.4300) | Acc: 0.9062 (avg: 0.9167) | LR: 7.14e-04
  Batch 1150/1500 ( 76.7%) | Loss: 0.4578 (avg: 0.4310) | Acc: 0.8906 (avg: 0.9161) | LR: 7.22e-04
  Batch 1200/1500 ( 80.0%) | Loss: 0.4718 (avg: 0.4320) | Acc: 0.9062 (avg: 0.9157) | LR: 7.30e-04
  Batch 1250/1500 ( 83.3%) | Loss: 0.4522 (avg: 0.4333) | Acc: 0.9062 (avg: 0.9152) | LR: 7.38e-04
  Batch 1300/1500 ( 86.7%) | Loss: 0.4986 (avg: 0.4342) | Acc: 0.8750 (avg: 0.9147) | LR: 7.45e-04
  Batch 1350/1500 ( 90.0%) | Loss: 0.3648 (avg: 0.4349) | Acc: 0.9688 (avg: 0.9144) | LR: 7.53e-04
  Batch 1400/1500 ( 93.3%) | Loss: 0.5578 (avg: 0.4362) | Acc: 0.8125 (avg: 0.9137) | LR: 7.60e-04
  Batch 1450/1500 ( 96.7%) | Loss: 0.4561 (avg: 0.4367) | Acc: 0.8594 (avg: 0.9134) | LR: 7.68e-04
  Batch 1500/1500 (100.0%) | Loss: 0.4705 (avg: 0.4380) | Acc: 0.9062 (avg: 0.9128) | LR: 7.75e-04

Train Summary:
  Final Loss: 0.4380 | Final Acc: 0.9128
  Final LR: 7.75e-04
  Loss std: 0.0705 | Acc std: 0.0374

Val Epoch 4 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4456 | Final Acc: 0.9096
  Loss std: 0.0785 | Acc std: 0.0374
Epoch 04/20 | train 0.4380/0.9128 | val 0.4456/0.9096

Train Epoch 5 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.4815 (avg: 0.3601) | Acc: 0.8750 (avg: 0.9513) | LR: 7.83e-04
  Batch  100/1500 (  6.7%) | Loss: 0.3745 (avg: 0.3688) | Acc: 0.9375 (avg: 0.9461) | LR: 7.90e-04
  Batch  150/1500 ( 10.0%) | Loss: 0.4154 (avg: 0.3733) | Acc: 0.9062 (avg: 0.9450) | LR: 7.97e-04
  Batch  200/1500 ( 13.3%) | Loss: 0.4376 (avg: 0.3767) | Acc: 0.9375 (avg: 0.9425) | LR: 8.04e-04
  Batch  250/1500 ( 16.7%) | Loss: 0.3617 (avg: 0.3760) | Acc: 0.9375 (avg: 0.9424) | LR: 8.11e-04
  Batch  300/1500 ( 20.0%) | Loss: 0.3187 (avg: 0.3761) | Acc: 0.9531 (avg: 0.9416) | LR: 8.18e-04
  Batch  350/1500 ( 23.3%) | Loss: 0.4753 (avg: 0.3788) | Acc: 0.8906 (avg: 0.9408) | LR: 8.25e-04
  Batch  400/1500 ( 26.7%) | Loss: 0.3194 (avg: 0.3812) | Acc: 0.9688 (avg: 0.9404) | LR: 8.32e-04
  Batch  450/1500 ( 30.0%) | Loss: 0.4279 (avg: 0.3836) | Acc: 0.9062 (avg: 0.9393) | LR: 8.39e-04
  Batch  500/1500 ( 33.3%) | Loss: 0.3305 (avg: 0.3867) | Acc: 0.9844 (avg: 0.9373) | LR: 8.45e-04
  Batch  550/1500 ( 36.7%) | Loss: 0.4883 (avg: 0.3893) | Acc: 0.8750 (avg: 0.9361) | LR: 8.52e-04
  Batch  600/1500 ( 40.0%) | Loss: 0.4398 (avg: 0.3913) | Acc: 0.9062 (avg: 0.9352) | LR: 8.58e-04
  Batch  650/1500 ( 43.3%) | Loss: 0.3495 (avg: 0.3923) | Acc: 0.9531 (avg: 0.9345) | LR: 8.65e-04
  Batch  700/1500 ( 46.7%) | Loss: 0.3991 (avg: 0.3934) | Acc: 0.9219 (avg: 0.9339) | LR: 8.71e-04
  Batch  750/1500 ( 50.0%) | Loss: 0.4456 (avg: 0.3940) | Acc: 0.9219 (avg: 0.9338) | LR: 8.77e-04
  Batch  800/1500 ( 53.3%) | Loss: 0.4545 (avg: 0.3951) | Acc: 0.8750 (avg: 0.9331) | LR: 8.83e-04
  Batch  850/1500 ( 56.7%) | Loss: 0.4579 (avg: 0.3966) | Acc: 0.9531 (avg: 0.9328) | LR: 8.89e-04
  Batch  900/1500 ( 60.0%) | Loss: 0.5313 (avg: 0.3976) | Acc: 0.8594 (avg: 0.9320) | LR: 8.94e-04
  Batch  950/1500 ( 63.3%) | Loss: 0.4993 (avg: 0.3988) | Acc: 0.9062 (avg: 0.9316) | LR: 9.00e-04
  Batch 1000/1500 ( 66.7%) | Loss: 0.3871 (avg: 0.3991) | Acc: 0.9375 (avg: 0.9316) | LR: 9.06e-04
  Batch 1050/1500 ( 70.0%) | Loss: 0.4008 (avg: 0.4001) | Acc: 0.9531 (avg: 0.9313) | LR: 9.11e-04
  Batch 1100/1500 ( 73.3%) | Loss: 0.4006 (avg: 0.4010) | Acc: 0.9219 (avg: 0.9310) | LR: 9.16e-04
  Batch 1150/1500 ( 76.7%) | Loss: 0.3541 (avg: 0.4022) | Acc: 0.9531 (avg: 0.9304) | LR: 9.22e-04
  Batch 1200/1500 ( 80.0%) | Loss: 0.4322 (avg: 0.4027) | Acc: 0.9375 (avg: 0.9303) | LR: 9.27e-04
  Batch 1250/1500 ( 83.3%) | Loss: 0.4719 (avg: 0.4038) | Acc: 0.8750 (avg: 0.9300) | LR: 9.32e-04
  Batch 1300/1500 ( 86.7%) | Loss: 0.4516 (avg: 0.4048) | Acc: 0.8281 (avg: 0.9295) | LR: 9.37e-04
  Batch 1350/1500 ( 90.0%) | Loss: 0.4413 (avg: 0.4058) | Acc: 0.9531 (avg: 0.9289) | LR: 9.41e-04
  Batch 1400/1500 ( 93.3%) | Loss: 0.3947 (avg: 0.4065) | Acc: 0.9219 (avg: 0.9286) | LR: 9.46e-04
  Batch 1450/1500 ( 96.7%) | Loss: 0.5702 (avg: 0.4078) | Acc: 0.8906 (avg: 0.9280) | LR: 9.50e-04
  Batch 1500/1500 (100.0%) | Loss: 0.4258 (avg: 0.4089) | Acc: 0.9219 (avg: 0.9274) | LR: 9.55e-04

Train Summary:
  Final Loss: 0.4089 | Final Acc: 0.9274
  Final LR: 9.55e-04
  Loss std: 0.0647 | Acc std: 0.0342

Val Epoch 5 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4873 | Final Acc: 0.8913
  Loss std: 0.0815 | Acc std: 0.0404
Epoch 05/20 | train 0.4089/0.9274 | val 0.4873/0.8913

Train Epoch 6 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.3488 (avg: 0.3526) | Acc: 0.9688 (avg: 0.9553) | LR: 9.59e-04
  Batch  100/1500 (  6.7%) | Loss: 0.3004 (avg: 0.3509) | Acc: 1.0000 (avg: 0.9553) | LR: 9.63e-04
  Batch  150/1500 ( 10.0%) | Loss: 0.3853 (avg: 0.3492) | Acc: 0.9531 (avg: 0.9559) | LR: 9.67e-04
  Batch  200/1500 ( 13.3%) | Loss: 0.3647 (avg: 0.3517) | Acc: 0.9531 (avg: 0.9541) | LR: 9.71e-04
  Batch  250/1500 ( 16.7%) | Loss: 0.4436 (avg: 0.3528) | Acc: 0.8750 (avg: 0.9532) | LR: 9.74e-04
  Batch  300/1500 ( 20.0%) | Loss: 0.3513 (avg: 0.3547) | Acc: 0.9531 (avg: 0.9525) | LR: 9.78e-04
  Batch  350/1500 ( 23.3%) | Loss: 0.3597 (avg: 0.3569) | Acc: 0.9375 (avg: 0.9521) | LR: 9.81e-04
  Batch  400/1500 ( 26.7%) | Loss: 0.4489 (avg: 0.3610) | Acc: 0.8750 (avg: 0.9502) | LR: 9.85e-04
  Batch  450/1500 ( 30.0%) | Loss: 0.3755 (avg: 0.3622) | Acc: 0.9531 (avg: 0.9494) | LR: 9.88e-04
  Batch  500/1500 ( 33.3%) | Loss: 0.4557 (avg: 0.3620) | Acc: 0.9375 (avg: 0.9500) | LR: 9.91e-04
  Batch  550/1500 ( 36.7%) | Loss: 0.3763 (avg: 0.3632) | Acc: 0.9375 (avg: 0.9495) | LR: 9.94e-04
  Batch  600/1500 ( 40.0%) | Loss: 0.3410 (avg: 0.3649) | Acc: 0.9688 (avg: 0.9489) | LR: 9.96e-04
  Batch  650/1500 ( 43.3%) | Loss: 0.6839 (avg: 0.3663) | Acc: 0.8594 (avg: 0.9480) | LR: 9.99e-04
  Batch  700/1500 ( 46.7%) | Loss: 0.3724 (avg: 0.3669) | Acc: 0.9375 (avg: 0.9474) | LR: 1.00e-03
  Batch  750/1500 ( 50.0%) | Loss: 0.3641 (avg: 0.3690) | Acc: 0.9375 (avg: 0.9467) | LR: 1.00e-03
  Batch  800/1500 ( 53.3%) | Loss: 0.4262 (avg: 0.3697) | Acc: 0.9062 (avg: 0.9460) | LR: 1.01e-03
  Batch  850/1500 ( 56.7%) | Loss: 0.3570 (avg: 0.3707) | Acc: 0.9531 (avg: 0.9455) | LR: 1.01e-03
  Batch  900/1500 ( 60.0%) | Loss: 0.3848 (avg: 0.3723) | Acc: 0.9688 (avg: 0.9448) | LR: 1.01e-03
  Batch  950/1500 ( 63.3%) | Loss: 0.3500 (avg: 0.3742) | Acc: 0.9375 (avg: 0.9439) | LR: 1.01e-03
  Batch 1000/1500 ( 66.7%) | Loss: 0.4648 (avg: 0.3753) | Acc: 0.9062 (avg: 0.9433) | LR: 1.01e-03
  Batch 1050/1500 ( 70.0%) | Loss: 0.3928 (avg: 0.3763) | Acc: 0.9375 (avg: 0.9428) | LR: 1.01e-03
  Batch 1100/1500 ( 73.3%) | Loss: 0.3977 (avg: 0.3761) | Acc: 0.9219 (avg: 0.9428) | LR: 1.02e-03
  Batch 1150/1500 ( 76.7%) | Loss: 0.4096 (avg: 0.3767) | Acc: 0.9219 (avg: 0.9426) | LR: 1.02e-03
  Batch 1200/1500 ( 80.0%) | Loss: 0.3862 (avg: 0.3777) | Acc: 0.9375 (avg: 0.9421) | LR: 1.02e-03
  Batch 1250/1500 ( 83.3%) | Loss: 0.4237 (avg: 0.3786) | Acc: 0.9375 (avg: 0.9417) | LR: 1.02e-03
  Batch 1300/1500 ( 86.7%) | Loss: 0.3906 (avg: 0.3790) | Acc: 0.8906 (avg: 0.9415) | LR: 1.02e-03
  Batch 1350/1500 ( 90.0%) | Loss: 0.3640 (avg: 0.3807) | Acc: 0.9375 (avg: 0.9406) | LR: 1.02e-03
  Batch 1400/1500 ( 93.3%) | Loss: 0.2975 (avg: 0.3810) | Acc: 0.9688 (avg: 0.9406) | LR: 1.02e-03
  Batch 1450/1500 ( 96.7%) | Loss: 0.3827 (avg: 0.3821) | Acc: 0.9375 (avg: 0.9399) | LR: 1.02e-03
  Batch 1500/1500 (100.0%) | Loss: 0.4520 (avg: 0.3829) | Acc: 0.9219 (avg: 0.9395) | LR: 1.02e-03

Train Summary:
  Final Loss: 0.3829 | Final Acc: 0.9395
  Final LR: 1.02e-03
  Loss std: 0.0604 | Acc std: 0.0320

Val Epoch 6 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4634 | Final Acc: 0.9065
  Loss std: 0.0874 | Acc std: 0.0376
Epoch 06/20 | train 0.3829/0.9395 | val 0.4634/0.9065

Train Epoch 7 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.3128 (avg: 0.3238) | Acc: 0.9688 (avg: 0.9656) | LR: 1.02e-03
  Batch  100/1500 (  6.7%) | Loss: 0.3173 (avg: 0.3299) | Acc: 0.9844 (avg: 0.9627) | LR: 1.02e-03
  Batch  150/1500 ( 10.0%) | Loss: 0.3254 (avg: 0.3316) | Acc: 0.9688 (avg: 0.9609) | LR: 1.02e-03
  Batch  200/1500 ( 13.3%) | Loss: 0.3891 (avg: 0.3304) | Acc: 0.9531 (avg: 0.9622) | LR: 1.02e-03
  Batch  250/1500 ( 16.7%) | Loss: 0.4010 (avg: 0.3332) | Acc: 0.8906 (avg: 0.9607) | LR: 1.02e-03
  Batch  300/1500 ( 20.0%) | Loss: 0.3248 (avg: 0.3346) | Acc: 0.9531 (avg: 0.9601) | LR: 1.02e-03
  Batch  350/1500 ( 23.3%) | Loss: 0.3081 (avg: 0.3350) | Acc: 0.9688 (avg: 0.9600) | LR: 1.02e-03
  Batch  400/1500 ( 26.7%) | Loss: 0.4954 (avg: 0.3364) | Acc: 0.8750 (avg: 0.9598) | LR: 1.02e-03
  Batch  450/1500 ( 30.0%) | Loss: 0.3674 (avg: 0.3370) | Acc: 0.9531 (avg: 0.9598) | LR: 1.02e-03
  Batch  500/1500 ( 33.3%) | Loss: 0.3400 (avg: 0.3377) | Acc: 0.9688 (avg: 0.9598) | LR: 1.02e-03
  Batch  550/1500 ( 36.7%) | Loss: 0.3297 (avg: 0.3393) | Acc: 0.9531 (avg: 0.9590) | LR: 1.02e-03
  Batch  600/1500 ( 40.0%) | Loss: 0.3413 (avg: 0.3405) | Acc: 0.9688 (avg: 0.9586) | LR: 1.02e-03
  Batch  650/1500 ( 43.3%) | Loss: 0.3736 (avg: 0.3411) | Acc: 0.9531 (avg: 0.9585) | LR: 1.02e-03
  Batch  700/1500 ( 46.7%) | Loss: 0.4356 (avg: 0.3414) | Acc: 0.8906 (avg: 0.9581) | LR: 1.02e-03
  Batch  750/1500 ( 50.0%) | Loss: 0.3035 (avg: 0.3425) | Acc: 0.9688 (avg: 0.9574) | LR: 1.02e-03
  Batch  800/1500 ( 53.3%) | Loss: 0.3263 (avg: 0.3433) | Acc: 0.9688 (avg: 0.9571) | LR: 1.02e-03
  Batch  850/1500 ( 56.7%) | Loss: 0.3491 (avg: 0.3443) | Acc: 0.9531 (avg: 0.9567) | LR: 1.02e-03
  Batch  900/1500 ( 60.0%) | Loss: 0.3186 (avg: 0.3448) | Acc: 0.9688 (avg: 0.9564) | LR: 1.02e-03
  Batch  950/1500 ( 63.3%) | Loss: 0.2699 (avg: 0.3457) | Acc: 1.0000 (avg: 0.9561) | LR: 1.02e-03
  Batch 1000/1500 ( 66.7%) | Loss: 0.3902 (avg: 0.3466) | Acc: 0.8750 (avg: 0.9556) | LR: 1.01e-03
  Batch 1050/1500 ( 70.0%) | Loss: 0.3573 (avg: 0.3476) | Acc: 0.9219 (avg: 0.9553) | LR: 1.01e-03
  Batch 1100/1500 ( 73.3%) | Loss: 0.3173 (avg: 0.3482) | Acc: 0.9688 (avg: 0.9549) | LR: 1.01e-03
  Batch 1150/1500 ( 76.7%) | Loss: 0.3758 (avg: 0.3491) | Acc: 0.9531 (avg: 0.9545) | LR: 1.01e-03
  Batch 1200/1500 ( 80.0%) | Loss: 0.3921 (avg: 0.3495) | Acc: 0.9375 (avg: 0.9543) | LR: 1.01e-03
  Batch 1250/1500 ( 83.3%) | Loss: 0.3643 (avg: 0.3498) | Acc: 0.9219 (avg: 0.9540) | LR: 1.01e-03
  Batch 1300/1500 ( 86.7%) | Loss: 0.5146 (avg: 0.3503) | Acc: 0.9062 (avg: 0.9537) | LR: 1.01e-03
  Batch 1350/1500 ( 90.0%) | Loss: 0.3996 (avg: 0.3506) | Acc: 0.9375 (avg: 0.9537) | LR: 1.01e-03
  Batch 1400/1500 ( 93.3%) | Loss: 0.3322 (avg: 0.3508) | Acc: 0.9531 (avg: 0.9537) | LR: 1.01e-03
  Batch 1450/1500 ( 96.7%) | Loss: 0.3981 (avg: 0.3518) | Acc: 0.9219 (avg: 0.9533) | LR: 1.01e-03
  Batch 1500/1500 (100.0%) | Loss: 0.3608 (avg: 0.3530) | Acc: 0.9375 (avg: 0.9528) | LR: 1.01e-03

Train Summary:
  Final Loss: 0.3530 | Final Acc: 0.9528
  Final LR: 1.01e-03
  Loss std: 0.0499 | Acc std: 0.0271

Val Epoch 7 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4996 | Final Acc: 0.8930
  Loss std: 0.0912 | Acc std: 0.0400
Epoch 07/20 | train 0.3530/0.9528 | val 0.4996/0.8930
Trial pruned at epoch 7
Trial 5 failed:
wandb:
wandb: Run history:
wandb:               batch ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÖ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñà
wandb:          best_epoch ‚ñÅ‚ñÉ‚ñÜ‚ñà
wandb:        best_val_acc ‚ñÅ‚ñÜ‚ñá‚ñà
wandb:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: final_learning_rate ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà
wandb:       learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:           train_acc ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:       train_acc_std ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:       train_avg_acc ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:      train_avg_loss ‚ñà‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                 +12 ...
wandb:
wandb: Run summary:
wandb:               batch 1500
wandb:          best_epoch 4
wandb:        best_val_acc 0.90963
wandb:               epoch 7
wandb:               error
wandb: final_learning_rate 0.00101
wandb:       learning_rate 0.00101
wandb:              status failed
wandb:           train_acc 0.95282
wandb:       train_acc_std 0.02709
wandb:                 +14 ...
wandb:
wandb: üöÄ View run trial_005 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/e19r722p
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_064839-e19r722p/logs
[I 2025-10-06 06:58:27,186] Trial 5 pruned.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_065827-0y6rmuuf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run revived-jazz-14
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/0y6rmuuf
wandb: Finishing previous runs because reinit is set to True.
wandb:
wandb: Run history:
wandb:   batch_size ‚ñÅ
wandb:     channels ‚ñÅ
wandb:      dropout ‚ñÅ
wandb:      emb_dim ‚ñÅ
wandb:    grad_clip ‚ñÅ
wandb:           lr ‚ñÅ
wandb:      max_len ‚ñÅ
wandb:      val_acc ‚ñÅ
wandb: weight_decay ‚ñÅ
wandb:
wandb: Run summary:
wandb:   batch_size 64
wandb:     channels 192
wandb:      dropout 0.3
wandb:      emb_dim 300
wandb:    grad_clip 1
wandb:           lr 0.00102
wandb:      max_len 160
wandb:    optimizer adamw
wandb:      val_acc 0.893
wandb: weight_decay 4e-05
wandb:
wandb: üöÄ View run trial/5/revived-jazz-14 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/0y6rmuuf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_065827-0y6rmuuf/logs
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_065827-g2u1e6v3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_006
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/g2u1e6v3

Train Epoch 1 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 1.2724 (avg: 1.3370) | Acc: 0.4453 (avg: 0.3681) | LR: 1.39e-04
  Batch  100/750 ( 13.3%) | Loss: 1.0263 (avg: 1.2438) | Acc: 0.6797 (avg: 0.4618) | LR: 1.42e-04
  Batch  150/750 ( 20.0%) | Loss: 0.9263 (avg: 1.1659) | Acc: 0.6875 (avg: 0.5271) | LR: 1.47e-04
  Batch  200/750 ( 26.7%) | Loss: 0.8152 (avg: 1.0983) | Acc: 0.7734 (avg: 0.5737) | LR: 1.54e-04
  Batch  250/750 ( 33.3%) | Loss: 0.8253 (avg: 1.0411) | Acc: 0.7578 (avg: 0.6081) | LR: 1.63e-04
  Batch  300/750 ( 40.0%) | Loss: 0.6832 (avg: 0.9925) | Acc: 0.8203 (avg: 0.6348) | LR: 1.74e-04
  Batch  350/750 ( 46.7%) | Loss: 0.7379 (avg: 0.9516) | Acc: 0.7891 (avg: 0.6563) | LR: 1.87e-04
  Batch  400/750 ( 53.3%) | Loss: 0.6692 (avg: 0.9186) | Acc: 0.8047 (avg: 0.6736) | LR: 2.02e-04
  Batch  450/750 ( 60.0%) | Loss: 0.6618 (avg: 0.8884) | Acc: 0.7734 (avg: 0.6893) | LR: 2.19e-04
  Batch  500/750 ( 66.7%) | Loss: 0.7489 (avg: 0.8626) | Acc: 0.7500 (avg: 0.7022) | LR: 2.38e-04
  Batch  550/750 ( 73.3%) | Loss: 0.5774 (avg: 0.8399) | Acc: 0.8359 (avg: 0.7142) | LR: 2.58e-04
  Batch  600/750 ( 80.0%) | Loss: 0.5926 (avg: 0.8200) | Acc: 0.8672 (avg: 0.7243) | LR: 2.81e-04
  Batch  650/750 ( 86.7%) | Loss: 0.5449 (avg: 0.8025) | Acc: 0.8594 (avg: 0.7333) | LR: 3.05e-04
  Batch  700/750 ( 93.3%) | Loss: 0.5792 (avg: 0.7870) | Acc: 0.8359 (avg: 0.7411) | LR: 3.32e-04
  Batch  750/750 (100.0%) | Loss: 0.5695 (avg: 0.7723) | Acc: 0.8516 (avg: 0.7486) | LR: 3.60e-04

Train Summary:
  Final Loss: 0.7723 | Final Acc: 0.7486
  Final LR: 3.59e-04
  Loss std: 0.2301 | Acc std: 0.1359

Val Epoch 1 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5248 | Final Acc: 0.8738
  Loss std: 0.0498 | Acc std: 0.0284
Epoch 01/20 | train 0.7723/0.7486 | val 0.5248/0.8738

Train Epoch 2 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.5442 (avg: 0.4820) | Acc: 0.8828 (avg: 0.8942) | LR: 3.89e-04
  Batch  100/750 ( 13.3%) | Loss: 0.5181 (avg: 0.4847) | Acc: 0.8516 (avg: 0.8940) | LR: 4.21e-04
  Batch  150/750 ( 20.0%) | Loss: 0.4865 (avg: 0.4840) | Acc: 0.8672 (avg: 0.8933) | LR: 4.54e-04
  Batch  200/750 ( 26.7%) | Loss: 0.4968 (avg: 0.4836) | Acc: 0.9062 (avg: 0.8921) | LR: 4.89e-04
  Batch  250/750 ( 33.3%) | Loss: 0.5539 (avg: 0.4862) | Acc: 0.8594 (avg: 0.8908) | LR: 5.25e-04
  Batch  300/750 ( 40.0%) | Loss: 0.4534 (avg: 0.4864) | Acc: 0.9219 (avg: 0.8899) | LR: 5.63e-04
  Batch  350/750 ( 46.7%) | Loss: 0.4833 (avg: 0.4879) | Acc: 0.8750 (avg: 0.8896) | LR: 6.02e-04
  Batch  400/750 ( 53.3%) | Loss: 0.4715 (avg: 0.4909) | Acc: 0.9141 (avg: 0.8885) | LR: 6.43e-04
  Batch  450/750 ( 60.0%) | Loss: 0.4928 (avg: 0.4915) | Acc: 0.9062 (avg: 0.8882) | LR: 6.85e-04
  Batch  500/750 ( 66.7%) | Loss: 0.4960 (avg: 0.4934) | Acc: 0.8750 (avg: 0.8869) | LR: 7.29e-04
  Batch  550/750 ( 73.3%) | Loss: 0.4726 (avg: 0.4948) | Acc: 0.8906 (avg: 0.8860) | LR: 7.74e-04
  Batch  600/750 ( 80.0%) | Loss: 0.5427 (avg: 0.4958) | Acc: 0.8828 (avg: 0.8855) | LR: 8.20e-04
  Batch  650/750 ( 86.7%) | Loss: 0.5695 (avg: 0.4968) | Acc: 0.8438 (avg: 0.8850) | LR: 8.67e-04
  Batch  700/750 ( 93.3%) | Loss: 0.4783 (avg: 0.4976) | Acc: 0.8906 (avg: 0.8847) | LR: 9.16e-04
  Batch  750/750 (100.0%) | Loss: 0.4949 (avg: 0.4975) | Acc: 0.8828 (avg: 0.8845) | LR: 9.65e-04

Train Summary:
  Final Loss: 0.4975 | Final Acc: 0.8845
  Final LR: 9.64e-04
  Loss std: 0.0525 | Acc std: 0.0283

Val Epoch 2 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4746 | Final Acc: 0.8935
  Loss std: 0.0501 | Acc std: 0.0279
Epoch 02/20 | train 0.4975/0.8845 | val 0.4746/0.8935

Train Epoch 3 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.3853 (avg: 0.3988) | Acc: 0.9375 (avg: 0.9295) | LR: 1.02e-03
  Batch  100/750 ( 13.3%) | Loss: 0.4026 (avg: 0.3971) | Acc: 0.9375 (avg: 0.9317) | LR: 1.07e-03
  Batch  150/750 ( 20.0%) | Loss: 0.4155 (avg: 0.4025) | Acc: 0.9219 (avg: 0.9296) | LR: 1.12e-03
  Batch  200/750 ( 26.7%) | Loss: 0.3922 (avg: 0.4075) | Acc: 0.9375 (avg: 0.9278) | LR: 1.17e-03
  Batch  250/750 ( 33.3%) | Loss: 0.3991 (avg: 0.4118) | Acc: 0.9297 (avg: 0.9250) | LR: 1.23e-03
  Batch  300/750 ( 40.0%) | Loss: 0.5048 (avg: 0.4179) | Acc: 0.8828 (avg: 0.9220) | LR: 1.28e-03
  Batch  350/750 ( 46.7%) | Loss: 0.4614 (avg: 0.4232) | Acc: 0.9141 (avg: 0.9197) | LR: 1.34e-03
  Batch  400/750 ( 53.3%) | Loss: 0.4716 (avg: 0.4268) | Acc: 0.9219 (avg: 0.9178) | LR: 1.39e-03
  Batch  450/750 ( 60.0%) | Loss: 0.3962 (avg: 0.4299) | Acc: 0.9375 (avg: 0.9168) | LR: 1.45e-03
  Batch  500/750 ( 66.7%) | Loss: 0.4440 (avg: 0.4329) | Acc: 0.8984 (avg: 0.9155) | LR: 1.51e-03
  Batch  550/750 ( 73.3%) | Loss: 0.4528 (avg: 0.4357) | Acc: 0.8906 (avg: 0.9139) | LR: 1.56e-03
  Batch  600/750 ( 80.0%) | Loss: 0.4053 (avg: 0.4376) | Acc: 0.9141 (avg: 0.9131) | LR: 1.62e-03
  Batch  650/750 ( 86.7%) | Loss: 0.4027 (avg: 0.4399) | Acc: 0.9297 (avg: 0.9120) | LR: 1.68e-03
  Batch  700/750 ( 93.3%) | Loss: 0.4693 (avg: 0.4423) | Acc: 0.8906 (avg: 0.9111) | LR: 1.73e-03
  Batch  750/750 (100.0%) | Loss: 0.4593 (avg: 0.4426) | Acc: 0.8906 (avg: 0.9109) | LR: 1.79e-03

Train Summary:
  Final Loss: 0.4426 | Final Acc: 0.9109
  Final LR: 1.79e-03
  Loss std: 0.0517 | Acc std: 0.0277

Val Epoch 3 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4601 | Final Acc: 0.9046
  Loss std: 0.0535 | Acc std: 0.0273
Epoch 03/20 | train 0.4426/0.9109 | val 0.4601/0.9046

Train Epoch 4 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.3548 (avg: 0.3646) | Acc: 0.9453 (avg: 0.9470) | LR: 1.85e-03
  Batch  100/750 ( 13.3%) | Loss: 0.3066 (avg: 0.3675) | Acc: 0.9844 (avg: 0.9466) | LR: 1.91e-03
  Batch  150/750 ( 20.0%) | Loss: 0.4681 (avg: 0.3711) | Acc: 0.9141 (avg: 0.9451) | LR: 1.97e-03
  Batch  200/750 ( 26.7%) | Loss: 0.4070 (avg: 0.3763) | Acc: 0.9141 (avg: 0.9425) | LR: 2.02e-03
  Batch  250/750 ( 33.3%) | Loss: 0.4619 (avg: 0.3791) | Acc: 0.9062 (avg: 0.9413) | LR: 2.08e-03
  Batch  300/750 ( 40.0%) | Loss: 0.3371 (avg: 0.3820) | Acc: 0.9453 (avg: 0.9399) | LR: 2.14e-03
  Batch  350/750 ( 46.7%) | Loss: 0.3641 (avg: 0.3852) | Acc: 0.9453 (avg: 0.9387) | LR: 2.19e-03
  Batch  400/750 ( 53.3%) | Loss: 0.4215 (avg: 0.3878) | Acc: 0.9531 (avg: 0.9374) | LR: 2.25e-03
  Batch  450/750 ( 60.0%) | Loss: 0.4227 (avg: 0.3911) | Acc: 0.9297 (avg: 0.9361) | LR: 2.30e-03
  Batch  500/750 ( 66.7%) | Loss: 0.3852 (avg: 0.3941) | Acc: 0.9297 (avg: 0.9345) | LR: 2.36e-03
  Batch  550/750 ( 73.3%) | Loss: 0.3995 (avg: 0.3956) | Acc: 0.9297 (avg: 0.9335) | LR: 2.41e-03
  Batch  600/750 ( 80.0%) | Loss: 0.4025 (avg: 0.3994) | Acc: 0.9375 (avg: 0.9320) | LR: 2.47e-03
  Batch  650/750 ( 86.7%) | Loss: 0.4479 (avg: 0.4016) | Acc: 0.9062 (avg: 0.9309) | LR: 2.52e-03
  Batch  700/750 ( 93.3%) | Loss: 0.4227 (avg: 0.4037) | Acc: 0.9375 (avg: 0.9301) | LR: 2.57e-03
  Batch  750/750 (100.0%) | Loss: 0.3920 (avg: 0.4061) | Acc: 0.9297 (avg: 0.9291) | LR: 2.62e-03

Train Summary:
  Final Loss: 0.4061 | Final Acc: 0.9291
  Final LR: 2.62e-03
  Loss std: 0.0498 | Acc std: 0.0250

Val Epoch 4 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5165 | Final Acc: 0.8777
  Loss std: 0.0606 | Acc std: 0.0278
Epoch 04/20 | train 0.4061/0.9291 | val 0.5165/0.8777

Train Epoch 5 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.2948 (avg: 0.3436) | Acc: 0.9766 (avg: 0.9578) | LR: 2.67e-03
  Batch  100/750 ( 13.3%) | Loss: 0.3810 (avg: 0.3487) | Acc: 0.9453 (avg: 0.9559) | LR: 2.72e-03
  Batch  150/750 ( 20.0%) | Loss: 0.3583 (avg: 0.3500) | Acc: 0.9375 (avg: 0.9565) | LR: 2.76e-03
  Batch  200/750 ( 26.7%) | Loss: 0.3232 (avg: 0.3505) | Acc: 0.9844 (avg: 0.9558) | LR: 2.81e-03
  Batch  250/750 ( 33.3%) | Loss: 0.4992 (avg: 0.3539) | Acc: 0.8984 (avg: 0.9541) | LR: 2.86e-03
  Batch  300/750 ( 40.0%) | Loss: 0.4002 (avg: 0.3613) | Acc: 0.9297 (avg: 0.9503) | LR: 2.90e-03
  Batch  350/750 ( 46.7%) | Loss: 0.3147 (avg: 0.3655) | Acc: 0.9688 (avg: 0.9486) | LR: 2.94e-03
  Batch  400/750 ( 53.3%) | Loss: 0.3687 (avg: 0.3675) | Acc: 0.9453 (avg: 0.9478) | LR: 2.98e-03
  Batch  450/750 ( 60.0%) | Loss: 0.3179 (avg: 0.3695) | Acc: 0.9688 (avg: 0.9473) | LR: 3.02e-03
  Batch  500/750 ( 66.7%) | Loss: 0.3986 (avg: 0.3720) | Acc: 0.9375 (avg: 0.9463) | LR: 3.06e-03
  Batch  550/750 ( 73.3%) | Loss: 0.4760 (avg: 0.3745) | Acc: 0.8984 (avg: 0.9449) | LR: 3.10e-03
  Batch  600/750 ( 80.0%) | Loss: 0.3649 (avg: 0.3775) | Acc: 0.9688 (avg: 0.9438) | LR: 3.13e-03
  Batch  650/750 ( 86.7%) | Loss: 0.3767 (avg: 0.3806) | Acc: 0.9531 (avg: 0.9423) | LR: 3.16e-03
  Batch  700/750 ( 93.3%) | Loss: 0.3672 (avg: 0.3816) | Acc: 0.9297 (avg: 0.9418) | LR: 3.20e-03
  Batch  750/750 (100.0%) | Loss: 0.3957 (avg: 0.3842) | Acc: 0.9453 (avg: 0.9407) | LR: 3.23e-03

Train Summary:
  Final Loss: 0.3842 | Final Acc: 0.9407
  Final LR: 3.22e-03
  Loss std: 0.0560 | Acc std: 0.0279

Val Epoch 5 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4898 | Final Acc: 0.8955
  Loss std: 0.0636 | Acc std: 0.0276
Epoch 05/20 | train 0.3842/0.9407 | val 0.4898/0.8955

Train Epoch 6 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.2929 (avg: 0.3431) | Acc: 0.9844 (avg: 0.9591) | LR: 3.25e-03
  Batch  100/750 ( 13.3%) | Loss: 0.3773 (avg: 0.3442) | Acc: 0.9453 (avg: 0.9615) | LR: 3.28e-03
  Batch  150/750 ( 20.0%) | Loss: 0.3456 (avg: 0.3387) | Acc: 0.9297 (avg: 0.9630) | LR: 3.30e-03
  Batch  200/750 ( 26.7%) | Loss: 0.3624 (avg: 0.3420) | Acc: 0.9453 (avg: 0.9617) | LR: 3.33e-03
  Batch  250/750 ( 33.3%) | Loss: 0.3886 (avg: 0.3428) | Acc: 0.9375 (avg: 0.9607) | LR: 3.35e-03
  Batch  300/750 ( 40.0%) | Loss: 0.3762 (avg: 0.3493) | Acc: 0.9531 (avg: 0.9578) | LR: 3.37e-03
  Batch  350/750 ( 46.7%) | Loss: 0.3496 (avg: 0.3535) | Acc: 0.9531 (avg: 0.9560) | LR: 3.38e-03
  Batch  400/750 ( 53.3%) | Loss: 0.3719 (avg: 0.3560) | Acc: 0.9453 (avg: 0.9549) | LR: 3.40e-03
  Batch  450/750 ( 60.0%) | Loss: 0.3634 (avg: 0.3591) | Acc: 0.9609 (avg: 0.9537) | LR: 3.41e-03
  Batch  500/750 ( 66.7%) | Loss: 0.3674 (avg: 0.3637) | Acc: 0.9609 (avg: 0.9516) | LR: 3.42e-03
  Batch  550/750 ( 73.3%) | Loss: 0.4297 (avg: 0.3649) | Acc: 0.9062 (avg: 0.9508) | LR: 3.43e-03
  Batch  600/750 ( 80.0%) | Loss: 0.4288 (avg: 0.3673) | Acc: 0.9219 (avg: 0.9500) | LR: 3.44e-03
  Batch  650/750 ( 86.7%) | Loss: 0.4148 (avg: 0.3689) | Acc: 0.9297 (avg: 0.9495) | LR: 3.44e-03
  Batch  700/750 ( 93.3%) | Loss: 0.3067 (avg: 0.3702) | Acc: 0.9766 (avg: 0.9488) | LR: 3.45e-03
  Batch  750/750 (100.0%) | Loss: 0.4801 (avg: 0.3721) | Acc: 0.9141 (avg: 0.9476) | LR: 3.45e-03

Train Summary:
  Final Loss: 0.3721 | Final Acc: 0.9476
  Final LR: 3.45e-03
  Loss std: 0.0581 | Acc std: 0.0274

Val Epoch 6 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5196 | Final Acc: 0.8955
  Loss std: 0.0707 | Acc std: 0.0272
Epoch 06/20 | train 0.3721/0.9476 | val 0.5196/0.8955
Early stopping at epoch 6
wandb:
wandb: Run history:
wandb:               batch ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñá‚ñá‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñÅ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñà
wandb:          best_epoch ‚ñÅ‚ñÖ‚ñà
wandb:        best_val_acc ‚ñÅ‚ñÖ‚ñà
wandb:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:    final_best_epoch ‚ñÅ
wandb:  final_best_val_acc ‚ñÅ
wandb: final_learning_rate ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñà
wandb:      final_val_loss ‚ñÅ
wandb:       learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:           train_acc ‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà
wandb:                 +15 ...
wandb:
wandb: Run summary:
wandb:               batch 750
wandb:          best_epoch 3
wandb:        best_val_acc 0.90458
wandb:               epoch 6
wandb:    final_best_epoch 3
wandb:  final_best_val_acc 0.90458
wandb: final_learning_rate 0.00345
wandb:      final_val_loss 0.51963
wandb:       learning_rate 0.00345
wandb:              status completed
wandb:                 +16 ...
wandb:
wandb: üöÄ View run trial_006 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/g2u1e6v3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_065827-g2u1e6v3/logs
[I 2025-10-06 07:06:11,305] Trial 6 finished with value: 0.9045833333333333 and parameters: {'emb_dim': 300, 'channels': 128, 'kernel_sizes': (3, 4, 5, 6), 'dropout': 0.2, 'lr': 0.003446174676174919, 'weight_decay': 5.730901320938438e-05, 'batch_size': 128, 'max_len': 160, 'grad_clip': 1.0, 'optimizer': 'adamw'}. Best is trial 3 with value: 0.923125.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_070611-fygt8t6l
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-sponge-16
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/fygt8t6l
wandb: Finishing previous runs because reinit is set to True.
wandb:
wandb: Run history:
wandb:   batch_size ‚ñÅ
wandb:     channels ‚ñÅ
wandb:      dropout ‚ñÅ
wandb:      emb_dim ‚ñÅ
wandb:    grad_clip ‚ñÅ
wandb:           lr ‚ñÅ
wandb:      max_len ‚ñÅ
wandb:      val_acc ‚ñÅ
wandb: weight_decay ‚ñÅ
wandb:
wandb: Run summary:
wandb:   batch_size 128
wandb:     channels 128
wandb:      dropout 0.2
wandb:      emb_dim 300
wandb:    grad_clip 1
wandb:           lr 0.00345
wandb:      max_len 160
wandb:    optimizer adamw
wandb:      val_acc 0.90458
wandb: weight_decay 6e-05
wandb:
wandb: üöÄ View run trial/6/serene-sponge-16 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/fygt8t6l
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_070611-fygt8t6l/logs
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_070611-iuznir0o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_007
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/iuznir0o

Train Epoch 1 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 1.5961 (avg: 1.5708) | Acc: 0.1875 (avg: 0.2437) | LR: 1.20e-05
  Batch  100/1500 (  6.7%) | Loss: 1.4740 (avg: 1.5251) | Acc: 0.2656 (avg: 0.2550) | LR: 1.20e-05
  Batch  150/1500 ( 10.0%) | Loss: 1.5189 (avg: 1.5089) | Acc: 0.1875 (avg: 0.2520) | LR: 1.21e-05
  Batch  200/1500 ( 13.3%) | Loss: 1.5204 (avg: 1.4959) | Acc: 0.1875 (avg: 0.2536) | LR: 1.23e-05
  Batch  250/1500 ( 16.7%) | Loss: 1.4196 (avg: 1.4856) | Acc: 0.2500 (avg: 0.2585) | LR: 1.25e-05
  Batch  300/1500 ( 20.0%) | Loss: 1.5033 (avg: 1.4783) | Acc: 0.2031 (avg: 0.2619) | LR: 1.27e-05
  Batch  350/1500 ( 23.3%) | Loss: 1.4591 (avg: 1.4728) | Acc: 0.3125 (avg: 0.2636) | LR: 1.30e-05
  Batch  400/1500 ( 26.7%) | Loss: 1.4514 (avg: 1.4686) | Acc: 0.2656 (avg: 0.2647) | LR: 1.33e-05
  Batch  450/1500 ( 30.0%) | Loss: 1.4875 (avg: 1.4641) | Acc: 0.2188 (avg: 0.2669) | LR: 1.37e-05
  Batch  500/1500 ( 33.3%) | Loss: 1.5167 (avg: 1.4610) | Acc: 0.2344 (avg: 0.2674) | LR: 1.41e-05
  Batch  550/1500 ( 36.7%) | Loss: 1.4650 (avg: 1.4573) | Acc: 0.2344 (avg: 0.2699) | LR: 1.46e-05
  Batch  600/1500 ( 40.0%) | Loss: 1.3718 (avg: 1.4543) | Acc: 0.2812 (avg: 0.2712) | LR: 1.51e-05
  Batch  650/1500 ( 43.3%) | Loss: 1.4252 (avg: 1.4510) | Acc: 0.2812 (avg: 0.2730) | LR: 1.56e-05
  Batch  700/1500 ( 46.7%) | Loss: 1.3691 (avg: 1.4479) | Acc: 0.3594 (avg: 0.2746) | LR: 1.62e-05
  Batch  750/1500 ( 50.0%) | Loss: 1.3037 (avg: 1.4447) | Acc: 0.3438 (avg: 0.2764) | LR: 1.68e-05
  Batch  800/1500 ( 53.3%) | Loss: 1.3592 (avg: 1.4416) | Acc: 0.3438 (avg: 0.2781) | LR: 1.75e-05
  Batch  850/1500 ( 56.7%) | Loss: 1.3925 (avg: 1.4390) | Acc: 0.2969 (avg: 0.2794) | LR: 1.82e-05
  Batch  900/1500 ( 60.0%) | Loss: 1.3506 (avg: 1.4360) | Acc: 0.3281 (avg: 0.2815) | LR: 1.89e-05
  Batch  950/1500 ( 63.3%) | Loss: 1.3432 (avg: 1.4333) | Acc: 0.3750 (avg: 0.2839) | LR: 1.97e-05
  Batch 1000/1500 ( 66.7%) | Loss: 1.4277 (avg: 1.4307) | Acc: 0.2969 (avg: 0.2857) | LR: 2.06e-05
  Batch 1050/1500 ( 70.0%) | Loss: 1.3210 (avg: 1.4278) | Acc: 0.3594 (avg: 0.2879) | LR: 2.14e-05
  Batch 1100/1500 ( 73.3%) | Loss: 1.3595 (avg: 1.4253) | Acc: 0.3438 (avg: 0.2901) | LR: 2.24e-05
  Batch 1150/1500 ( 76.7%) | Loss: 1.3542 (avg: 1.4219) | Acc: 0.3906 (avg: 0.2927) | LR: 2.33e-05
  Batch 1200/1500 ( 80.0%) | Loss: 1.4187 (avg: 1.4193) | Acc: 0.2500 (avg: 0.2947) | LR: 2.43e-05
  Batch 1250/1500 ( 83.3%) | Loss: 1.3743 (avg: 1.4162) | Acc: 0.3750 (avg: 0.2973) | LR: 2.53e-05
  Batch 1300/1500 ( 86.7%) | Loss: 1.2676 (avg: 1.4131) | Acc: 0.4531 (avg: 0.3001) | LR: 2.64e-05
  Batch 1350/1500 ( 90.0%) | Loss: 1.3839 (avg: 1.4104) | Acc: 0.3750 (avg: 0.3026) | LR: 2.75e-05
  Batch 1400/1500 ( 93.3%) | Loss: 1.3542 (avg: 1.4073) | Acc: 0.3281 (avg: 0.3055) | LR: 2.87e-05
  Batch 1450/1500 ( 96.7%) | Loss: 1.3673 (avg: 1.4038) | Acc: 0.3125 (avg: 0.3087) | LR: 2.99e-05
  Batch 1500/1500 (100.0%) | Loss: 1.3483 (avg: 1.4007) | Acc: 0.3281 (avg: 0.3114) | LR: 3.11e-05

Train Summary:
  Final Loss: 1.4007 | Final Acc: 0.3114
  Final LR: 3.11e-05
  Loss std: 0.0752 | Acc std: 0.0712

Val Epoch 1 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 1.2440 | Final Acc: 0.5647
  Loss std: 0.0246 | Acc std: 0.0653
Epoch 01/20 | train 1.4007/0.3114 | val 1.2440/0.5647

Train Epoch 2 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 1.2533 (avg: 1.2822) | Acc: 0.4219 (avg: 0.4194) | LR: 3.24e-05
  Batch  100/1500 (  6.7%) | Loss: 1.1698 (avg: 1.2845) | Acc: 0.4844 (avg: 0.4173) | LR: 3.37e-05
  Batch  150/1500 ( 10.0%) | Loss: 1.3240 (avg: 1.2806) | Acc: 0.2969 (avg: 0.4213) | LR: 3.50e-05
  Batch  200/1500 ( 13.3%) | Loss: 1.2362 (avg: 1.2795) | Acc: 0.4531 (avg: 0.4212) | LR: 3.64e-05
  Batch  250/1500 ( 16.7%) | Loss: 1.2083 (avg: 1.2752) | Acc: 0.4531 (avg: 0.4264) | LR: 3.78e-05
  Batch  300/1500 ( 20.0%) | Loss: 1.3285 (avg: 1.2707) | Acc: 0.2969 (avg: 0.4296) | LR: 3.93e-05
  Batch  350/1500 ( 23.3%) | Loss: 1.2511 (avg: 1.2664) | Acc: 0.4531 (avg: 0.4336) | LR: 4.08e-05
  Batch  400/1500 ( 26.7%) | Loss: 1.2026 (avg: 1.2633) | Acc: 0.5156 (avg: 0.4350) | LR: 4.23e-05
  Batch  450/1500 ( 30.0%) | Loss: 1.1974 (avg: 1.2585) | Acc: 0.5000 (avg: 0.4401) | LR: 4.38e-05
  Batch  500/1500 ( 33.3%) | Loss: 1.2388 (avg: 1.2556) | Acc: 0.4688 (avg: 0.4436) | LR: 4.54e-05
  Batch  550/1500 ( 36.7%) | Loss: 1.1826 (avg: 1.2515) | Acc: 0.5625 (avg: 0.4472) | LR: 4.71e-05
  Batch  600/1500 ( 40.0%) | Loss: 1.2637 (avg: 1.2467) | Acc: 0.5000 (avg: 0.4512) | LR: 4.87e-05
  Batch  650/1500 ( 43.3%) | Loss: 1.1671 (avg: 1.2409) | Acc: 0.5781 (avg: 0.4570) | LR: 5.04e-05
  Batch  700/1500 ( 46.7%) | Loss: 1.2149 (avg: 1.2368) | Acc: 0.4531 (avg: 0.4599) | LR: 5.21e-05
  Batch  750/1500 ( 50.0%) | Loss: 1.1557 (avg: 1.2320) | Acc: 0.6406 (avg: 0.4649) | LR: 5.39e-05
  Batch  800/1500 ( 53.3%) | Loss: 1.1680 (avg: 1.2277) | Acc: 0.4844 (avg: 0.4682) | LR: 5.57e-05
  Batch  850/1500 ( 56.7%) | Loss: 1.1325 (avg: 1.2227) | Acc: 0.6094 (avg: 0.4722) | LR: 5.75e-05
  Batch  900/1500 ( 60.0%) | Loss: 1.1027 (avg: 1.2176) | Acc: 0.5781 (avg: 0.4767) | LR: 5.93e-05
  Batch  950/1500 ( 63.3%) | Loss: 1.0844 (avg: 1.2121) | Acc: 0.5781 (avg: 0.4816) | LR: 6.12e-05
  Batch 1000/1500 ( 66.7%) | Loss: 1.1833 (avg: 1.2071) | Acc: 0.3906 (avg: 0.4858) | LR: 6.31e-05
  Batch 1050/1500 ( 70.0%) | Loss: 0.9956 (avg: 1.2008) | Acc: 0.7188 (avg: 0.4906) | LR: 6.50e-05
  Batch 1100/1500 ( 73.3%) | Loss: 1.0576 (avg: 1.1961) | Acc: 0.6094 (avg: 0.4943) | LR: 6.70e-05
  Batch 1150/1500 ( 76.7%) | Loss: 1.0952 (avg: 1.1914) | Acc: 0.6094 (avg: 0.4977) | LR: 6.90e-05
  Batch 1200/1500 ( 80.0%) | Loss: 1.1136 (avg: 1.1863) | Acc: 0.5625 (avg: 0.5019) | LR: 7.10e-05
  Batch 1250/1500 ( 83.3%) | Loss: 0.9886 (avg: 1.1805) | Acc: 0.6719 (avg: 0.5063) | LR: 7.30e-05
  Batch 1300/1500 ( 86.7%) | Loss: 1.1298 (avg: 1.1749) | Acc: 0.5781 (avg: 0.5108) | LR: 7.51e-05
  Batch 1350/1500 ( 90.0%) | Loss: 1.0074 (avg: 1.1693) | Acc: 0.5938 (avg: 0.5149) | LR: 7.71e-05
  Batch 1400/1500 ( 93.3%) | Loss: 1.1433 (avg: 1.1634) | Acc: 0.5000 (avg: 0.5192) | LR: 7.92e-05
  Batch 1450/1500 ( 96.7%) | Loss: 0.8898 (avg: 1.1582) | Acc: 0.7031 (avg: 0.5228) | LR: 8.14e-05
  Batch 1500/1500 (100.0%) | Loss: 0.9555 (avg: 1.1522) | Acc: 0.6406 (avg: 0.5267) | LR: 8.35e-05

Train Summary:
  Final Loss: 1.1522 | Final Acc: 0.5267
  Final LR: 8.35e-05
  Loss std: 0.1078 | Acc std: 0.0950

Val Epoch 2 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.9293 | Final Acc: 0.7205
  Loss std: 0.0554 | Acc std: 0.0565
Epoch 02/20 | train 1.1522/0.5267 | val 0.9293/0.7205

Train Epoch 3 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.9893 (avg: 0.9692) | Acc: 0.6562 (avg: 0.6534) | LR: 8.57e-05
  Batch  100/1500 (  6.7%) | Loss: 0.9216 (avg: 0.9597) | Acc: 0.7031 (avg: 0.6567) | LR: 8.79e-05
  Batch  150/1500 ( 10.0%) | Loss: 0.9221 (avg: 0.9587) | Acc: 0.6719 (avg: 0.6561) | LR: 9.01e-05
  Batch  200/1500 ( 13.3%) | Loss: 0.9145 (avg: 0.9540) | Acc: 0.6875 (avg: 0.6609) | LR: 9.24e-05
  Batch  250/1500 ( 16.7%) | Loss: 0.8631 (avg: 0.9504) | Acc: 0.7500 (avg: 0.6603) | LR: 9.46e-05
  Batch  300/1500 ( 20.0%) | Loss: 0.9976 (avg: 0.9497) | Acc: 0.6250 (avg: 0.6585) | LR: 9.69e-05
  Batch  350/1500 ( 23.3%) | Loss: 0.8807 (avg: 0.9444) | Acc: 0.7344 (avg: 0.6627) | LR: 9.92e-05
  Batch  400/1500 ( 26.7%) | Loss: 0.7998 (avg: 0.9388) | Acc: 0.7500 (avg: 0.6663) | LR: 1.01e-04
  Batch  450/1500 ( 30.0%) | Loss: 0.8740 (avg: 0.9350) | Acc: 0.7500 (avg: 0.6677) | LR: 1.04e-04
  Batch  500/1500 ( 33.3%) | Loss: 0.8432 (avg: 0.9309) | Acc: 0.7344 (avg: 0.6693) | LR: 1.06e-04
  Batch  550/1500 ( 36.7%) | Loss: 0.8347 (avg: 0.9270) | Acc: 0.7188 (avg: 0.6719) | LR: 1.09e-04
  Batch  600/1500 ( 40.0%) | Loss: 0.7264 (avg: 0.9223) | Acc: 0.8125 (avg: 0.6743) | LR: 1.11e-04
  Batch  650/1500 ( 43.3%) | Loss: 0.7713 (avg: 0.9170) | Acc: 0.7812 (avg: 0.6774) | LR: 1.13e-04
  Batch  700/1500 ( 46.7%) | Loss: 0.8776 (avg: 0.9129) | Acc: 0.6719 (avg: 0.6794) | LR: 1.16e-04
  Batch  750/1500 ( 50.0%) | Loss: 0.9182 (avg: 0.9079) | Acc: 0.7031 (avg: 0.6825) | LR: 1.18e-04
  Batch  800/1500 ( 53.3%) | Loss: 0.7808 (avg: 0.9038) | Acc: 0.7188 (avg: 0.6847) | LR: 1.20e-04
  Batch  850/1500 ( 56.7%) | Loss: 0.8662 (avg: 0.9002) | Acc: 0.7344 (avg: 0.6865) | LR: 1.23e-04
  Batch  900/1500 ( 60.0%) | Loss: 0.7790 (avg: 0.8961) | Acc: 0.7344 (avg: 0.6884) | LR: 1.25e-04
  Batch  950/1500 ( 63.3%) | Loss: 0.8394 (avg: 0.8918) | Acc: 0.7188 (avg: 0.6907) | LR: 1.28e-04
  Batch 1000/1500 ( 66.7%) | Loss: 0.8338 (avg: 0.8882) | Acc: 0.6719 (avg: 0.6927) | LR: 1.30e-04
  Batch 1050/1500 ( 70.0%) | Loss: 0.8765 (avg: 0.8843) | Acc: 0.6719 (avg: 0.6942) | LR: 1.33e-04
  Batch 1100/1500 ( 73.3%) | Loss: 0.9035 (avg: 0.8807) | Acc: 0.7188 (avg: 0.6959) | LR: 1.35e-04
  Batch 1150/1500 ( 76.7%) | Loss: 0.8245 (avg: 0.8779) | Acc: 0.6875 (avg: 0.6971) | LR: 1.38e-04
  Batch 1200/1500 ( 80.0%) | Loss: 0.7824 (avg: 0.8744) | Acc: 0.7969 (avg: 0.6988) | LR: 1.40e-04
  Batch 1250/1500 ( 83.3%) | Loss: 0.7771 (avg: 0.8720) | Acc: 0.7031 (avg: 0.6999) | LR: 1.43e-04
  Batch 1300/1500 ( 86.7%) | Loss: 0.7908 (avg: 0.8690) | Acc: 0.7344 (avg: 0.7014) | LR: 1.45e-04
  Batch 1350/1500 ( 90.0%) | Loss: 0.9180 (avg: 0.8651) | Acc: 0.6562 (avg: 0.7035) | LR: 1.48e-04
  Batch 1400/1500 ( 93.3%) | Loss: 0.6873 (avg: 0.8616) | Acc: 0.7500 (avg: 0.7049) | LR: 1.50e-04
  Batch 1450/1500 ( 96.7%) | Loss: 0.7513 (avg: 0.8595) | Acc: 0.8125 (avg: 0.7058) | LR: 1.53e-04
  Batch 1500/1500 (100.0%) | Loss: 0.7133 (avg: 0.8560) | Acc: 0.7656 (avg: 0.7075) | LR: 1.55e-04

Train Summary:
  Final Loss: 0.8560 | Final Acc: 0.7075
  Final LR: 1.55e-04
  Loss std: 0.0948 | Acc std: 0.0640

Val Epoch 3 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.7077 | Final Acc: 0.8012
  Loss std: 0.0696 | Acc std: 0.0511
Epoch 03/20 | train 0.8560/0.7075 | val 0.7077/0.8012
Trial pruned at epoch 3
Trial 7 failed:
wandb:
wandb: Run history:
wandb:               batch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñà‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb:          best_epoch ‚ñÅ‚ñà
wandb:        best_val_acc ‚ñÅ‚ñà
wandb:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: final_learning_rate ‚ñÅ‚ñÑ‚ñà
wandb:       learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:           train_acc ‚ñÅ‚ñÖ‚ñà
wandb:       train_acc_std ‚ñÉ‚ñà‚ñÅ
wandb:       train_avg_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:      train_avg_loss ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                 +12 ...
wandb:
wandb: Run summary:
wandb:               batch 1500
wandb:          best_epoch 2
wandb:        best_val_acc 0.7205
wandb:               epoch 3
wandb:               error
wandb: final_learning_rate 0.00016
wandb:       learning_rate 0.00016
wandb:              status failed
wandb:           train_acc 0.70747
wandb:       train_acc_std 0.06398
wandb:                 +14 ...
wandb:
wandb: üöÄ View run trial_007 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/iuznir0o
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_070611-iuznir0o/logs
[I 2025-10-06 07:09:42,109] Trial 7 pruned.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_070942-kv6i5psk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run resilient-darkness-18
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/kv6i5psk
wandb: Finishing previous runs because reinit is set to True.
wandb:
wandb: Run history:
wandb:   batch_size ‚ñÅ
wandb:     channels ‚ñÅ
wandb:      dropout ‚ñÅ
wandb:      emb_dim ‚ñÅ
wandb:    grad_clip ‚ñÅ
wandb:           lr ‚ñÅ
wandb:      max_len ‚ñÅ
wandb:      val_acc ‚ñÅ
wandb: weight_decay ‚ñÅ
wandb:
wandb: Run summary:
wandb:   batch_size 64
wandb:     channels 128
wandb:      dropout 0.3
wandb:      emb_dim 100
wandb:    grad_clip 0.5
wandb:           lr 0.0003
wandb:      max_len 256
wandb:    optimizer adam
wandb:      val_acc 0.80121
wandb: weight_decay 0.00022
wandb:
wandb: üöÄ View run trial/7/resilient-darkness-18 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/kv6i5psk
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_070942-kv6i5psk/logs
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_070942-4ejt1iyv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_008
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/4ejt1iyv

Train Epoch 1 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 1.4304 (avg: 1.4402) | Acc: 0.2500 (avg: 0.2605) | LR: 1.07e-05
  Batch  100/375 ( 26.7%) | Loss: 1.3686 (avg: 1.4215) | Acc: 0.3125 (avg: 0.2749) | LR: 1.16e-05
  Batch  150/375 ( 40.0%) | Loss: 1.2979 (avg: 1.4011) | Acc: 0.4219 (avg: 0.2941) | LR: 1.31e-05
  Batch  200/375 ( 53.3%) | Loss: 1.2801 (avg: 1.3818) | Acc: 0.4180 (avg: 0.3146) | LR: 1.52e-05
  Batch  250/375 ( 66.7%) | Loss: 1.2707 (avg: 1.3619) | Acc: 0.4375 (avg: 0.3380) | LR: 1.79e-05
  Batch  300/375 ( 80.0%) | Loss: 1.1890 (avg: 1.3411) | Acc: 0.5273 (avg: 0.3612) | LR: 2.11e-05
  Batch  350/375 ( 93.3%) | Loss: 1.1840 (avg: 1.3199) | Acc: 0.5312 (avg: 0.3838) | LR: 2.50e-05

Train Summary:
  Final Loss: 1.3087 | Final Acc: 0.3950
  Final LR: 2.70e-05
  Loss std: 0.0936 | Acc std: 0.1017

Val Epoch 1 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 1.1033 | Final Acc: 0.6993
  Loss std: 0.0167 | Acc std: 0.0289
Epoch 01/20 | train 1.3087/0.3950 | val 1.1033/0.6993

Train Epoch 2 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 1.0880 (avg: 1.1045) | Acc: 0.5781 (avg: 0.6009) | LR: 3.17e-05
  Batch  100/375 ( 26.7%) | Loss: 1.0602 (avg: 1.0783) | Acc: 0.6367 (avg: 0.6179) | LR: 3.68e-05
  Batch  150/375 ( 40.0%) | Loss: 0.9711 (avg: 1.0485) | Acc: 0.6992 (avg: 0.6365) | LR: 4.24e-05
  Batch  200/375 ( 53.3%) | Loss: 0.9505 (avg: 1.0207) | Acc: 0.6992 (avg: 0.6519) | LR: 4.84e-05
  Batch  250/375 ( 66.7%) | Loss: 0.8628 (avg: 0.9923) | Acc: 0.7344 (avg: 0.6669) | LR: 5.49e-05
  Batch  300/375 ( 80.0%) | Loss: 0.7849 (avg: 0.9640) | Acc: 0.7695 (avg: 0.6816) | LR: 6.17e-05
  Batch  350/375 ( 93.3%) | Loss: 0.7352 (avg: 0.9383) | Acc: 0.7891 (avg: 0.6935) | LR: 6.89e-05

Train Summary:
  Final Loss: 0.9265 | Final Acc: 0.6985
  Final LR: 7.25e-05
  Loss std: 0.1192 | Acc std: 0.0651

Val Epoch 2 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.7141 | Final Acc: 0.8118
  Loss std: 0.0326 | Acc std: 0.0224
Epoch 02/20 | train 0.9265/0.6985 | val 0.7141/0.8118

Train Epoch 3 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.6378 (avg: 0.7094) | Acc: 0.8555 (avg: 0.7958) | LR: 8.04e-05
  Batch  100/375 ( 26.7%) | Loss: 0.6750 (avg: 0.6898) | Acc: 0.7930 (avg: 0.8052) | LR: 8.83e-05
  Batch  150/375 ( 40.0%) | Loss: 0.5977 (avg: 0.6770) | Acc: 0.8359 (avg: 0.8093) | LR: 9.65e-05
  Batch  200/375 ( 53.3%) | Loss: 0.6252 (avg: 0.6657) | Acc: 0.7891 (avg: 0.8141) | LR: 1.05e-04
  Batch  250/375 ( 66.7%) | Loss: 0.6056 (avg: 0.6550) | Acc: 0.8477 (avg: 0.8182) | LR: 1.13e-04
  Batch  300/375 ( 80.0%) | Loss: 0.5468 (avg: 0.6455) | Acc: 0.8672 (avg: 0.8223) | LR: 1.22e-04
  Batch  350/375 ( 93.3%) | Loss: 0.6254 (avg: 0.6369) | Acc: 0.8555 (avg: 0.8260) | LR: 1.31e-04

Train Summary:
  Final Loss: 0.6331 | Final Acc: 0.8275
  Final LR: 1.35e-04
  Loss std: 0.0552 | Acc std: 0.0295

Val Epoch 3 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5601 | Final Acc: 0.8588
  Loss std: 0.0353 | Acc std: 0.0209
Epoch 03/20 | train 0.6331/0.8275 | val 0.5601/0.8588
Trial pruned at epoch 3
Trial 8 failed:
wandb:
wandb: Run history:
wandb:               batch ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà
wandb:          best_epoch ‚ñÅ‚ñà
wandb:        best_val_acc ‚ñÅ‚ñà
wandb:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: final_learning_rate ‚ñÅ‚ñÑ‚ñà
wandb:       learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà
wandb:           train_acc ‚ñÅ‚ñÜ‚ñà
wandb:       train_acc_std ‚ñà‚ñÑ‚ñÅ
wandb:       train_avg_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:      train_avg_loss ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                 +12 ...
wandb:
wandb: Run summary:
wandb:               batch 350
wandb:          best_epoch 2
wandb:        best_val_acc 0.81175
wandb:               epoch 3
wandb:               error
wandb: final_learning_rate 0.00013
wandb:       learning_rate 0.00013
wandb:              status failed
wandb:           train_acc 0.82749
wandb:       train_acc_std 0.02952
wandb:                 +14 ...
wandb:
wandb: üöÄ View run trial_008 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/4ejt1iyv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_070942-4ejt1iyv/logs
[I 2025-10-06 07:13:34,990] Trial 8 pruned.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_071334-f9pzcm4g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run crimson-sunset-20
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/f9pzcm4g
wandb: Finishing previous runs because reinit is set to True.
wandb:
wandb: Run history:
wandb:   batch_size ‚ñÅ
wandb:     channels ‚ñÅ
wandb:      dropout ‚ñÅ
wandb:      emb_dim ‚ñÅ
wandb:    grad_clip ‚ñÅ
wandb:           lr ‚ñÅ
wandb:      max_len ‚ñÅ
wandb:      val_acc ‚ñÅ
wandb: weight_decay ‚ñÅ
wandb:
wandb: Run summary:
wandb:   batch_size 256
wandb:     channels 256
wandb:      dropout 0.2
wandb:      emb_dim 300
wandb:    grad_clip 1.5
wandb:           lr 0.00026
wandb:      max_len 160
wandb:    optimizer adamw
wandb:      val_acc 0.85883
wandb: weight_decay 1e-05
wandb:
wandb: üöÄ View run trial/8/crimson-sunset-20 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/f9pzcm4g
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_071334-f9pzcm4g/logs
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_071335-5n6itwuv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_009
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/5n6itwuv

Train Epoch 1 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 1.3703 (avg: 1.4097) | Acc: 0.3281 (avg: 0.3006) | LR: 1.66e-04
  Batch  100/375 ( 26.7%) | Loss: 1.2670 (avg: 1.3593) | Acc: 0.4219 (avg: 0.3461) | LR: 1.80e-04
  Batch  150/375 ( 40.0%) | Loss: 1.1593 (avg: 1.3119) | Acc: 0.5195 (avg: 0.3898) | LR: 2.03e-04
  Batch  200/375 ( 53.3%) | Loss: 1.0901 (avg: 1.2696) | Acc: 0.5352 (avg: 0.4258) | LR: 2.36e-04
  Batch  250/375 ( 66.7%) | Loss: 1.0578 (avg: 1.2276) | Acc: 0.5508 (avg: 0.4585) | LR: 2.77e-04
  Batch  300/375 ( 80.0%) | Loss: 0.9663 (avg: 1.1882) | Acc: 0.6406 (avg: 0.4875) | LR: 3.28e-04
  Batch  350/375 ( 93.3%) | Loss: 0.9744 (avg: 1.1514) | Acc: 0.6172 (avg: 0.5120) | LR: 3.87e-04

Train Summary:
  Final Loss: 1.1338 | Final Acc: 0.5234
  Final LR: 4.18e-04
  Loss std: 0.1724 | Acc std: 0.1299

Val Epoch 1 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.8033 | Final Acc: 0.7551
  Loss std: 0.0350 | Acc std: 0.0289
Epoch 01/20 | train 1.1338/0.5234 | val 0.8033/0.7551

Train Epoch 2 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.7832 (avg: 0.8299) | Acc: 0.7383 (avg: 0.7190) | LR: 4.91e-04
  Batch  100/375 ( 26.7%) | Loss: 0.8124 (avg: 0.8126) | Acc: 0.7109 (avg: 0.7257) | LR: 5.71e-04
  Batch  150/375 ( 40.0%) | Loss: 0.7612 (avg: 0.8001) | Acc: 0.7617 (avg: 0.7334) | LR: 6.57e-04
  Batch  200/375 ( 53.3%) | Loss: 0.7334 (avg: 0.7830) | Acc: 0.7539 (avg: 0.7413) | LR: 7.51e-04
  Batch  250/375 ( 66.7%) | Loss: 0.7739 (avg: 0.7699) | Acc: 0.7031 (avg: 0.7480) | LR: 8.51e-04
  Batch  300/375 ( 80.0%) | Loss: 0.7537 (avg: 0.7604) | Acc: 0.7422 (avg: 0.7529) | LR: 9.57e-04
  Batch  350/375 ( 93.3%) | Loss: 0.6606 (avg: 0.7508) | Acc: 0.8203 (avg: 0.7576) | LR: 1.07e-03

Train Summary:
  Final Loss: 0.7454 | Final Acc: 0.7604
  Final LR: 1.12e-03
  Loss std: 0.0619 | Acc std: 0.0357

Val Epoch 2 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.6113 | Final Acc: 0.8348
  Loss std: 0.0359 | Acc std: 0.0245
Epoch 02/20 | train 0.7454/0.7604 | val 0.6113/0.8348

Train Epoch 3 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.6202 (avg: 0.6295) | Acc: 0.8281 (avg: 0.8209) | LR: 1.25e-03
  Batch  100/375 ( 26.7%) | Loss: 0.7072 (avg: 0.6325) | Acc: 0.8086 (avg: 0.8196) | LR: 1.37e-03
  Batch  150/375 ( 40.0%) | Loss: 0.6603 (avg: 0.6319) | Acc: 0.8125 (avg: 0.8205) | LR: 1.50e-03
  Batch  200/375 ( 53.3%) | Loss: 0.5918 (avg: 0.6295) | Acc: 0.8125 (avg: 0.8210) | LR: 1.63e-03
  Batch  250/375 ( 66.7%) | Loss: 0.6582 (avg: 0.6306) | Acc: 0.8125 (avg: 0.8201) | LR: 1.76e-03
  Batch  300/375 ( 80.0%) | Loss: 0.6029 (avg: 0.6290) | Acc: 0.8359 (avg: 0.8206) | LR: 1.89e-03
  Batch  350/375 ( 93.3%) | Loss: 0.6480 (avg: 0.6264) | Acc: 0.8164 (avg: 0.8221) | LR: 2.03e-03

Train Summary:
  Final Loss: 0.6252 | Final Acc: 0.8227
  Final LR: 2.09e-03
  Loss std: 0.0418 | Acc std: 0.0242

Val Epoch 3 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5582 | Final Acc: 0.8573
  Loss std: 0.0361 | Acc std: 0.0215
Epoch 03/20 | train 0.6252/0.8227 | val 0.5582/0.8573
Trial pruned at epoch 3
Trial 9 failed:
wandb:
wandb: Run history:
wandb:               batch ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà
wandb:          best_epoch ‚ñÅ‚ñà
wandb:        best_val_acc ‚ñÅ‚ñà
wandb:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: final_learning_rate ‚ñÅ‚ñÑ‚ñà
wandb:       learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà
wandb:           train_acc ‚ñÅ‚ñá‚ñà
wandb:       train_acc_std ‚ñà‚ñÇ‚ñÅ
wandb:       train_avg_acc ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:      train_avg_loss ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                 +12 ...
wandb:
wandb: Run summary:
wandb:               batch 350
wandb:          best_epoch 2
wandb:        best_val_acc 0.83479
wandb:               epoch 3
wandb:               error
wandb: final_learning_rate 0.00209
wandb:       learning_rate 0.00203
wandb:              status failed
wandb:           train_acc 0.8227
wandb:       train_acc_std 0.02425
wandb:                 +14 ...
wandb:
wandb: üöÄ View run trial_009 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/5n6itwuv
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_071335-5n6itwuv/logs
[I 2025-10-06 07:16:52,514] Trial 9 pruned.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_071652-jxjc4b3u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run denim-vortex-22
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/jxjc4b3u
wandb: Finishing previous runs because reinit is set to True.
wandb:
wandb: Run history:
wandb:   batch_size ‚ñÅ
wandb:     channels ‚ñÅ
wandb:      dropout ‚ñÅ
wandb:      emb_dim ‚ñÅ
wandb:    grad_clip ‚ñÅ
wandb:           lr ‚ñÅ
wandb:      max_len ‚ñÅ
wandb:      val_acc ‚ñÅ
wandb: weight_decay ‚ñÅ
wandb:
wandb: Run summary:
wandb:   batch_size 256
wandb:     channels 128
wandb:      dropout 0.3
wandb:      emb_dim 100
wandb:    grad_clip 1.5
wandb:           lr 0.00402
wandb:      max_len 256
wandb:    optimizer adam
wandb:      val_acc 0.85733
wandb: weight_decay 0.00045
wandb:
wandb: üöÄ View run trial/9/denim-vortex-22 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/jxjc4b3u
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_071652-jxjc4b3u/logs
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_071653-4jw3gdgr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_010
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/4jw3gdgr

Train Epoch 1 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 1.5089 (avg: 1.5985) | Acc: 0.2773 (avg: 0.2623) | LR: 3.40e-05
  Batch  100/375 ( 26.7%) | Loss: 1.5405 (avg: 1.5685) | Acc: 0.2773 (avg: 0.2736) | LR: 3.69e-05
  Batch  150/375 ( 40.0%) | Loss: 1.4630 (avg: 1.5471) | Acc: 0.3984 (avg: 0.2853) | LR: 4.17e-05
  Batch  200/375 ( 53.3%) | Loss: 1.4004 (avg: 1.5206) | Acc: 0.3750 (avg: 0.2995) | LR: 4.84e-05
  Batch  250/375 ( 66.7%) | Loss: 1.3635 (avg: 1.4932) | Acc: 0.3828 (avg: 0.3135) | LR: 5.70e-05
  Batch  300/375 ( 80.0%) | Loss: 1.2741 (avg: 1.4644) | Acc: 0.4570 (avg: 0.3295) | LR: 6.73e-05
  Batch  350/375 ( 93.3%) | Loss: 1.1882 (avg: 1.4373) | Acc: 0.4961 (avg: 0.3450) | LR: 7.95e-05

Train Summary:
  Final Loss: 1.4228 | Final Acc: 0.3535
  Final LR: 8.59e-05
  Loss std: 0.1266 | Acc std: 0.0728

Val Epoch 1 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 1.0234 | Final Acc: 0.7375
  Loss std: 0.0199 | Acc std: 0.0253
Epoch 01/20 | train 1.4228/0.3535 | val 1.0234/0.7375

Train Epoch 2 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 1.1430 (avg: 1.1668) | Acc: 0.5195 (avg: 0.5078) | LR: 1.01e-04
  Batch  100/375 ( 26.7%) | Loss: 1.0257 (avg: 1.1242) | Acc: 0.5859 (avg: 0.5334) | LR: 1.17e-04
  Batch  150/375 ( 40.0%) | Loss: 0.9758 (avg: 1.0913) | Acc: 0.6250 (avg: 0.5546) | LR: 1.35e-04
  Batch  200/375 ( 53.3%) | Loss: 0.8976 (avg: 1.0594) | Acc: 0.6953 (avg: 0.5749) | LR: 1.54e-04
  Batch  250/375 ( 66.7%) | Loss: 0.8329 (avg: 1.0297) | Acc: 0.6953 (avg: 0.5921) | LR: 1.75e-04
  Batch  300/375 ( 80.0%) | Loss: 0.8753 (avg: 1.0006) | Acc: 0.6875 (avg: 0.6099) | LR: 1.97e-04
  Batch  350/375 ( 93.3%) | Loss: 0.8232 (avg: 0.9736) | Acc: 0.7305 (avg: 0.6259) | LR: 2.20e-04

Train Summary:
  Final Loss: 0.9617 | Final Acc: 0.6330
  Final LR: 2.31e-04
  Loss std: 0.1290 | Acc std: 0.0793

Val Epoch 2 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.6553 | Final Acc: 0.8220
  Loss std: 0.0357 | Acc std: 0.0229
Epoch 02/20 | train 0.9617/0.6330 | val 0.6553/0.8220

Train Epoch 3 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.8152 (avg: 0.7488) | Acc: 0.7422 (avg: 0.7555) | LR: 2.56e-04
  Batch  100/375 ( 26.7%) | Loss: 0.8067 (avg: 0.7392) | Acc: 0.7305 (avg: 0.7625) | LR: 2.81e-04
  Batch  150/375 ( 40.0%) | Loss: 0.7068 (avg: 0.7266) | Acc: 0.7852 (avg: 0.7686) | LR: 3.07e-04
  Batch  200/375 ( 53.3%) | Loss: 0.7175 (avg: 0.7190) | Acc: 0.7891 (avg: 0.7722) | LR: 3.34e-04
  Batch  250/375 ( 66.7%) | Loss: 0.6794 (avg: 0.7069) | Acc: 0.8008 (avg: 0.7779) | LR: 3.61e-04
  Batch  300/375 ( 80.0%) | Loss: 0.6081 (avg: 0.6992) | Acc: 0.8438 (avg: 0.7821) | LR: 3.88e-04
  Batch  350/375 ( 93.3%) | Loss: 0.6693 (avg: 0.6893) | Acc: 0.7891 (avg: 0.7881) | LR: 4.16e-04

Train Summary:
  Final Loss: 0.6854 | Final Acc: 0.7904
  Final LR: 4.29e-04
  Loss std: 0.0593 | Acc std: 0.0328

Val Epoch 3 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5444 | Final Acc: 0.8674
  Loss std: 0.0368 | Acc std: 0.0206
Epoch 03/20 | train 0.6854/0.7904 | val 0.5444/0.8674
Trial pruned at epoch 3
Trial 10 failed:
wandb:
wandb: Run history:
wandb:               batch ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà
wandb:          best_epoch ‚ñÅ‚ñà
wandb:        best_val_acc ‚ñÅ‚ñà
wandb:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: final_learning_rate ‚ñÅ‚ñÑ‚ñà
wandb:       learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà
wandb:           train_acc ‚ñÅ‚ñÖ‚ñà
wandb:       train_acc_std ‚ñá‚ñà‚ñÅ
wandb:       train_avg_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:      train_avg_loss ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                 +12 ...
wandb:
wandb: Run summary:
wandb:               batch 350
wandb:          best_epoch 2
wandb:        best_val_acc 0.82196
wandb:               epoch 3
wandb:               error
wandb: final_learning_rate 0.00043
wandb:       learning_rate 0.00042
wandb:              status failed
wandb:           train_acc 0.79041
wandb:       train_acc_std 0.03277
wandb:                 +14 ...
wandb:
wandb: üöÄ View run trial_010 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/4jw3gdgr
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_071653-4jw3gdgr/logs
[I 2025-10-06 07:20:35,657] Trial 10 pruned.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_072035-ite0hk4j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-oath-24
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/ite0hk4j
wandb: Finishing previous runs because reinit is set to True.
wandb:
wandb: Run history:
wandb:   batch_size ‚ñÅ
wandb:     channels ‚ñÅ
wandb:      dropout ‚ñÅ
wandb:      emb_dim ‚ñÅ
wandb:    grad_clip ‚ñÅ
wandb:           lr ‚ñÅ
wandb:      max_len ‚ñÅ
wandb:      val_acc ‚ñÅ
wandb: weight_decay ‚ñÅ
wandb:
wandb: Run summary:
wandb:   batch_size 256
wandb:     channels 192
wandb:      dropout 0.6
wandb:      emb_dim 200
wandb:    grad_clip 2
wandb:           lr 0.00083
wandb:      max_len 200
wandb:    optimizer adam
wandb:      val_acc 0.86742
wandb: weight_decay 0.0
wandb:
wandb: üöÄ View run trial/10/glorious-oath-24 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/ite0hk4j
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_072035-ite0hk4j/logs
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_072036-imx9qkzg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_011
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/imx9qkzg

Train Epoch 1 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 1.6174 (avg: 1.5340) | Acc: 0.2031 (avg: 0.2655) | LR: 7.06e-05
  Batch  100/750 ( 13.3%) | Loss: 1.3689 (avg: 1.5037) | Acc: 0.3438 (avg: 0.2812) | LR: 7.21e-05
  Batch  150/750 ( 20.0%) | Loss: 1.4059 (avg: 1.4809) | Acc: 0.3125 (avg: 0.2913) | LR: 7.47e-05
  Batch  200/750 ( 26.7%) | Loss: 1.4334 (avg: 1.4573) | Acc: 0.2969 (avg: 0.3036) | LR: 7.83e-05
  Batch  250/750 ( 33.3%) | Loss: 1.3583 (avg: 1.4378) | Acc: 0.3750 (avg: 0.3168) | LR: 8.29e-05
  Batch  300/750 ( 40.0%) | Loss: 1.2895 (avg: 1.4169) | Acc: 0.3359 (avg: 0.3304) | LR: 8.85e-05
  Batch  350/750 ( 46.7%) | Loss: 1.2614 (avg: 1.3965) | Acc: 0.4609 (avg: 0.3448) | LR: 9.51e-05
  Batch  400/750 ( 53.3%) | Loss: 1.1707 (avg: 1.3748) | Acc: 0.4688 (avg: 0.3600) | LR: 1.03e-04
  Batch  450/750 ( 60.0%) | Loss: 1.1808 (avg: 1.3560) | Acc: 0.4922 (avg: 0.3742) | LR: 1.11e-04
  Batch  500/750 ( 66.7%) | Loss: 1.1933 (avg: 1.3365) | Acc: 0.4609 (avg: 0.3874) | LR: 1.21e-04
  Batch  550/750 ( 73.3%) | Loss: 1.0770 (avg: 1.3167) | Acc: 0.5625 (avg: 0.4010) | LR: 1.31e-04
  Batch  600/750 ( 80.0%) | Loss: 1.0846 (avg: 1.2972) | Acc: 0.5234 (avg: 0.4148) | LR: 1.43e-04
  Batch  650/750 ( 86.7%) | Loss: 1.0829 (avg: 1.2776) | Acc: 0.5703 (avg: 0.4284) | LR: 1.55e-04
  Batch  700/750 ( 93.3%) | Loss: 0.9906 (avg: 1.2585) | Acc: 0.6172 (avg: 0.4413) | LR: 1.69e-04
  Batch  750/750 (100.0%) | Loss: 0.9716 (avg: 1.2386) | Acc: 0.6562 (avg: 0.4547) | LR: 1.83e-04

Train Summary:
  Final Loss: 1.2386 | Final Acc: 0.4547
  Final LR: 1.83e-04
  Loss std: 0.1783 | Acc std: 0.1246

Val Epoch 1 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.8192 | Final Acc: 0.7722
  Loss std: 0.0379 | Acc std: 0.0361
Epoch 01/20 | train 1.2386/0.4547 | val 0.8192/0.7722

Train Epoch 2 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.9362 (avg: 0.9077) | Acc: 0.6719 (avg: 0.6702) | LR: 1.98e-04
  Batch  100/750 ( 13.3%) | Loss: 0.8819 (avg: 0.8854) | Acc: 0.6875 (avg: 0.6820) | LR: 2.14e-04
  Batch  150/750 ( 20.0%) | Loss: 0.8404 (avg: 0.8749) | Acc: 0.6953 (avg: 0.6881) | LR: 2.31e-04
  Batch  200/750 ( 26.7%) | Loss: 0.8501 (avg: 0.8595) | Acc: 0.6953 (avg: 0.6980) | LR: 2.48e-04
  Batch  250/750 ( 33.3%) | Loss: 0.6794 (avg: 0.8454) | Acc: 0.7656 (avg: 0.7057) | LR: 2.67e-04
  Batch  300/750 ( 40.0%) | Loss: 0.8548 (avg: 0.8347) | Acc: 0.7500 (avg: 0.7115) | LR: 2.86e-04
  Batch  350/750 ( 46.7%) | Loss: 0.6888 (avg: 0.8236) | Acc: 0.8047 (avg: 0.7169) | LR: 3.06e-04
  Batch  400/750 ( 53.3%) | Loss: 0.7359 (avg: 0.8148) | Acc: 0.7422 (avg: 0.7222) | LR: 3.27e-04
  Batch  450/750 ( 60.0%) | Loss: 0.7683 (avg: 0.8028) | Acc: 0.7031 (avg: 0.7283) | LR: 3.48e-04
  Batch  500/750 ( 66.7%) | Loss: 0.7784 (avg: 0.7921) | Acc: 0.7188 (avg: 0.7338) | LR: 3.71e-04
  Batch  550/750 ( 73.3%) | Loss: 0.6548 (avg: 0.7821) | Acc: 0.8047 (avg: 0.7392) | LR: 3.93e-04
  Batch  600/750 ( 80.0%) | Loss: 0.6780 (avg: 0.7735) | Acc: 0.7812 (avg: 0.7439) | LR: 4.17e-04
  Batch  650/750 ( 86.7%) | Loss: 0.7274 (avg: 0.7667) | Acc: 0.7422 (avg: 0.7474) | LR: 4.41e-04
  Batch  700/750 ( 93.3%) | Loss: 0.6596 (avg: 0.7592) | Acc: 0.8203 (avg: 0.7517) | LR: 4.66e-04
  Batch  750/750 (100.0%) | Loss: 0.6503 (avg: 0.7528) | Acc: 0.8125 (avg: 0.7552) | LR: 4.91e-04

Train Summary:
  Final Loss: 0.7528 | Final Acc: 0.7552
  Final LR: 4.90e-04
  Loss std: 0.0984 | Acc std: 0.0569

Val Epoch 2 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5665 | Final Acc: 0.8546
  Loss std: 0.0490 | Acc std: 0.0287
Epoch 02/20 | train 0.7528/0.7552 | val 0.5665/0.8546

Train Epoch 3 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.7231 (avg: 0.6176) | Acc: 0.7891 (avg: 0.8289) | LR: 5.16e-04
  Batch  100/750 ( 13.3%) | Loss: 0.5870 (avg: 0.6122) | Acc: 0.8203 (avg: 0.8280) | LR: 5.43e-04
  Batch  150/750 ( 20.0%) | Loss: 0.5382 (avg: 0.6145) | Acc: 0.8672 (avg: 0.8269) | LR: 5.69e-04
  Batch  200/750 ( 26.7%) | Loss: 0.5052 (avg: 0.6095) | Acc: 0.8984 (avg: 0.8295) | LR: 5.96e-04
  Batch  250/750 ( 33.3%) | Loss: 0.5307 (avg: 0.6060) | Acc: 0.8750 (avg: 0.8321) | LR: 6.24e-04
  Batch  300/750 ( 40.0%) | Loss: 0.6008 (avg: 0.6038) | Acc: 0.8516 (avg: 0.8335) | LR: 6.51e-04
  Batch  350/750 ( 46.7%) | Loss: 0.5122 (avg: 0.6009) | Acc: 0.8984 (avg: 0.8352) | LR: 6.80e-04
  Batch  400/750 ( 53.3%) | Loss: 0.6582 (avg: 0.5983) | Acc: 0.8125 (avg: 0.8363) | LR: 7.08e-04
  Batch  450/750 ( 60.0%) | Loss: 0.4729 (avg: 0.5954) | Acc: 0.8906 (avg: 0.8375) | LR: 7.37e-04
  Batch  500/750 ( 66.7%) | Loss: 0.5720 (avg: 0.5934) | Acc: 0.8359 (avg: 0.8381) | LR: 7.65e-04
  Batch  550/750 ( 73.3%) | Loss: 0.5404 (avg: 0.5916) | Acc: 0.8672 (avg: 0.8392) | LR: 7.94e-04
  Batch  600/750 ( 80.0%) | Loss: 0.5388 (avg: 0.5886) | Acc: 0.8750 (avg: 0.8409) | LR: 8.24e-04
  Batch  650/750 ( 86.7%) | Loss: 0.6109 (avg: 0.5858) | Acc: 0.8203 (avg: 0.8422) | LR: 8.53e-04
  Batch  700/750 ( 93.3%) | Loss: 0.5317 (avg: 0.5830) | Acc: 0.8672 (avg: 0.8437) | LR: 8.82e-04
  Batch  750/750 (100.0%) | Loss: 0.4969 (avg: 0.5808) | Acc: 0.8906 (avg: 0.8448) | LR: 9.11e-04

Train Summary:
  Final Loss: 0.5808 | Final Acc: 0.8448
  Final LR: 9.11e-04
  Loss std: 0.0639 | Acc std: 0.0346

Val Epoch 3 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4971 | Final Acc: 0.8867
  Loss std: 0.0508 | Acc std: 0.0254
Epoch 03/20 | train 0.5808/0.8448 | val 0.4971/0.8867
Trial pruned at epoch 3
Trial 11 failed:
wandb:
wandb: Run history:
wandb:               batch ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb:          best_epoch ‚ñÅ‚ñà
wandb:        best_val_acc ‚ñÅ‚ñà
wandb:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: final_learning_rate ‚ñÅ‚ñÑ‚ñà
wandb:       learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:           train_acc ‚ñÅ‚ñÜ‚ñà
wandb:       train_acc_std ‚ñà‚ñÉ‚ñÅ
wandb:       train_avg_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:      train_avg_loss ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                 +12 ...
wandb:
wandb: Run summary:
wandb:               batch 750
wandb:          best_epoch 2
wandb:        best_val_acc 0.85462
wandb:               epoch 3
wandb:               error
wandb: final_learning_rate 0.00091
wandb:       learning_rate 0.00091
wandb:              status failed
wandb:           train_acc 0.84481
wandb:       train_acc_std 0.03457
wandb:                 +14 ...
wandb:
wandb: üöÄ View run trial_011 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/imx9qkzg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_072036-imx9qkzg/logs
[I 2025-10-06 07:24:12,357] Trial 11 pruned.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_072412-ux6lca4q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-frog-26
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/ux6lca4q
wandb: Finishing previous runs because reinit is set to True.
wandb:
wandb: Run history:
wandb:   batch_size ‚ñÅ
wandb:     channels ‚ñÅ
wandb:      dropout ‚ñÅ
wandb:      emb_dim ‚ñÅ
wandb:    grad_clip ‚ñÅ
wandb:           lr ‚ñÅ
wandb:      max_len ‚ñÅ
wandb:      val_acc ‚ñÅ
wandb: weight_decay ‚ñÅ
wandb:
wandb: Run summary:
wandb:   batch_size 128
wandb:     channels 256
wandb:      dropout 0.5
wandb:      emb_dim 100
wandb:    grad_clip 1.5
wandb:           lr 0.00175
wandb:      max_len 200
wandb:    optimizer adam
wandb:      val_acc 0.88671
wandb: weight_decay 1e-05
wandb:
wandb: üöÄ View run trial/11/devoted-frog-26 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/ux6lca4q
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_072412-ux6lca4q/logs
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_072412-92p8skxp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_012
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/92p8skxp

Train Epoch 1 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 1.4106 (avg: 1.5421) | Acc: 0.3320 (avg: 0.2661) | LR: 6.10e-05
  Batch  100/375 ( 26.7%) | Loss: 1.3775 (avg: 1.4811) | Acc: 0.3398 (avg: 0.2963) | LR: 6.62e-05
  Batch  150/375 ( 40.0%) | Loss: 1.2908 (avg: 1.4320) | Acc: 0.4648 (avg: 0.3268) | LR: 7.48e-05
  Batch  200/375 ( 53.3%) | Loss: 1.2190 (avg: 1.3861) | Acc: 0.4570 (avg: 0.3564) | LR: 8.68e-05
  Batch  250/375 ( 66.7%) | Loss: 1.1535 (avg: 1.3440) | Acc: 0.5000 (avg: 0.3851) | LR: 1.02e-04
  Batch  300/375 ( 80.0%) | Loss: 1.0689 (avg: 1.3007) | Acc: 0.5820 (avg: 0.4150) | LR: 1.21e-04
  Batch  350/375 ( 93.3%) | Loss: 0.9354 (avg: 1.2589) | Acc: 0.6875 (avg: 0.4430) | LR: 1.43e-04

Train Summary:
  Final Loss: 1.2382 | Final Acc: 0.4561
  Final LR: 1.54e-04
  Loss std: 0.1910 | Acc std: 0.1266

Val Epoch 1 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.8129 | Final Acc: 0.7784
  Loss std: 0.0288 | Acc std: 0.0235
Epoch 01/20 | train 1.2382/0.4561 | val 0.8129/0.7784

Train Epoch 2 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.7835 (avg: 0.8690) | Acc: 0.7578 (avg: 0.6958) | LR: 1.81e-04
  Batch  100/375 ( 26.7%) | Loss: 0.8456 (avg: 0.8457) | Acc: 0.6836 (avg: 0.7083) | LR: 2.10e-04
  Batch  150/375 ( 40.0%) | Loss: 0.7751 (avg: 0.8241) | Acc: 0.7305 (avg: 0.7184) | LR: 2.42e-04
  Batch  200/375 ( 53.3%) | Loss: 0.7207 (avg: 0.8043) | Acc: 0.7656 (avg: 0.7289) | LR: 2.77e-04
  Batch  250/375 ( 66.7%) | Loss: 0.5930 (avg: 0.7855) | Acc: 0.8438 (avg: 0.7382) | LR: 3.13e-04
  Batch  300/375 ( 80.0%) | Loss: 0.6847 (avg: 0.7692) | Acc: 0.7891 (avg: 0.7464) | LR: 3.53e-04
  Batch  350/375 ( 93.3%) | Loss: 0.7071 (avg: 0.7549) | Acc: 0.8164 (avg: 0.7538) | LR: 3.94e-04

Train Summary:
  Final Loss: 0.7474 | Final Acc: 0.7576
  Final LR: 4.14e-04
  Loss std: 0.0840 | Acc std: 0.0461

Val Epoch 2 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5738 | Final Acc: 0.8538
  Loss std: 0.0371 | Acc std: 0.0216
Epoch 02/20 | train 0.7474/0.7576 | val 0.5738/0.8538

Train Epoch 3 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.5596 (avg: 0.5941) | Acc: 0.8633 (avg: 0.8343) | LR: 4.59e-04
  Batch  100/375 ( 26.7%) | Loss: 0.5929 (avg: 0.5999) | Acc: 0.8398 (avg: 0.8335) | LR: 5.04e-04
  Batch  150/375 ( 40.0%) | Loss: 0.5262 (avg: 0.5938) | Acc: 0.8594 (avg: 0.8368) | LR: 5.51e-04
  Batch  200/375 ( 53.3%) | Loss: 0.6538 (avg: 0.5906) | Acc: 0.8125 (avg: 0.8393) | LR: 5.99e-04
  Batch  250/375 ( 66.7%) | Loss: 0.5000 (avg: 0.5875) | Acc: 0.8633 (avg: 0.8401) | LR: 6.47e-04
  Batch  300/375 ( 80.0%) | Loss: 0.5586 (avg: 0.5851) | Acc: 0.8477 (avg: 0.8418) | LR: 6.97e-04
  Batch  350/375 ( 93.3%) | Loss: 0.5189 (avg: 0.5826) | Acc: 0.8867 (avg: 0.8433) | LR: 7.46e-04

Train Summary:
  Final Loss: 0.5805 | Final Acc: 0.8443
  Final LR: 7.70e-04
  Loss std: 0.0431 | Acc std: 0.0244

Val Epoch 3 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5052 | Final Acc: 0.8799
  Loss std: 0.0357 | Acc std: 0.0200
Epoch 03/20 | train 0.5805/0.8443 | val 0.5052/0.8799
Trial pruned at epoch 3
Trial 12 failed:
wandb:
wandb: Run history:
wandb:               batch ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà
wandb:          best_epoch ‚ñÅ‚ñà
wandb:        best_val_acc ‚ñÅ‚ñà
wandb:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: final_learning_rate ‚ñÅ‚ñÑ‚ñà
wandb:       learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà
wandb:           train_acc ‚ñÅ‚ñÜ‚ñà
wandb:       train_acc_std ‚ñà‚ñÇ‚ñÅ
wandb:       train_avg_acc ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:      train_avg_loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                 +12 ...
wandb:
wandb: Run summary:
wandb:               batch 350
wandb:          best_epoch 2
wandb:        best_val_acc 0.85383
wandb:               epoch 3
wandb:               error
wandb: final_learning_rate 0.00077
wandb:       learning_rate 0.00075
wandb:              status failed
wandb:           train_acc 0.84431
wandb:       train_acc_std 0.02438
wandb:                 +14 ...
wandb:
wandb: üöÄ View run trial_012 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/92p8skxp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_072412-92p8skxp/logs
[I 2025-10-06 07:27:59,595] Trial 12 pruned.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_072759-2mbr8oc9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run elated-snowball-28
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/2mbr8oc9
wandb: Finishing previous runs because reinit is set to True.
wandb:
wandb: Run history:
wandb:   batch_size ‚ñÅ
wandb:     channels ‚ñÅ
wandb:      dropout ‚ñÅ
wandb:      emb_dim ‚ñÅ
wandb:    grad_clip ‚ñÅ
wandb:           lr ‚ñÅ
wandb:      max_len ‚ñÅ
wandb:      val_acc ‚ñÅ
wandb: weight_decay ‚ñÅ
wandb:
wandb: Run summary:
wandb:   batch_size 256
wandb:     channels 256
wandb:      dropout 0.5
wandb:      emb_dim 200
wandb:    grad_clip 1
wandb:           lr 0.00148
wandb:      max_len 200
wandb:    optimizer adam
wandb:      val_acc 0.87988
wandb: weight_decay 1e-05
wandb:
wandb: üöÄ View run trial/12/elated-snowball-28 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/2mbr8oc9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_072759-2mbr8oc9/logs
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_072800-d6inprqg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_013
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/d6inprqg

Train Epoch 1 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 1.6589 (avg: 1.7198) | Acc: 0.2617 (avg: 0.2551) | LR: 4.53e-06
  Batch  100/375 ( 26.7%) | Loss: 1.6073 (avg: 1.6929) | Acc: 0.2930 (avg: 0.2554) | LR: 4.91e-06
  Batch  150/375 ( 40.0%) | Loss: 1.5513 (avg: 1.6669) | Acc: 0.2734 (avg: 0.2557) | LR: 5.55e-06
  Batch  200/375 ( 53.3%) | Loss: 1.5794 (avg: 1.6450) | Acc: 0.2383 (avg: 0.2553) | LR: 6.44e-06
  Batch  250/375 ( 66.7%) | Loss: 1.5499 (avg: 1.6241) | Acc: 0.2500 (avg: 0.2575) | LR: 7.58e-06
  Batch  300/375 ( 80.0%) | Loss: 1.5228 (avg: 1.6088) | Acc: 0.2656 (avg: 0.2583) | LR: 8.96e-06
  Batch  350/375 ( 93.3%) | Loss: 1.4795 (avg: 1.5956) | Acc: 0.3047 (avg: 0.2599) | LR: 1.06e-05

Train Summary:
  Final Loss: 1.5901 | Final Acc: 0.2606
  Final LR: 1.14e-05
  Loss std: 0.0828 | Acc std: 0.0275

Val Epoch 1 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 1.3599 | Final Acc: 0.3416
  Loss std: 0.0082 | Acc std: 0.0274
Epoch 01/20 | train 1.5901/0.2606 | val 1.3599/0.3416

Train Epoch 2 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 1.5085 (avg: 1.5060) | Acc: 0.2969 (avg: 0.2707) | LR: 1.34e-05
  Batch  100/375 ( 26.7%) | Loss: 1.4971 (avg: 1.4978) | Acc: 0.3125 (avg: 0.2807) | LR: 1.56e-05
  Batch  150/375 ( 40.0%) | Loss: 1.3834 (avg: 1.4932) | Acc: 0.3242 (avg: 0.2836) | LR: 1.80e-05
  Batch  200/375 ( 53.3%) | Loss: 1.5822 (avg: 1.4875) | Acc: 0.2383 (avg: 0.2851) | LR: 2.05e-05
  Batch  250/375 ( 66.7%) | Loss: 1.5200 (avg: 1.4823) | Acc: 0.2539 (avg: 0.2878) | LR: 2.33e-05
  Batch  300/375 ( 80.0%) | Loss: 1.4480 (avg: 1.4741) | Acc: 0.3164 (avg: 0.2928) | LR: 2.62e-05
  Batch  350/375 ( 93.3%) | Loss: 1.4890 (avg: 1.4686) | Acc: 0.2852 (avg: 0.2956) | LR: 2.92e-05

Train Summary:
  Final Loss: 1.4643 | Final Acc: 0.2979
  Final LR: 3.07e-05
  Loss std: 0.0453 | Acc std: 0.0324

Val Epoch 2 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 1.2694 | Final Acc: 0.5376
  Loss std: 0.0094 | Acc std: 0.0311
Epoch 02/20 | train 1.4643/0.2979 | val 1.2694/0.5376

Train Epoch 3 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 1.4254 (avg: 1.3979) | Acc: 0.3164 (avg: 0.3354) | LR: 3.41e-05
  Batch  100/375 ( 26.7%) | Loss: 1.3633 (avg: 1.3892) | Acc: 0.3398 (avg: 0.3430) | LR: 3.74e-05
  Batch  150/375 ( 40.0%) | Loss: 1.3232 (avg: 1.3755) | Acc: 0.3945 (avg: 0.3516) | LR: 4.09e-05
  Batch  200/375 ( 53.3%) | Loss: 1.3295 (avg: 1.3638) | Acc: 0.3633 (avg: 0.3607) | LR: 4.44e-05
  Batch  250/375 ( 66.7%) | Loss: 1.3023 (avg: 1.3535) | Acc: 0.3828 (avg: 0.3685) | LR: 4.80e-05
  Batch  300/375 ( 80.0%) | Loss: 1.2899 (avg: 1.3423) | Acc: 0.3906 (avg: 0.3776) | LR: 5.17e-05
  Batch  350/375 ( 93.3%) | Loss: 1.3041 (avg: 1.3327) | Acc: 0.4180 (avg: 0.3851) | LR: 5.54e-05

Train Summary:
  Final Loss: 1.3274 | Final Acc: 0.3894
  Final LR: 5.71e-05
  Loss std: 0.0552 | Acc std: 0.0461

Val Epoch 3 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 1.1210 | Final Acc: 0.6867
  Loss std: 0.0150 | Acc std: 0.0273
Epoch 03/20 | train 1.3274/0.3894 | val 1.1210/0.6867
Trial pruned at epoch 3
Trial 13 failed:
wandb:
wandb: Run history:
wandb:               batch ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà
wandb:          best_epoch ‚ñÅ‚ñà
wandb:        best_val_acc ‚ñÅ‚ñà
wandb:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: final_learning_rate ‚ñÅ‚ñÑ‚ñà
wandb:       learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà
wandb:           train_acc ‚ñÅ‚ñÉ‚ñà
wandb:       train_acc_std ‚ñÅ‚ñÉ‚ñà
wandb:       train_avg_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà
wandb:      train_avg_loss ‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:                 +12 ...
wandb:
wandb: Run summary:
wandb:               batch 350
wandb:          best_epoch 2
wandb:        best_val_acc 0.53763
wandb:               epoch 3
wandb:               error
wandb: final_learning_rate 6e-05
wandb:       learning_rate 6e-05
wandb:              status failed
wandb:           train_acc 0.38941
wandb:       train_acc_std 0.04613
wandb:                 +14 ...
wandb:
wandb: üöÄ View run trial_013 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/d6inprqg
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_072800-d6inprqg/logs
[I 2025-10-06 07:31:15,596] Trial 13 pruned.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_073115-43vjh4tj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-microwave-30
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/43vjh4tj
wandb: Finishing previous runs because reinit is set to True.
wandb:
wandb: Run history:
wandb:   batch_size ‚ñÅ
wandb:     channels ‚ñÅ
wandb:      dropout ‚ñÅ
wandb:      emb_dim ‚ñÅ
wandb:    grad_clip ‚ñÅ
wandb:           lr ‚ñÅ
wandb:      max_len ‚ñÅ
wandb:      val_acc ‚ñÅ
wandb: weight_decay ‚ñÅ
wandb:
wandb: Run summary:
wandb:   batch_size 256
wandb:     channels 128
wandb:      dropout 0.5
wandb:      emb_dim 100
wandb:    grad_clip 1.5
wandb:           lr 0.00011
wandb:      max_len 200
wandb:    optimizer adam
wandb:      val_acc 0.68675
wandb: weight_decay 0.0
wandb:
wandb: üöÄ View run trial/13/lyric-microwave-30 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/43vjh4tj
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_073115-43vjh4tj/logs
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_073116-f1bq79yc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_014
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/f1bq79yc

Train Epoch 1 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 1.4735 (avg: 1.4810) | Acc: 0.2891 (avg: 0.2684) | LR: 7.15e-05
  Batch  100/750 ( 13.3%) | Loss: 1.4836 (avg: 1.4603) | Acc: 0.2266 (avg: 0.2760) | LR: 7.30e-05
  Batch  150/750 ( 20.0%) | Loss: 1.3754 (avg: 1.4367) | Acc: 0.3125 (avg: 0.2948) | LR: 7.56e-05
  Batch  200/750 ( 26.7%) | Loss: 1.1991 (avg: 1.4152) | Acc: 0.4609 (avg: 0.3108) | LR: 7.92e-05
  Batch  250/750 ( 33.3%) | Loss: 1.4111 (avg: 1.3950) | Acc: 0.3359 (avg: 0.3272) | LR: 8.39e-05
  Batch  300/750 ( 40.0%) | Loss: 1.2857 (avg: 1.3736) | Acc: 0.4219 (avg: 0.3445) | LR: 8.95e-05
  Batch  350/750 ( 46.7%) | Loss: 1.3067 (avg: 1.3539) | Acc: 0.3984 (avg: 0.3605) | LR: 9.62e-05
  Batch  400/750 ( 53.3%) | Loss: 1.1686 (avg: 1.3340) | Acc: 0.4922 (avg: 0.3762) | LR: 1.04e-04
  Batch  450/750 ( 60.0%) | Loss: 1.1735 (avg: 1.3143) | Acc: 0.4531 (avg: 0.3917) | LR: 1.13e-04
  Batch  500/750 ( 66.7%) | Loss: 1.1588 (avg: 1.2951) | Acc: 0.5234 (avg: 0.4075) | LR: 1.22e-04
  Batch  550/750 ( 73.3%) | Loss: 1.0632 (avg: 1.2756) | Acc: 0.6562 (avg: 0.4223) | LR: 1.33e-04
  Batch  600/750 ( 80.0%) | Loss: 1.0473 (avg: 1.2551) | Acc: 0.6016 (avg: 0.4379) | LR: 1.45e-04
  Batch  650/750 ( 86.7%) | Loss: 1.0419 (avg: 1.2351) | Acc: 0.5938 (avg: 0.4526) | LR: 1.57e-04
  Batch  700/750 ( 93.3%) | Loss: 1.0092 (avg: 1.2159) | Acc: 0.6094 (avg: 0.4660) | LR: 1.71e-04
  Batch  750/750 (100.0%) | Loss: 0.9015 (avg: 1.1962) | Acc: 0.6328 (avg: 0.4797) | LR: 1.85e-04

Train Summary:
  Final Loss: 1.1962 | Final Acc: 0.4797
  Final LR: 1.85e-04
  Loss std: 0.1775 | Acc std: 0.1355

Val Epoch 1 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.8167 | Final Acc: 0.7572
  Loss std: 0.0441 | Acc std: 0.0374
Epoch 01/20 | train 1.1962/0.4797 | val 0.8167/0.7572

Train Epoch 2 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.9469 (avg: 0.8610) | Acc: 0.6406 (avg: 0.7017) | LR: 2.00e-04
  Batch  100/750 ( 13.3%) | Loss: 0.9208 (avg: 0.8534) | Acc: 0.6250 (avg: 0.7020) | LR: 2.17e-04
  Batch  150/750 ( 20.0%) | Loss: 0.8073 (avg: 0.8421) | Acc: 0.6875 (avg: 0.7061) | LR: 2.34e-04
  Batch  200/750 ( 26.7%) | Loss: 0.7217 (avg: 0.8310) | Acc: 0.7734 (avg: 0.7134) | LR: 2.51e-04
  Batch  250/750 ( 33.3%) | Loss: 0.7879 (avg: 0.8201) | Acc: 0.7578 (avg: 0.7191) | LR: 2.70e-04
  Batch  300/750 ( 40.0%) | Loss: 0.6866 (avg: 0.8103) | Acc: 0.7891 (avg: 0.7243) | LR: 2.90e-04
  Batch  350/750 ( 46.7%) | Loss: 0.6996 (avg: 0.7968) | Acc: 0.7812 (avg: 0.7318) | LR: 3.10e-04
  Batch  400/750 ( 53.3%) | Loss: 0.6123 (avg: 0.7859) | Acc: 0.8281 (avg: 0.7367) | LR: 3.31e-04
  Batch  450/750 ( 60.0%) | Loss: 0.7233 (avg: 0.7778) | Acc: 0.7656 (avg: 0.7416) | LR: 3.53e-04
  Batch  500/750 ( 66.7%) | Loss: 0.7594 (avg: 0.7699) | Acc: 0.7500 (avg: 0.7456) | LR: 3.75e-04
  Batch  550/750 ( 73.3%) | Loss: 0.7242 (avg: 0.7610) | Acc: 0.7812 (avg: 0.7498) | LR: 3.98e-04
  Batch  600/750 ( 80.0%) | Loss: 0.6087 (avg: 0.7526) | Acc: 0.8281 (avg: 0.7543) | LR: 4.22e-04
  Batch  650/750 ( 86.7%) | Loss: 0.6544 (avg: 0.7462) | Acc: 0.7734 (avg: 0.7580) | LR: 4.46e-04
  Batch  700/750 ( 93.3%) | Loss: 0.5729 (avg: 0.7399) | Acc: 0.8750 (avg: 0.7614) | LR: 4.71e-04
  Batch  750/750 (100.0%) | Loss: 0.6272 (avg: 0.7342) | Acc: 0.8125 (avg: 0.7649) | LR: 4.97e-04

Train Summary:
  Final Loss: 0.7342 | Final Acc: 0.7649
  Final LR: 4.96e-04
  Loss std: 0.0917 | Acc std: 0.0516

Val Epoch 2 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5757 | Final Acc: 0.8493
  Loss std: 0.0524 | Acc std: 0.0311
Epoch 02/20 | train 0.7342/0.7649 | val 0.5757/0.8493

Train Epoch 3 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.5651 (avg: 0.5741) | Acc: 0.8828 (avg: 0.8448) | LR: 5.23e-04
  Batch  100/750 ( 13.3%) | Loss: 0.5195 (avg: 0.5873) | Acc: 0.8672 (avg: 0.8402) | LR: 5.49e-04
  Batch  150/750 ( 20.0%) | Loss: 0.5682 (avg: 0.5879) | Acc: 0.8359 (avg: 0.8401) | LR: 5.76e-04
  Batch  200/750 ( 26.7%) | Loss: 0.5655 (avg: 0.5832) | Acc: 0.8672 (avg: 0.8429) | LR: 6.04e-04
  Batch  250/750 ( 33.3%) | Loss: 0.6485 (avg: 0.5826) | Acc: 0.8281 (avg: 0.8440) | LR: 6.31e-04
  Batch  300/750 ( 40.0%) | Loss: 0.5898 (avg: 0.5825) | Acc: 0.8125 (avg: 0.8435) | LR: 6.59e-04
  Batch  350/750 ( 46.7%) | Loss: 0.5876 (avg: 0.5804) | Acc: 0.8281 (avg: 0.8445) | LR: 6.88e-04
  Batch  400/750 ( 53.3%) | Loss: 0.5014 (avg: 0.5796) | Acc: 0.8906 (avg: 0.8449) | LR: 7.17e-04
  Batch  450/750 ( 60.0%) | Loss: 0.6544 (avg: 0.5767) | Acc: 0.7969 (avg: 0.8462) | LR: 7.45e-04
  Batch  500/750 ( 66.7%) | Loss: 0.5877 (avg: 0.5764) | Acc: 0.8125 (avg: 0.8462) | LR: 7.75e-04
  Batch  550/750 ( 73.3%) | Loss: 0.6258 (avg: 0.5734) | Acc: 0.8438 (avg: 0.8479) | LR: 8.04e-04
  Batch  600/750 ( 80.0%) | Loss: 0.5027 (avg: 0.5711) | Acc: 0.9062 (avg: 0.8488) | LR: 8.33e-04
  Batch  650/750 ( 86.7%) | Loss: 0.5239 (avg: 0.5692) | Acc: 0.8516 (avg: 0.8499) | LR: 8.63e-04
  Batch  700/750 ( 93.3%) | Loss: 0.4779 (avg: 0.5672) | Acc: 0.8984 (avg: 0.8507) | LR: 8.93e-04
  Batch  750/750 (100.0%) | Loss: 0.6984 (avg: 0.5664) | Acc: 0.8125 (avg: 0.8512) | LR: 9.23e-04

Train Summary:
  Final Loss: 0.5664 | Final Acc: 0.8512
  Final LR: 9.22e-04
  Loss std: 0.0593 | Acc std: 0.0336

Val Epoch 3 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5066 | Final Acc: 0.8828
  Loss std: 0.0505 | Acc std: 0.0274
Epoch 03/20 | train 0.5664/0.8512 | val 0.5066/0.8828
Trial pruned at epoch 3
Trial 14 failed:
wandb:
wandb: Run history:
wandb:               batch ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb:          best_epoch ‚ñÅ‚ñà
wandb:        best_val_acc ‚ñÅ‚ñà
wandb:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: final_learning_rate ‚ñÅ‚ñÑ‚ñà
wandb:       learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:           train_acc ‚ñÅ‚ñÜ‚ñà
wandb:       train_acc_std ‚ñà‚ñÇ‚ñÅ
wandb:       train_avg_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:      train_avg_loss ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                 +12 ...
wandb:
wandb: Run summary:
wandb:               batch 750
wandb:          best_epoch 2
wandb:        best_val_acc 0.84933
wandb:               epoch 3
wandb:               error
wandb: final_learning_rate 0.00092
wandb:       learning_rate 0.00092
wandb:              status failed
wandb:           train_acc 0.8512
wandb:       train_acc_std 0.03361
wandb:                 +14 ...
wandb:
wandb: üöÄ View run trial_014 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/f1bq79yc
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_073116-f1bq79yc/logs
[I 2025-10-06 07:34:51,530] Trial 14 pruned.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_073451-17bwoaby
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run chocolate-dragon-32
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/17bwoaby
wandb: Finishing previous runs because reinit is set to True.
wandb:
wandb: Run history:
wandb:   batch_size ‚ñÅ
wandb:     channels ‚ñÅ
wandb:      dropout ‚ñÅ
wandb:      emb_dim ‚ñÅ
wandb:    grad_clip ‚ñÅ
wandb:           lr ‚ñÅ
wandb:      max_len ‚ñÅ
wandb:      val_acc ‚ñÅ
wandb: weight_decay ‚ñÅ
wandb:
wandb: Run summary:
wandb:   batch_size 128
wandb:     channels 192
wandb:      dropout 0.4
wandb:      emb_dim 100
wandb:    grad_clip 1
wandb:           lr 0.00177
wandb:      max_len 200
wandb:    optimizer adam
wandb:      val_acc 0.88275
wandb: weight_decay 2e-05
wandb:
wandb: üöÄ View run trial/14/chocolate-dragon-32 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/17bwoaby
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_073451-17bwoaby/logs
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_073452-remys147
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_015
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/remys147

Train Epoch 1 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 1.4317 (avg: 1.5358) | Acc: 0.3281 (avg: 0.2502) | LR: 2.53e-05
  Batch  100/750 ( 13.3%) | Loss: 1.5228 (avg: 1.5037) | Acc: 0.1953 (avg: 0.2602) | LR: 2.59e-05
  Batch  150/750 ( 20.0%) | Loss: 1.4632 (avg: 1.4834) | Acc: 0.2734 (avg: 0.2702) | LR: 2.68e-05
  Batch  200/750 ( 26.7%) | Loss: 1.4295 (avg: 1.4699) | Acc: 0.2812 (avg: 0.2771) | LR: 2.81e-05
  Batch  250/750 ( 33.3%) | Loss: 1.4307 (avg: 1.4560) | Acc: 0.3281 (avg: 0.2850) | LR: 2.97e-05
  Batch  300/750 ( 40.0%) | Loss: 1.3332 (avg: 1.4414) | Acc: 0.4297 (avg: 0.2956) | LR: 3.17e-05
  Batch  350/750 ( 46.7%) | Loss: 1.3525 (avg: 1.4292) | Acc: 0.3438 (avg: 0.3046) | LR: 3.41e-05
  Batch  400/750 ( 53.3%) | Loss: 1.3267 (avg: 1.4167) | Acc: 0.3750 (avg: 0.3133) | LR: 3.68e-05
  Batch  450/750 ( 60.0%) | Loss: 1.3412 (avg: 1.4037) | Acc: 0.3828 (avg: 0.3225) | LR: 3.99e-05
  Batch  500/750 ( 66.7%) | Loss: 1.2658 (avg: 1.3905) | Acc: 0.4062 (avg: 0.3329) | LR: 4.33e-05
  Batch  550/750 ( 73.3%) | Loss: 1.2458 (avg: 1.3769) | Acc: 0.4609 (avg: 0.3443) | LR: 4.71e-05
  Batch  600/750 ( 80.0%) | Loss: 1.1806 (avg: 1.3629) | Acc: 0.5078 (avg: 0.3558) | LR: 5.12e-05
  Batch  650/750 ( 86.7%) | Loss: 1.1966 (avg: 1.3488) | Acc: 0.4766 (avg: 0.3669) | LR: 5.56e-05
  Batch  700/750 ( 93.3%) | Loss: 1.1048 (avg: 1.3351) | Acc: 0.6016 (avg: 0.3779) | LR: 6.04e-05
  Batch  750/750 (100.0%) | Loss: 1.1140 (avg: 1.3211) | Acc: 0.5547 (avg: 0.3890) | LR: 6.55e-05

Train Summary:
  Final Loss: 1.3211 | Final Acc: 0.3890
  Final LR: 6.54e-05
  Loss std: 0.1267 | Acc std: 0.1017

Val Epoch 1 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 1.0106 | Final Acc: 0.7252
  Loss std: 0.0298 | Acc std: 0.0392
Epoch 01/20 | train 1.3211/0.3890 | val 1.0106/0.7252

Train Epoch 2 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 1.0033 (avg: 1.0696) | Acc: 0.6719 (avg: 0.5894) | LR: 7.09e-05
  Batch  100/750 ( 13.3%) | Loss: 0.9699 (avg: 1.0529) | Acc: 0.6250 (avg: 0.5970) | LR: 7.67e-05
  Batch  150/750 ( 20.0%) | Loss: 0.9776 (avg: 1.0352) | Acc: 0.6172 (avg: 0.6068) | LR: 8.27e-05
  Batch  200/750 ( 26.7%) | Loss: 0.9595 (avg: 1.0212) | Acc: 0.6484 (avg: 0.6150) | LR: 8.91e-05
  Batch  250/750 ( 33.3%) | Loss: 0.9455 (avg: 1.0057) | Acc: 0.6641 (avg: 0.6236) | LR: 9.57e-05
  Batch  300/750 ( 40.0%) | Loss: 0.8650 (avg: 0.9920) | Acc: 0.7031 (avg: 0.6309) | LR: 1.03e-04
  Batch  350/750 ( 46.7%) | Loss: 0.8554 (avg: 0.9779) | Acc: 0.7188 (avg: 0.6384) | LR: 1.10e-04
  Batch  400/750 ( 53.3%) | Loss: 0.9241 (avg: 0.9636) | Acc: 0.6250 (avg: 0.6462) | LR: 1.17e-04
  Batch  450/750 ( 60.0%) | Loss: 0.8123 (avg: 0.9493) | Acc: 0.7578 (avg: 0.6541) | LR: 1.25e-04
  Batch  500/750 ( 66.7%) | Loss: 0.8901 (avg: 0.9358) | Acc: 0.7109 (avg: 0.6614) | LR: 1.33e-04
  Batch  550/750 ( 73.3%) | Loss: 0.8563 (avg: 0.9219) | Acc: 0.7188 (avg: 0.6691) | LR: 1.41e-04
  Batch  600/750 ( 80.0%) | Loss: 0.7454 (avg: 0.9098) | Acc: 0.7500 (avg: 0.6755) | LR: 1.49e-04
  Batch  650/750 ( 86.7%) | Loss: 0.7447 (avg: 0.8976) | Acc: 0.7500 (avg: 0.6818) | LR: 1.58e-04
  Batch  700/750 ( 93.3%) | Loss: 0.6839 (avg: 0.8861) | Acc: 0.7734 (avg: 0.6879) | LR: 1.67e-04
  Batch  750/750 (100.0%) | Loss: 0.7584 (avg: 0.8762) | Acc: 0.7500 (avg: 0.6929) | LR: 1.76e-04

Train Summary:
  Final Loss: 0.8762 | Final Acc: 0.6929
  Final LR: 1.76e-04
  Loss std: 0.1205 | Acc std: 0.0702

Val Epoch 2 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.6379 | Final Acc: 0.8321
  Loss std: 0.0486 | Acc std: 0.0334
Epoch 02/20 | train 0.8762/0.6929 | val 0.6379/0.8321

Train Epoch 3 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.7406 (avg: 0.6813) | Acc: 0.7812 (avg: 0.7950) | LR: 1.85e-04
  Batch  100/750 ( 13.3%) | Loss: 0.7793 (avg: 0.6681) | Acc: 0.8047 (avg: 0.8030) | LR: 1.94e-04
  Batch  150/750 ( 20.0%) | Loss: 0.6162 (avg: 0.6677) | Acc: 0.8281 (avg: 0.8021) | LR: 2.04e-04
  Batch  200/750 ( 26.7%) | Loss: 0.7120 (avg: 0.6652) | Acc: 0.7656 (avg: 0.8041) | LR: 2.14e-04
  Batch  250/750 ( 33.3%) | Loss: 0.6521 (avg: 0.6623) | Acc: 0.8359 (avg: 0.8056) | LR: 2.24e-04
  Batch  300/750 ( 40.0%) | Loss: 0.5493 (avg: 0.6606) | Acc: 0.8594 (avg: 0.8060) | LR: 2.33e-04
  Batch  350/750 ( 46.7%) | Loss: 0.6501 (avg: 0.6563) | Acc: 0.8047 (avg: 0.8086) | LR: 2.44e-04
  Batch  400/750 ( 53.3%) | Loss: 0.6265 (avg: 0.6538) | Acc: 0.8281 (avg: 0.8100) | LR: 2.54e-04
  Batch  450/750 ( 60.0%) | Loss: 0.6513 (avg: 0.6503) | Acc: 0.8281 (avg: 0.8117) | LR: 2.64e-04
  Batch  500/750 ( 66.7%) | Loss: 0.6286 (avg: 0.6479) | Acc: 0.8281 (avg: 0.8129) | LR: 2.74e-04
  Batch  550/750 ( 73.3%) | Loss: 0.5639 (avg: 0.6445) | Acc: 0.8438 (avg: 0.8142) | LR: 2.85e-04
  Batch  600/750 ( 80.0%) | Loss: 0.6341 (avg: 0.6413) | Acc: 0.8516 (avg: 0.8155) | LR: 2.95e-04
  Batch  650/750 ( 86.7%) | Loss: 0.6325 (avg: 0.6387) | Acc: 0.7969 (avg: 0.8166) | LR: 3.06e-04
  Batch  700/750 ( 93.3%) | Loss: 0.5273 (avg: 0.6356) | Acc: 0.8750 (avg: 0.8181) | LR: 3.16e-04
  Batch  750/750 (100.0%) | Loss: 0.5460 (avg: 0.6318) | Acc: 0.8516 (avg: 0.8199) | LR: 3.27e-04

Train Summary:
  Final Loss: 0.6318 | Final Acc: 0.8199
  Final LR: 3.26e-04
  Loss std: 0.0661 | Acc std: 0.0364

Val Epoch 3 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5293 | Final Acc: 0.8755
  Loss std: 0.0491 | Acc std: 0.0301
Epoch 03/20 | train 0.6318/0.8199 | val 0.5293/0.8755
Trial pruned at epoch 3
Trial 15 failed:
wandb:
wandb: Run history:
wandb:               batch ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb:          best_epoch ‚ñÅ‚ñà
wandb:        best_val_acc ‚ñÅ‚ñà
wandb:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: final_learning_rate ‚ñÅ‚ñÑ‚ñà
wandb:       learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:           train_acc ‚ñÅ‚ñÜ‚ñà
wandb:       train_acc_std ‚ñà‚ñÖ‚ñÅ
wandb:       train_avg_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:      train_avg_loss ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                 +12 ...
wandb:
wandb: Run summary:
wandb:               batch 750
wandb:          best_epoch 2
wandb:        best_val_acc 0.83208
wandb:               epoch 3
wandb:               error
wandb: final_learning_rate 0.00033
wandb:       learning_rate 0.00033
wandb:              status failed
wandb:           train_acc 0.81995
wandb:       train_acc_std 0.03637
wandb:                 +14 ...
wandb:
wandb: üöÄ View run trial_015 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/remys147
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_073452-remys147/logs
[I 2025-10-06 07:38:28,224] Trial 15 pruned.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_073828-etg3259p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-sponge-34
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/etg3259p
wandb: Finishing previous runs because reinit is set to True.
wandb:
wandb: Run history:
wandb:   batch_size ‚ñÅ
wandb:     channels ‚ñÅ
wandb:      dropout ‚ñÅ
wandb:      emb_dim ‚ñÅ
wandb:    grad_clip ‚ñÅ
wandb:           lr ‚ñÅ
wandb:      max_len ‚ñÅ
wandb:      val_acc ‚ñÅ
wandb: weight_decay ‚ñÅ
wandb:
wandb: Run summary:
wandb:   batch_size 128
wandb:     channels 128
wandb:      dropout 0.4
wandb:      emb_dim 200
wandb:    grad_clip 2
wandb:           lr 0.00063
wandb:      max_len 200
wandb:    optimizer adam
wandb:      val_acc 0.87546
wandb: weight_decay 0.0
wandb:
wandb: üöÄ View run trial/15/solar-sponge-34 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/etg3259p
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_073828-etg3259p/logs
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_073828-sq13m4n6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_016
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/sq13m4n6

Train Epoch 1 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 1.4531 (avg: 1.5079) | Acc: 0.2891 (avg: 0.2765) | LR: 1.04e-04
  Batch  100/375 ( 26.7%) | Loss: 1.3573 (avg: 1.4725) | Acc: 0.3477 (avg: 0.2954) | LR: 1.13e-04
  Batch  150/375 ( 40.0%) | Loss: 1.3115 (avg: 1.4301) | Acc: 0.3984 (avg: 0.3221) | LR: 1.28e-04
  Batch  200/375 ( 53.3%) | Loss: 1.2952 (avg: 1.3915) | Acc: 0.4453 (avg: 0.3481) | LR: 1.49e-04
  Batch  250/375 ( 66.7%) | Loss: 1.1625 (avg: 1.3529) | Acc: 0.5586 (avg: 0.3757) | LR: 1.75e-04
  Batch  300/375 ( 80.0%) | Loss: 1.0829 (avg: 1.3138) | Acc: 0.5703 (avg: 0.4034) | LR: 2.07e-04
  Batch  350/375 ( 93.3%) | Loss: 1.0579 (avg: 1.2758) | Acc: 0.5664 (avg: 0.4297) | LR: 2.44e-04

Train Summary:
  Final Loss: 1.2568 | Final Acc: 0.4421
  Final LR: 2.64e-04
  Loss std: 0.1704 | Acc std: 0.1177

Val Epoch 1 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.8591 | Final Acc: 0.7530
  Loss std: 0.0292 | Acc std: 0.0238
Epoch 01/20 | train 1.2568/0.4421 | val 0.8591/0.7530

Train Epoch 2 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.8573 (avg: 0.9061) | Acc: 0.6914 (avg: 0.6694) | LR: 3.10e-04
  Batch  100/375 ( 26.7%) | Loss: 0.7949 (avg: 0.8800) | Acc: 0.7852 (avg: 0.6848) | LR: 3.60e-04
  Batch  150/375 ( 40.0%) | Loss: 0.8534 (avg: 0.8553) | Acc: 0.6836 (avg: 0.6985) | LR: 4.14e-04
  Batch  200/375 ( 53.3%) | Loss: 0.7312 (avg: 0.8352) | Acc: 0.7891 (avg: 0.7104) | LR: 4.73e-04
  Batch  250/375 ( 66.7%) | Loss: 0.7215 (avg: 0.8149) | Acc: 0.7773 (avg: 0.7211) | LR: 5.36e-04
  Batch  300/375 ( 80.0%) | Loss: 0.7145 (avg: 0.7977) | Acc: 0.7773 (avg: 0.7307) | LR: 6.03e-04
  Batch  350/375 ( 93.3%) | Loss: 0.7040 (avg: 0.7802) | Acc: 0.7852 (avg: 0.7398) | LR: 6.74e-04

Train Summary:
  Final Loss: 0.7717 | Final Acc: 0.7440
  Final LR: 7.09e-04
  Loss std: 0.0899 | Acc std: 0.0513

Val Epoch 2 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5800 | Final Acc: 0.8485
  Loss std: 0.0379 | Acc std: 0.0206
Epoch 02/20 | train 0.7717/0.7440 | val 0.5800/0.8485

Train Epoch 3 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.5711 (avg: 0.6181) | Acc: 0.8242 (avg: 0.8245) | LR: 7.85e-04
  Batch  100/375 ( 26.7%) | Loss: 0.6421 (avg: 0.6139) | Acc: 0.7930 (avg: 0.8266) | LR: 8.63e-04
  Batch  150/375 ( 40.0%) | Loss: 0.6328 (avg: 0.6080) | Acc: 0.8008 (avg: 0.8296) | LR: 9.43e-04
  Batch  200/375 ( 53.3%) | Loss: 0.5775 (avg: 0.6080) | Acc: 0.8516 (avg: 0.8292) | LR: 1.02e-03
  Batch  250/375 ( 66.7%) | Loss: 0.6045 (avg: 0.6056) | Acc: 0.8203 (avg: 0.8312) | LR: 1.11e-03
  Batch  300/375 ( 80.0%) | Loss: 0.5541 (avg: 0.6013) | Acc: 0.8711 (avg: 0.8338) | LR: 1.19e-03
  Batch  350/375 ( 93.3%) | Loss: 0.5455 (avg: 0.5971) | Acc: 0.8672 (avg: 0.8359) | LR: 1.28e-03

Train Summary:
  Final Loss: 0.5948 | Final Acc: 0.8369
  Final LR: 1.32e-03
  Loss std: 0.0453 | Acc std: 0.0239

Val Epoch 3 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5065 | Final Acc: 0.8817
  Loss std: 0.0360 | Acc std: 0.0181
Epoch 03/20 | train 0.5948/0.8369 | val 0.5065/0.8817
Trial pruned at epoch 3
Trial 16 failed:
wandb:
wandb: Run history:
wandb:               batch ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà
wandb:          best_epoch ‚ñÅ‚ñà
wandb:        best_val_acc ‚ñÅ‚ñà
wandb:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: final_learning_rate ‚ñÅ‚ñÑ‚ñà
wandb:       learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà
wandb:           train_acc ‚ñÅ‚ñÜ‚ñà
wandb:       train_acc_std ‚ñà‚ñÉ‚ñÅ
wandb:       train_avg_acc ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:      train_avg_loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                 +12 ...
wandb:
wandb: Run summary:
wandb:               batch 350
wandb:          best_epoch 2
wandb:        best_val_acc 0.84846
wandb:               epoch 3
wandb:               error
wandb: final_learning_rate 0.00132
wandb:       learning_rate 0.00128
wandb:              status failed
wandb:           train_acc 0.83692
wandb:       train_acc_std 0.02388
wandb:                 +14 ...
wandb:
wandb: üöÄ View run trial_016 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/sq13m4n6
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_073828-sq13m4n6/logs
[I 2025-10-06 07:42:02,294] Trial 16 pruned.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_074202-fbs83vuw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-bird-36
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/fbs83vuw
wandb: Finishing previous runs because reinit is set to True.
wandb:
wandb: Run history:
wandb:   batch_size ‚ñÅ
wandb:     channels ‚ñÅ
wandb:      dropout ‚ñÅ
wandb:      emb_dim ‚ñÅ
wandb:    grad_clip ‚ñÅ
wandb:           lr ‚ñÅ
wandb:      max_len ‚ñÅ
wandb:      val_acc ‚ñÅ
wandb: weight_decay ‚ñÅ
wandb:
wandb: Run summary:
wandb:   batch_size 256
wandb:     channels 256
wandb:      dropout 0.5
wandb:      emb_dim 100
wandb:    grad_clip 1.5
wandb:           lr 0.00253
wandb:      max_len 200
wandb:    optimizer adam
wandb:      val_acc 0.88167
wandb: weight_decay 2e-05
wandb:
wandb: üöÄ View run trial/16/glad-bird-36 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/fbs83vuw
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_074202-fbs83vuw/logs
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_074202-wdicjk70
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_017
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/wdicjk70

Train Epoch 1 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 1.5698 (avg: 1.6086) | Acc: 0.2617 (avg: 0.2603) | LR: 4.56e-05
  Batch  100/375 ( 26.7%) | Loss: 1.4867 (avg: 1.5666) | Acc: 0.3594 (avg: 0.2786) | LR: 4.95e-05
  Batch  150/375 ( 40.0%) | Loss: 1.4386 (avg: 1.5319) | Acc: 0.3164 (avg: 0.2933) | LR: 5.60e-05
  Batch  200/375 ( 53.3%) | Loss: 1.3571 (avg: 1.4967) | Acc: 0.3828 (avg: 0.3105) | LR: 6.50e-05
  Batch  250/375 ( 66.7%) | Loss: 1.3134 (avg: 1.4617) | Acc: 0.4219 (avg: 0.3290) | LR: 7.65e-05
  Batch  300/375 ( 80.0%) | Loss: 1.2452 (avg: 1.4294) | Acc: 0.4570 (avg: 0.3481) | LR: 9.04e-05
  Batch  350/375 ( 93.3%) | Loss: 1.1789 (avg: 1.3951) | Acc: 0.5547 (avg: 0.3682) | LR: 1.07e-04

Train Summary:
  Final Loss: 1.3783 | Final Acc: 0.3787
  Final LR: 1.15e-04
  Loss std: 0.1531 | Acc std: 0.0889

Val Epoch 1 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.9646 | Final Acc: 0.7479
  Loss std: 0.0226 | Acc std: 0.0235
Epoch 01/20 | train 1.3783/0.3787 | val 0.9646/0.7479

Train Epoch 2 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 1.0291 (avg: 1.0741) | Acc: 0.6406 (avg: 0.5681) | LR: 1.35e-04
  Batch  100/375 ( 26.7%) | Loss: 0.9729 (avg: 1.0432) | Acc: 0.6172 (avg: 0.5878) | LR: 1.57e-04
  Batch  150/375 ( 40.0%) | Loss: 0.9354 (avg: 1.0127) | Acc: 0.6797 (avg: 0.6064) | LR: 1.81e-04
  Batch  200/375 ( 53.3%) | Loss: 0.9115 (avg: 0.9833) | Acc: 0.7031 (avg: 0.6241) | LR: 2.07e-04
  Batch  250/375 ( 66.7%) | Loss: 0.9064 (avg: 0.9606) | Acc: 0.6367 (avg: 0.6375) | LR: 2.35e-04
  Batch  300/375 ( 80.0%) | Loss: 0.8134 (avg: 0.9387) | Acc: 0.7266 (avg: 0.6497) | LR: 2.64e-04
  Batch  350/375 ( 93.3%) | Loss: 0.7760 (avg: 0.9183) | Acc: 0.7422 (avg: 0.6614) | LR: 2.95e-04

Train Summary:
  Final Loss: 0.9086 | Final Acc: 0.6668
  Final LR: 3.10e-04
  Loss std: 0.1065 | Acc std: 0.0642

Val Epoch 2 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.6518 | Final Acc: 0.8274
  Loss std: 0.0352 | Acc std: 0.0242
Epoch 02/20 | train 0.9086/0.6668 | val 0.6518/0.8274

Train Epoch 3 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.7116 (avg: 0.7266) | Acc: 0.7578 (avg: 0.7699) | LR: 3.43e-04
  Batch  100/375 ( 26.7%) | Loss: 0.7539 (avg: 0.7244) | Acc: 0.7305 (avg: 0.7716) | LR: 3.77e-04
  Batch  150/375 ( 40.0%) | Loss: 0.6518 (avg: 0.7179) | Acc: 0.8086 (avg: 0.7736) | LR: 4.12e-04
  Batch  200/375 ( 53.3%) | Loss: 0.7612 (avg: 0.7104) | Acc: 0.7617 (avg: 0.7775) | LR: 4.48e-04
  Batch  250/375 ( 66.7%) | Loss: 0.5866 (avg: 0.7050) | Acc: 0.8516 (avg: 0.7805) | LR: 4.84e-04
  Batch  300/375 ( 80.0%) | Loss: 0.6472 (avg: 0.6980) | Acc: 0.8086 (avg: 0.7840) | LR: 5.21e-04
  Batch  350/375 ( 93.3%) | Loss: 0.6359 (avg: 0.6911) | Acc: 0.8008 (avg: 0.7871) | LR: 5.58e-04

Train Summary:
  Final Loss: 0.6894 | Final Acc: 0.7882
  Final LR: 5.76e-04
  Loss std: 0.0502 | Acc std: 0.0280

Val Epoch 3 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5667 | Final Acc: 0.8552
  Loss std: 0.0359 | Acc std: 0.0210
Epoch 03/20 | train 0.6894/0.7882 | val 0.5667/0.8552
Trial pruned at epoch 3
Trial 17 failed:
wandb:
wandb: Run history:
wandb:               batch ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà
wandb:          best_epoch ‚ñÅ‚ñà
wandb:        best_val_acc ‚ñÅ‚ñà
wandb:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: final_learning_rate ‚ñÅ‚ñÑ‚ñà
wandb:       learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà
wandb:           train_acc ‚ñÅ‚ñÜ‚ñà
wandb:       train_acc_std ‚ñà‚ñÖ‚ñÅ
wandb:       train_avg_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:      train_avg_loss ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                 +12 ...
wandb:
wandb: Run summary:
wandb:               batch 350
wandb:          best_epoch 2
wandb:        best_val_acc 0.82737
wandb:               epoch 3
wandb:               error
wandb: final_learning_rate 0.00058
wandb:       learning_rate 0.00056
wandb:              status failed
wandb:           train_acc 0.78824
wandb:       train_acc_std 0.02795
wandb:                 +14 ...
wandb:
wandb: üöÄ View run trial_017 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/wdicjk70
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_074202-wdicjk70/logs
[I 2025-10-06 07:46:00,342] Trial 17 pruned.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_074600-76pcodlf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-totem-38
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/76pcodlf
wandb: Finishing previous runs because reinit is set to True.
wandb:
wandb: Run history:
wandb:   batch_size ‚ñÅ
wandb:     channels ‚ñÅ
wandb:      dropout ‚ñÅ
wandb:      emb_dim ‚ñÅ
wandb:    grad_clip ‚ñÅ
wandb:           lr ‚ñÅ
wandb:      max_len ‚ñÅ
wandb:      val_acc ‚ñÅ
wandb: weight_decay ‚ñÅ
wandb:
wandb: Run summary:
wandb:   batch_size 256
wandb:     channels 256
wandb:      dropout 0.6
wandb:      emb_dim 200
wandb:    grad_clip 1
wandb:           lr 0.00111
wandb:      max_len 256
wandb:    optimizer adam
wandb:      val_acc 0.85517
wandb: weight_decay 0.0001
wandb:
wandb: üöÄ View run trial/17/misunderstood-totem-38 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/76pcodlf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_074600-76pcodlf/logs
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_074601-fdl6rgfj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_018
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/fdl6rgfj

Train Epoch 1 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 1.5170 (avg: 1.8051) | Acc: 0.2109 (avg: 0.2422) | LR: 2.15e-05
  Batch  100/750 ( 13.3%) | Loss: 1.5268 (avg: 1.6642) | Acc: 0.2812 (avg: 0.2545) | LR: 2.20e-05
  Batch  150/750 ( 20.0%) | Loss: 1.4671 (avg: 1.5898) | Acc: 0.3203 (avg: 0.2657) | LR: 2.28e-05
  Batch  200/750 ( 26.7%) | Loss: 1.4507 (avg: 1.5487) | Acc: 0.3438 (avg: 0.2744) | LR: 2.39e-05
  Batch  250/750 ( 33.3%) | Loss: 1.3541 (avg: 1.5179) | Acc: 0.3516 (avg: 0.2850) | LR: 2.53e-05
  Batch  300/750 ( 40.0%) | Loss: 1.3252 (avg: 1.4929) | Acc: 0.3906 (avg: 0.2952) | LR: 2.70e-05
  Batch  350/750 ( 46.7%) | Loss: 1.2956 (avg: 1.4711) | Acc: 0.4062 (avg: 0.3056) | LR: 2.90e-05
  Batch  400/750 ( 53.3%) | Loss: 1.3477 (avg: 1.4531) | Acc: 0.3203 (avg: 0.3145) | LR: 3.13e-05
  Batch  450/750 ( 60.0%) | Loss: 1.2611 (avg: 1.4359) | Acc: 0.4062 (avg: 0.3243) | LR: 3.39e-05
  Batch  500/750 ( 66.7%) | Loss: 1.2162 (avg: 1.4191) | Acc: 0.4531 (avg: 0.3355) | LR: 3.68e-05
  Batch  550/750 ( 73.3%) | Loss: 1.2819 (avg: 1.4021) | Acc: 0.4297 (avg: 0.3471) | LR: 4.00e-05
  Batch  600/750 ( 80.0%) | Loss: 1.2138 (avg: 1.3857) | Acc: 0.4531 (avg: 0.3576) | LR: 4.35e-05
  Batch  650/750 ( 86.7%) | Loss: 1.2249 (avg: 1.3697) | Acc: 0.4375 (avg: 0.3687) | LR: 4.73e-05
  Batch  700/750 ( 93.3%) | Loss: 1.1455 (avg: 1.3533) | Acc: 0.5781 (avg: 0.3808) | LR: 5.14e-05
  Batch  750/750 (100.0%) | Loss: 1.1315 (avg: 1.3368) | Acc: 0.5312 (avg: 0.3928) | LR: 5.57e-05

Train Summary:
  Final Loss: 1.3368 | Final Acc: 0.3928
  Final LR: 5.56e-05
  Loss std: 0.1786 | Acc std: 0.1059

Val Epoch 1 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 1.0139 | Final Acc: 0.7228
  Loss std: 0.0287 | Acc std: 0.0409
Epoch 01/20 | train 1.3368/0.3928 | val 1.0139/0.7228

Train Epoch 2 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 1.1533 (avg: 1.0771) | Acc: 0.5312 (avg: 0.5747) | LR: 6.03e-05
  Batch  100/750 ( 13.3%) | Loss: 0.9738 (avg: 1.0629) | Acc: 0.6328 (avg: 0.5874) | LR: 6.52e-05
  Batch  150/750 ( 20.0%) | Loss: 0.8934 (avg: 1.0461) | Acc: 0.6562 (avg: 0.5989) | LR: 7.04e-05
  Batch  200/750 ( 26.7%) | Loss: 0.9672 (avg: 1.0290) | Acc: 0.6250 (avg: 0.6107) | LR: 7.57e-05
  Batch  250/750 ( 33.3%) | Loss: 0.9618 (avg: 1.0113) | Acc: 0.6875 (avg: 0.6218) | LR: 8.14e-05
  Batch  300/750 ( 40.0%) | Loss: 0.9093 (avg: 0.9970) | Acc: 0.7031 (avg: 0.6299) | LR: 8.73e-05
  Batch  350/750 ( 46.7%) | Loss: 0.8380 (avg: 0.9828) | Acc: 0.6719 (avg: 0.6374) | LR: 9.34e-05
  Batch  400/750 ( 53.3%) | Loss: 0.9360 (avg: 0.9683) | Acc: 0.6719 (avg: 0.6454) | LR: 9.97e-05
  Batch  450/750 ( 60.0%) | Loss: 0.8762 (avg: 0.9536) | Acc: 0.6641 (avg: 0.6533) | LR: 1.06e-04
  Batch  500/750 ( 66.7%) | Loss: 0.8090 (avg: 0.9399) | Acc: 0.7109 (avg: 0.6607) | LR: 1.13e-04
  Batch  550/750 ( 73.3%) | Loss: 0.7721 (avg: 0.9275) | Acc: 0.7422 (avg: 0.6674) | LR: 1.20e-04
  Batch  600/750 ( 80.0%) | Loss: 0.7693 (avg: 0.9152) | Acc: 0.7578 (avg: 0.6738) | LR: 1.27e-04
  Batch  650/750 ( 86.7%) | Loss: 0.6758 (avg: 0.9041) | Acc: 0.7969 (avg: 0.6795) | LR: 1.34e-04
  Batch  700/750 ( 93.3%) | Loss: 0.7790 (avg: 0.8931) | Acc: 0.6875 (avg: 0.6849) | LR: 1.42e-04
  Batch  750/750 (100.0%) | Loss: 0.6401 (avg: 0.8824) | Acc: 0.8125 (avg: 0.6906) | LR: 1.50e-04

Train Summary:
  Final Loss: 0.8824 | Final Acc: 0.6906
  Final LR: 1.49e-04
  Loss std: 0.1200 | Acc std: 0.0704

Val Epoch 2 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.6478 | Final Acc: 0.8265
  Loss std: 0.0482 | Acc std: 0.0337
Epoch 02/20 | train 0.8824/0.6906 | val 0.6478/0.8265

Train Epoch 3 - 750 batches
--------------------------------------------------
  Batch   50/750 (  6.7%) | Loss: 0.6390 (avg: 0.6951) | Acc: 0.8125 (avg: 0.7827) | LR: 1.57e-04
  Batch  100/750 ( 13.3%) | Loss: 0.7564 (avg: 0.6949) | Acc: 0.7734 (avg: 0.7854) | LR: 1.65e-04
  Batch  150/750 ( 20.0%) | Loss: 0.5858 (avg: 0.6861) | Acc: 0.8203 (avg: 0.7912) | LR: 1.74e-04
  Batch  200/750 ( 26.7%) | Loss: 0.6568 (avg: 0.6816) | Acc: 0.8359 (avg: 0.7936) | LR: 1.82e-04
  Batch  250/750 ( 33.3%) | Loss: 0.6935 (avg: 0.6761) | Acc: 0.8125 (avg: 0.7979) | LR: 1.90e-04
  Batch  300/750 ( 40.0%) | Loss: 0.5512 (avg: 0.6705) | Acc: 0.8906 (avg: 0.7994) | LR: 1.99e-04
  Batch  350/750 ( 46.7%) | Loss: 0.6081 (avg: 0.6656) | Acc: 0.8203 (avg: 0.8022) | LR: 2.07e-04
  Batch  400/750 ( 53.3%) | Loss: 0.7231 (avg: 0.6611) | Acc: 0.7656 (avg: 0.8051) | LR: 2.16e-04
  Batch  450/750 ( 60.0%) | Loss: 0.6512 (avg: 0.6573) | Acc: 0.8125 (avg: 0.8070) | LR: 2.25e-04
  Batch  500/750 ( 66.7%) | Loss: 0.6744 (avg: 0.6532) | Acc: 0.8281 (avg: 0.8094) | LR: 2.33e-04
  Batch  550/750 ( 73.3%) | Loss: 0.6519 (avg: 0.6495) | Acc: 0.8203 (avg: 0.8114) | LR: 2.42e-04
  Batch  600/750 ( 80.0%) | Loss: 0.6453 (avg: 0.6474) | Acc: 0.8438 (avg: 0.8124) | LR: 2.51e-04
  Batch  650/750 ( 86.7%) | Loss: 0.6814 (avg: 0.6441) | Acc: 0.7812 (avg: 0.8142) | LR: 2.60e-04
  Batch  700/750 ( 93.3%) | Loss: 0.6204 (avg: 0.6412) | Acc: 0.7969 (avg: 0.8158) | LR: 2.69e-04
  Batch  750/750 (100.0%) | Loss: 0.6184 (avg: 0.6381) | Acc: 0.8438 (avg: 0.8173) | LR: 2.78e-04

Train Summary:
  Final Loss: 0.6381 | Final Acc: 0.8173
  Final LR: 2.78e-04
  Loss std: 0.0651 | Acc std: 0.0368

Val Epoch 3 - 188 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5394 | Final Acc: 0.8681
  Loss std: 0.0498 | Acc std: 0.0289
Epoch 03/20 | train 0.6381/0.8173 | val 0.5394/0.8681
Trial pruned at epoch 3
Trial 18 failed:
wandb:
wandb: Run history:
wandb:               batch ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb:          best_epoch ‚ñÅ‚ñà
wandb:        best_val_acc ‚ñÅ‚ñà
wandb:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: final_learning_rate ‚ñÅ‚ñÑ‚ñà
wandb:       learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:           train_acc ‚ñÅ‚ñÜ‚ñà
wandb:       train_acc_std ‚ñà‚ñÑ‚ñÅ
wandb:       train_avg_acc ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:      train_avg_loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                 +12 ...
wandb:
wandb: Run summary:
wandb:               batch 750
wandb:          best_epoch 2
wandb:        best_val_acc 0.82654
wandb:               epoch 3
wandb:               error
wandb: final_learning_rate 0.00028
wandb:       learning_rate 0.00028
wandb:              status failed
wandb:           train_acc 0.81729
wandb:       train_acc_std 0.03685
wandb:                 +14 ...
wandb:
wandb: üöÄ View run trial_018 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/fdl6rgfj
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_074601-fdl6rgfj/logs
[I 2025-10-06 07:49:39,834] Trial 18 pruned.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_074939-9saz9etx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-sunset-40
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/9saz9etx
wandb: Finishing previous runs because reinit is set to True.
wandb:
wandb: Run history:
wandb:   batch_size ‚ñÅ
wandb:     channels ‚ñÅ
wandb:      dropout ‚ñÅ
wandb:      emb_dim ‚ñÅ
wandb:    grad_clip ‚ñÅ
wandb:           lr ‚ñÅ
wandb:      max_len ‚ñÅ
wandb:      val_acc ‚ñÅ
wandb: weight_decay ‚ñÅ
wandb:
wandb: Run summary:
wandb:   batch_size 128
wandb:     channels 128
wandb:      dropout 0.4
wandb:      emb_dim 200
wandb:    grad_clip 1.5
wandb:           lr 0.00053
wandb:      max_len 200
wandb:    optimizer adam
wandb:      val_acc 0.86808
wandb: weight_decay 2e-05
wandb:
wandb: üöÄ View run trial/18/royal-sunset-40 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/9saz9etx
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_074939-9saz9etx/logs
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_074940-3q1u5i0w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trial_019
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/3q1u5i0w

Train Epoch 1 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 1.5512 (avg: 1.5423) | Acc: 0.2656 (avg: 0.2541) | LR: 9.92e-05
  Batch  100/1500 (  6.7%) | Loss: 1.5091 (avg: 1.5135) | Acc: 0.2500 (avg: 0.2703) | LR: 9.98e-05
  Batch  150/1500 ( 10.0%) | Loss: 1.4801 (avg: 1.4928) | Acc: 0.2969 (avg: 0.2780) | LR: 1.01e-04
  Batch  200/1500 ( 13.3%) | Loss: 1.3430 (avg: 1.4731) | Acc: 0.3594 (avg: 0.2882) | LR: 1.02e-04
  Batch  250/1500 ( 16.7%) | Loss: 1.2745 (avg: 1.4553) | Acc: 0.4062 (avg: 0.2996) | LR: 1.04e-04
  Batch  300/1500 ( 20.0%) | Loss: 1.2831 (avg: 1.4368) | Acc: 0.4062 (avg: 0.3103) | LR: 1.06e-04
  Batch  350/1500 ( 23.3%) | Loss: 1.2874 (avg: 1.4206) | Acc: 0.3750 (avg: 0.3203) | LR: 1.08e-04
  Batch  400/1500 ( 26.7%) | Loss: 1.2067 (avg: 1.4015) | Acc: 0.5156 (avg: 0.3340) | LR: 1.11e-04
  Batch  450/1500 ( 30.0%) | Loss: 1.1861 (avg: 1.3856) | Acc: 0.5781 (avg: 0.3457) | LR: 1.14e-04
  Batch  500/1500 ( 33.3%) | Loss: 1.1601 (avg: 1.3680) | Acc: 0.5156 (avg: 0.3595) | LR: 1.17e-04
  Batch  550/1500 ( 36.7%) | Loss: 1.0728 (avg: 1.3529) | Acc: 0.5938 (avg: 0.3716) | LR: 1.21e-04
  Batch  600/1500 ( 40.0%) | Loss: 1.1016 (avg: 1.3356) | Acc: 0.5625 (avg: 0.3847) | LR: 1.25e-04
  Batch  650/1500 ( 43.3%) | Loss: 1.0918 (avg: 1.3205) | Acc: 0.5000 (avg: 0.3955) | LR: 1.30e-04
  Batch  700/1500 ( 46.7%) | Loss: 1.0877 (avg: 1.3051) | Acc: 0.5938 (avg: 0.4073) | LR: 1.34e-04
  Batch  750/1500 ( 50.0%) | Loss: 0.9782 (avg: 1.2891) | Acc: 0.6406 (avg: 0.4192) | LR: 1.40e-04
  Batch  800/1500 ( 53.3%) | Loss: 1.1564 (avg: 1.2737) | Acc: 0.5312 (avg: 0.4303) | LR: 1.45e-04
  Batch  850/1500 ( 56.7%) | Loss: 0.9090 (avg: 1.2592) | Acc: 0.6719 (avg: 0.4403) | LR: 1.51e-04
  Batch  900/1500 ( 60.0%) | Loss: 0.9931 (avg: 1.2447) | Acc: 0.6094 (avg: 0.4504) | LR: 1.57e-04
  Batch  950/1500 ( 63.3%) | Loss: 1.0103 (avg: 1.2301) | Acc: 0.6406 (avg: 0.4602) | LR: 1.64e-04
  Batch 1000/1500 ( 66.7%) | Loss: 0.9335 (avg: 1.2162) | Acc: 0.6406 (avg: 0.4694) | LR: 1.71e-04
  Batch 1050/1500 ( 70.0%) | Loss: 0.9926 (avg: 1.2016) | Acc: 0.6406 (avg: 0.4791) | LR: 1.78e-04
  Batch 1100/1500 ( 73.3%) | Loss: 0.8308 (avg: 1.1875) | Acc: 0.7031 (avg: 0.4886) | LR: 1.86e-04
  Batch 1150/1500 ( 76.7%) | Loss: 0.9443 (avg: 1.1743) | Acc: 0.6406 (avg: 0.4972) | LR: 1.94e-04
  Batch 1200/1500 ( 80.0%) | Loss: 0.8061 (avg: 1.1612) | Acc: 0.7188 (avg: 0.5056) | LR: 2.02e-04
  Batch 1250/1500 ( 83.3%) | Loss: 0.7457 (avg: 1.1477) | Acc: 0.7500 (avg: 0.5140) | LR: 2.10e-04
  Batch 1300/1500 ( 86.7%) | Loss: 0.6731 (avg: 1.1355) | Acc: 0.7656 (avg: 0.5217) | LR: 2.19e-04
  Batch 1350/1500 ( 90.0%) | Loss: 0.9199 (avg: 1.1235) | Acc: 0.6875 (avg: 0.5293) | LR: 2.29e-04
  Batch 1400/1500 ( 93.3%) | Loss: 0.8606 (avg: 1.1125) | Acc: 0.6250 (avg: 0.5360) | LR: 2.38e-04
  Batch 1450/1500 ( 96.7%) | Loss: 0.8408 (avg: 1.1013) | Acc: 0.7344 (avg: 0.5427) | LR: 2.48e-04
  Batch 1500/1500 (100.0%) | Loss: 0.7275 (avg: 1.0900) | Acc: 0.7812 (avg: 0.5496) | LR: 2.58e-04

Train Summary:
  Final Loss: 1.0900 | Final Acc: 0.5496
  Final LR: 2.58e-04
  Loss std: 0.2428 | Acc std: 0.1644

Val Epoch 1 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.6710 | Final Acc: 0.8053
  Loss std: 0.0738 | Acc std: 0.0498
Epoch 01/20 | train 1.0900/0.5496 | val 0.6710/0.8053

Train Epoch 2 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.9747 (avg: 0.7499) | Acc: 0.6250 (avg: 0.7506) | LR: 2.69e-04
  Batch  100/1500 (  6.7%) | Loss: 0.7215 (avg: 0.7344) | Acc: 0.7656 (avg: 0.7633) | LR: 2.80e-04
  Batch  150/1500 ( 10.0%) | Loss: 0.8968 (avg: 0.7238) | Acc: 0.6406 (avg: 0.7676) | LR: 2.91e-04
  Batch  200/1500 ( 13.3%) | Loss: 0.6912 (avg: 0.7095) | Acc: 0.7812 (avg: 0.7737) | LR: 3.02e-04
  Batch  250/1500 ( 16.7%) | Loss: 0.7000 (avg: 0.7080) | Acc: 0.8281 (avg: 0.7746) | LR: 3.14e-04
  Batch  300/1500 ( 20.0%) | Loss: 0.7711 (avg: 0.7070) | Acc: 0.7656 (avg: 0.7752) | LR: 3.26e-04
  Batch  350/1500 ( 23.3%) | Loss: 0.6476 (avg: 0.7051) | Acc: 0.7969 (avg: 0.7771) | LR: 3.38e-04
  Batch  400/1500 ( 26.7%) | Loss: 0.6579 (avg: 0.7010) | Acc: 0.8125 (avg: 0.7801) | LR: 3.51e-04
  Batch  450/1500 ( 30.0%) | Loss: 0.6036 (avg: 0.7002) | Acc: 0.8281 (avg: 0.7806) | LR: 3.64e-04
  Batch  500/1500 ( 33.3%) | Loss: 0.7721 (avg: 0.6979) | Acc: 0.7969 (avg: 0.7827) | LR: 3.77e-04
  Batch  550/1500 ( 36.7%) | Loss: 0.5454 (avg: 0.6928) | Acc: 0.8281 (avg: 0.7851) | LR: 3.91e-04
  Batch  600/1500 ( 40.0%) | Loss: 0.5900 (avg: 0.6884) | Acc: 0.8438 (avg: 0.7873) | LR: 4.04e-04
  Batch  650/1500 ( 43.3%) | Loss: 0.7806 (avg: 0.6856) | Acc: 0.7656 (avg: 0.7891) | LR: 4.18e-04
  Batch  700/1500 ( 46.7%) | Loss: 0.6294 (avg: 0.6821) | Acc: 0.8438 (avg: 0.7916) | LR: 4.33e-04
  Batch  750/1500 ( 50.0%) | Loss: 0.7326 (avg: 0.6795) | Acc: 0.8125 (avg: 0.7933) | LR: 4.47e-04
  Batch  800/1500 ( 53.3%) | Loss: 0.6140 (avg: 0.6767) | Acc: 0.8438 (avg: 0.7951) | LR: 4.62e-04
  Batch  850/1500 ( 56.7%) | Loss: 0.6047 (avg: 0.6737) | Acc: 0.8906 (avg: 0.7966) | LR: 4.77e-04
  Batch  900/1500 ( 60.0%) | Loss: 0.6173 (avg: 0.6706) | Acc: 0.8281 (avg: 0.7987) | LR: 4.92e-04
  Batch  950/1500 ( 63.3%) | Loss: 0.7337 (avg: 0.6676) | Acc: 0.7969 (avg: 0.8004) | LR: 5.08e-04
  Batch 1000/1500 ( 66.7%) | Loss: 0.5114 (avg: 0.6651) | Acc: 0.8906 (avg: 0.8019) | LR: 5.24e-04
  Batch 1050/1500 ( 70.0%) | Loss: 0.6692 (avg: 0.6623) | Acc: 0.8125 (avg: 0.8033) | LR: 5.40e-04
  Batch 1100/1500 ( 73.3%) | Loss: 0.5685 (avg: 0.6586) | Acc: 0.8750 (avg: 0.8055) | LR: 5.56e-04
  Batch 1150/1500 ( 76.7%) | Loss: 0.6775 (avg: 0.6560) | Acc: 0.7969 (avg: 0.8068) | LR: 5.72e-04
  Batch 1200/1500 ( 80.0%) | Loss: 0.5803 (avg: 0.6530) | Acc: 0.8594 (avg: 0.8085) | LR: 5.89e-04
  Batch 1250/1500 ( 83.3%) | Loss: 0.6841 (avg: 0.6503) | Acc: 0.7812 (avg: 0.8097) | LR: 6.06e-04
  Batch 1300/1500 ( 86.7%) | Loss: 0.5033 (avg: 0.6488) | Acc: 0.8438 (avg: 0.8106) | LR: 6.23e-04
  Batch 1350/1500 ( 90.0%) | Loss: 0.6950 (avg: 0.6465) | Acc: 0.7969 (avg: 0.8120) | LR: 6.40e-04
  Batch 1400/1500 ( 93.3%) | Loss: 0.5590 (avg: 0.6442) | Acc: 0.8438 (avg: 0.8130) | LR: 6.58e-04
  Batch 1450/1500 ( 96.7%) | Loss: 0.6965 (avg: 0.6429) | Acc: 0.7500 (avg: 0.8137) | LR: 6.76e-04
  Batch 1500/1500 (100.0%) | Loss: 0.6251 (avg: 0.6407) | Acc: 0.8438 (avg: 0.8147) | LR: 6.94e-04

Train Summary:
  Final Loss: 0.6407 | Final Acc: 0.8147
  Final LR: 6.93e-04
  Loss std: 0.1003 | Acc std: 0.0556

Val Epoch 2 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5386 | Final Acc: 0.8657
  Loss std: 0.0746 | Acc std: 0.0418
Epoch 02/20 | train 0.6407/0.8147 | val 0.5386/0.8657

Train Epoch 3 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.5275 (avg: 0.5350) | Acc: 0.8438 (avg: 0.8709) | LR: 7.12e-04
  Batch  100/1500 (  6.7%) | Loss: 0.5246 (avg: 0.5350) | Acc: 0.8438 (avg: 0.8697) | LR: 7.30e-04
  Batch  150/1500 ( 10.0%) | Loss: 0.4945 (avg: 0.5392) | Acc: 0.8750 (avg: 0.8688) | LR: 7.48e-04
  Batch  200/1500 ( 13.3%) | Loss: 0.5646 (avg: 0.5397) | Acc: 0.8438 (avg: 0.8683) | LR: 7.67e-04
  Batch  250/1500 ( 16.7%) | Loss: 0.4311 (avg: 0.5371) | Acc: 0.9062 (avg: 0.8707) | LR: 7.86e-04
  Batch  300/1500 ( 20.0%) | Loss: 0.5398 (avg: 0.5348) | Acc: 0.8750 (avg: 0.8694) | LR: 8.04e-04
  Batch  350/1500 ( 23.3%) | Loss: 0.4644 (avg: 0.5339) | Acc: 0.8750 (avg: 0.8701) | LR: 8.23e-04
  Batch  400/1500 ( 26.7%) | Loss: 0.3851 (avg: 0.5334) | Acc: 0.9062 (avg: 0.8709) | LR: 8.43e-04
  Batch  450/1500 ( 30.0%) | Loss: 0.5027 (avg: 0.5333) | Acc: 0.8750 (avg: 0.8704) | LR: 8.62e-04
  Batch  500/1500 ( 33.3%) | Loss: 0.6270 (avg: 0.5347) | Acc: 0.7812 (avg: 0.8698) | LR: 8.81e-04
  Batch  550/1500 ( 36.7%) | Loss: 0.5393 (avg: 0.5336) | Acc: 0.8750 (avg: 0.8707) | LR: 9.01e-04
  Batch  600/1500 ( 40.0%) | Loss: 0.6313 (avg: 0.5327) | Acc: 0.7344 (avg: 0.8703) | LR: 9.21e-04
  Batch  650/1500 ( 43.3%) | Loss: 0.4048 (avg: 0.5322) | Acc: 0.9219 (avg: 0.8707) | LR: 9.40e-04
  Batch  700/1500 ( 46.7%) | Loss: 0.3952 (avg: 0.5312) | Acc: 0.9219 (avg: 0.8711) | LR: 9.60e-04
  Batch  750/1500 ( 50.0%) | Loss: 0.5849 (avg: 0.5324) | Acc: 0.9062 (avg: 0.8705) | LR: 9.80e-04
  Batch  800/1500 ( 53.3%) | Loss: 0.5586 (avg: 0.5327) | Acc: 0.8125 (avg: 0.8705) | LR: 1.00e-03
  Batch  850/1500 ( 56.7%) | Loss: 0.7269 (avg: 0.5324) | Acc: 0.8281 (avg: 0.8706) | LR: 1.02e-03
  Batch  900/1500 ( 60.0%) | Loss: 0.4040 (avg: 0.5324) | Acc: 0.9375 (avg: 0.8703) | LR: 1.04e-03
  Batch  950/1500 ( 63.3%) | Loss: 0.4614 (avg: 0.5315) | Acc: 0.9062 (avg: 0.8707) | LR: 1.06e-03
  Batch 1000/1500 ( 66.7%) | Loss: 0.5224 (avg: 0.5305) | Acc: 0.9375 (avg: 0.8711) | LR: 1.08e-03
  Batch 1050/1500 ( 70.0%) | Loss: 0.5181 (avg: 0.5293) | Acc: 0.8594 (avg: 0.8718) | LR: 1.10e-03
  Batch 1100/1500 ( 73.3%) | Loss: 0.5307 (avg: 0.5289) | Acc: 0.8438 (avg: 0.8721) | LR: 1.12e-03
  Batch 1150/1500 ( 76.7%) | Loss: 0.4071 (avg: 0.5275) | Acc: 0.9688 (avg: 0.8727) | LR: 1.14e-03
  Batch 1200/1500 ( 80.0%) | Loss: 0.3961 (avg: 0.5271) | Acc: 0.9062 (avg: 0.8729) | LR: 1.16e-03
  Batch 1250/1500 ( 83.3%) | Loss: 0.5284 (avg: 0.5262) | Acc: 0.8438 (avg: 0.8734) | LR: 1.18e-03
  Batch 1300/1500 ( 86.7%) | Loss: 0.4523 (avg: 0.5254) | Acc: 0.9219 (avg: 0.8740) | LR: 1.21e-03
  Batch 1350/1500 ( 90.0%) | Loss: 0.5943 (avg: 0.5243) | Acc: 0.8906 (avg: 0.8744) | LR: 1.23e-03
  Batch 1400/1500 ( 93.3%) | Loss: 0.5194 (avg: 0.5240) | Acc: 0.9062 (avg: 0.8747) | LR: 1.25e-03
  Batch 1450/1500 ( 96.7%) | Loss: 0.4025 (avg: 0.5235) | Acc: 0.8906 (avg: 0.8750) | LR: 1.27e-03
  Batch 1500/1500 (100.0%) | Loss: 0.4096 (avg: 0.5228) | Acc: 0.9375 (avg: 0.8752) | LR: 1.29e-03

Train Summary:
  Final Loss: 0.5228 | Final Acc: 0.8752
  Final LR: 1.29e-03
  Loss std: 0.0833 | Acc std: 0.0417

Val Epoch 3 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4668 | Final Acc: 0.9002
  Loss std: 0.0789 | Acc std: 0.0383
Epoch 03/20 | train 0.5228/0.8752 | val 0.4668/0.9002

Train Epoch 4 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.4771 (avg: 0.4599) | Acc: 0.9062 (avg: 0.9031) | LR: 1.31e-03
  Batch  100/1500 (  6.7%) | Loss: 0.6782 (avg: 0.4595) | Acc: 0.8438 (avg: 0.9053) | LR: 1.33e-03
  Batch  150/1500 ( 10.0%) | Loss: 0.4749 (avg: 0.4575) | Acc: 0.8750 (avg: 0.9055) | LR: 1.35e-03
  Batch  200/1500 ( 13.3%) | Loss: 0.5426 (avg: 0.4560) | Acc: 0.8750 (avg: 0.9062) | LR: 1.37e-03
  Batch  250/1500 ( 16.7%) | Loss: 0.3829 (avg: 0.4587) | Acc: 0.9375 (avg: 0.9057) | LR: 1.39e-03
  Batch  300/1500 ( 20.0%) | Loss: 0.4492 (avg: 0.4593) | Acc: 0.9219 (avg: 0.9052) | LR: 1.41e-03
  Batch  350/1500 ( 23.3%) | Loss: 0.3554 (avg: 0.4586) | Acc: 0.9688 (avg: 0.9050) | LR: 1.43e-03
  Batch  400/1500 ( 26.7%) | Loss: 0.4434 (avg: 0.4590) | Acc: 0.9219 (avg: 0.9048) | LR: 1.45e-03
  Batch  450/1500 ( 30.0%) | Loss: 0.5043 (avg: 0.4624) | Acc: 0.9062 (avg: 0.9032) | LR: 1.47e-03
  Batch  500/1500 ( 33.3%) | Loss: 0.5355 (avg: 0.4650) | Acc: 0.9219 (avg: 0.9027) | LR: 1.49e-03
  Batch  550/1500 ( 36.7%) | Loss: 0.3331 (avg: 0.4643) | Acc: 0.9531 (avg: 0.9026) | LR: 1.51e-03
  Batch  600/1500 ( 40.0%) | Loss: 0.3718 (avg: 0.4646) | Acc: 0.9219 (avg: 0.9020) | LR: 1.54e-03
  Batch  650/1500 ( 43.3%) | Loss: 0.5656 (avg: 0.4662) | Acc: 0.8594 (avg: 0.9012) | LR: 1.56e-03
  Batch  700/1500 ( 46.7%) | Loss: 0.4009 (avg: 0.4679) | Acc: 0.8906 (avg: 0.9006) | LR: 1.58e-03
  Batch  750/1500 ( 50.0%) | Loss: 0.3763 (avg: 0.4692) | Acc: 0.9375 (avg: 0.9001) | LR: 1.60e-03
  Batch  800/1500 ( 53.3%) | Loss: 0.4407 (avg: 0.4690) | Acc: 0.9062 (avg: 0.8997) | LR: 1.62e-03
  Batch  850/1500 ( 56.7%) | Loss: 0.3577 (avg: 0.4686) | Acc: 0.9688 (avg: 0.8999) | LR: 1.64e-03
  Batch  900/1500 ( 60.0%) | Loss: 0.5330 (avg: 0.4689) | Acc: 0.8594 (avg: 0.8998) | LR: 1.66e-03
  Batch  950/1500 ( 63.3%) | Loss: 0.3824 (avg: 0.4698) | Acc: 0.9688 (avg: 0.8997) | LR: 1.68e-03
  Batch 1000/1500 ( 66.7%) | Loss: 0.4890 (avg: 0.4696) | Acc: 0.8594 (avg: 0.8999) | LR: 1.69e-03
  Batch 1050/1500 ( 70.0%) | Loss: 0.4563 (avg: 0.4699) | Acc: 0.9062 (avg: 0.8997) | LR: 1.71e-03
  Batch 1100/1500 ( 73.3%) | Loss: 0.5507 (avg: 0.4705) | Acc: 0.8750 (avg: 0.8994) | LR: 1.73e-03
  Batch 1150/1500 ( 76.7%) | Loss: 0.4898 (avg: 0.4708) | Acc: 0.9062 (avg: 0.8993) | LR: 1.75e-03
  Batch 1200/1500 ( 80.0%) | Loss: 0.3251 (avg: 0.4710) | Acc: 0.9688 (avg: 0.8995) | LR: 1.77e-03
  Batch 1250/1500 ( 83.3%) | Loss: 0.5043 (avg: 0.4715) | Acc: 0.8594 (avg: 0.8990) | LR: 1.79e-03
  Batch 1300/1500 ( 86.7%) | Loss: 0.5847 (avg: 0.4712) | Acc: 0.8594 (avg: 0.8993) | LR: 1.81e-03
  Batch 1350/1500 ( 90.0%) | Loss: 0.4651 (avg: 0.4718) | Acc: 0.8750 (avg: 0.8990) | LR: 1.83e-03
  Batch 1400/1500 ( 93.3%) | Loss: 0.3786 (avg: 0.4710) | Acc: 0.9375 (avg: 0.8995) | LR: 1.85e-03
  Batch 1450/1500 ( 96.7%) | Loss: 0.6195 (avg: 0.4712) | Acc: 0.8750 (avg: 0.8993) | LR: 1.86e-03
  Batch 1500/1500 (100.0%) | Loss: 0.3728 (avg: 0.4712) | Acc: 0.9375 (avg: 0.8994) | LR: 1.88e-03

Train Summary:
  Final Loss: 0.4712 | Final Acc: 0.8994
  Final LR: 1.88e-03
  Loss std: 0.0769 | Acc std: 0.0375

Val Epoch 4 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4575 | Final Acc: 0.9079
  Loss std: 0.0818 | Acc std: 0.0368
Epoch 04/20 | train 0.4712/0.8994 | val 0.4575/0.9079

Train Epoch 5 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.3587 (avg: 0.4122) | Acc: 0.9688 (avg: 0.9275) | LR: 1.90e-03
  Batch  100/1500 (  6.7%) | Loss: 0.3887 (avg: 0.4085) | Acc: 0.9375 (avg: 0.9277) | LR: 1.92e-03
  Batch  150/1500 ( 10.0%) | Loss: 0.5190 (avg: 0.4092) | Acc: 0.8906 (avg: 0.9266) | LR: 1.94e-03
  Batch  200/1500 ( 13.3%) | Loss: 0.3203 (avg: 0.4099) | Acc: 0.9531 (avg: 0.9273) | LR: 1.95e-03
  Batch  250/1500 ( 16.7%) | Loss: 0.7273 (avg: 0.4126) | Acc: 0.8438 (avg: 0.9263) | LR: 1.97e-03
  Batch  300/1500 ( 20.0%) | Loss: 0.4580 (avg: 0.4157) | Acc: 0.9062 (avg: 0.9244) | LR: 1.99e-03
  Batch  350/1500 ( 23.3%) | Loss: 0.4650 (avg: 0.4190) | Acc: 0.9375 (avg: 0.9233) | LR: 2.00e-03
  Batch  400/1500 ( 26.7%) | Loss: 0.5281 (avg: 0.4233) | Acc: 0.8594 (avg: 0.9217) | LR: 2.02e-03
  Batch  450/1500 ( 30.0%) | Loss: 0.5148 (avg: 0.4245) | Acc: 0.8438 (avg: 0.9216) | LR: 2.04e-03
  Batch  500/1500 ( 33.3%) | Loss: 0.4398 (avg: 0.4254) | Acc: 0.8906 (avg: 0.9215) | LR: 2.05e-03
  Batch  550/1500 ( 36.7%) | Loss: 0.3581 (avg: 0.4266) | Acc: 0.9531 (avg: 0.9213) | LR: 2.07e-03
  Batch  600/1500 ( 40.0%) | Loss: 0.4527 (avg: 0.4298) | Acc: 0.9375 (avg: 0.9201) | LR: 2.08e-03
  Batch  650/1500 ( 43.3%) | Loss: 0.4437 (avg: 0.4310) | Acc: 0.9375 (avg: 0.9200) | LR: 2.10e-03
  Batch  700/1500 ( 46.7%) | Loss: 0.4063 (avg: 0.4326) | Acc: 0.9062 (avg: 0.9197) | LR: 2.11e-03
  Batch  750/1500 ( 50.0%) | Loss: 0.4588 (avg: 0.4327) | Acc: 0.8906 (avg: 0.9195) | LR: 2.13e-03
  Batch  800/1500 ( 53.3%) | Loss: 0.3912 (avg: 0.4336) | Acc: 0.9375 (avg: 0.9192) | LR: 2.14e-03
  Batch  850/1500 ( 56.7%) | Loss: 0.3422 (avg: 0.4341) | Acc: 0.9531 (avg: 0.9190) | LR: 2.16e-03
  Batch  900/1500 ( 60.0%) | Loss: 0.3518 (avg: 0.4358) | Acc: 0.9531 (avg: 0.9184) | LR: 2.17e-03
  Batch  950/1500 ( 63.3%) | Loss: 0.4302 (avg: 0.4377) | Acc: 0.8906 (avg: 0.9175) | LR: 2.19e-03
  Batch 1000/1500 ( 66.7%) | Loss: 0.4269 (avg: 0.4377) | Acc: 0.9219 (avg: 0.9175) | LR: 2.20e-03
  Batch 1050/1500 ( 70.0%) | Loss: 0.4530 (avg: 0.4390) | Acc: 0.8750 (avg: 0.9171) | LR: 2.21e-03
  Batch 1100/1500 ( 73.3%) | Loss: 0.4287 (avg: 0.4406) | Acc: 0.9531 (avg: 0.9166) | LR: 2.22e-03
  Batch 1150/1500 ( 76.7%) | Loss: 0.5321 (avg: 0.4410) | Acc: 0.8750 (avg: 0.9161) | LR: 2.24e-03
  Batch 1200/1500 ( 80.0%) | Loss: 0.4654 (avg: 0.4413) | Acc: 0.8906 (avg: 0.9159) | LR: 2.25e-03
  Batch 1250/1500 ( 83.3%) | Loss: 0.4656 (avg: 0.4422) | Acc: 0.8750 (avg: 0.9154) | LR: 2.26e-03
  Batch 1300/1500 ( 86.7%) | Loss: 0.4776 (avg: 0.4435) | Acc: 0.9219 (avg: 0.9148) | LR: 2.27e-03
  Batch 1350/1500 ( 90.0%) | Loss: 0.4249 (avg: 0.4440) | Acc: 0.8906 (avg: 0.9145) | LR: 2.28e-03
  Batch 1400/1500 ( 93.3%) | Loss: 0.6109 (avg: 0.4441) | Acc: 0.8906 (avg: 0.9142) | LR: 2.30e-03
  Batch 1450/1500 ( 96.7%) | Loss: 0.6509 (avg: 0.4452) | Acc: 0.8281 (avg: 0.9136) | LR: 2.31e-03
  Batch 1500/1500 (100.0%) | Loss: 0.5270 (avg: 0.4455) | Acc: 0.8750 (avg: 0.9134) | LR: 2.32e-03

Train Summary:
  Final Loss: 0.4455 | Final Acc: 0.9134
  Final LR: 2.32e-03
  Loss std: 0.0751 | Acc std: 0.0357

Val Epoch 5 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4668 | Final Acc: 0.9027
  Loss std: 0.0795 | Acc std: 0.0398
Epoch 05/20 | train 0.4455/0.9134 | val 0.4668/0.9027

Train Epoch 6 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.2897 (avg: 0.3805) | Acc: 1.0000 (avg: 0.9416) | LR: 2.33e-03
  Batch  100/1500 (  6.7%) | Loss: 0.4583 (avg: 0.3851) | Acc: 0.8750 (avg: 0.9384) | LR: 2.34e-03
  Batch  150/1500 ( 10.0%) | Loss: 0.3709 (avg: 0.3888) | Acc: 0.9531 (avg: 0.9363) | LR: 2.35e-03
  Batch  200/1500 ( 13.3%) | Loss: 0.4341 (avg: 0.3918) | Acc: 0.9062 (avg: 0.9363) | LR: 2.36e-03
  Batch  250/1500 ( 16.7%) | Loss: 0.4236 (avg: 0.3957) | Acc: 0.8906 (avg: 0.9339) | LR: 2.37e-03
  Batch  300/1500 ( 20.0%) | Loss: 0.3552 (avg: 0.3983) | Acc: 0.9531 (avg: 0.9331) | LR: 2.37e-03
  Batch  350/1500 ( 23.3%) | Loss: 0.4216 (avg: 0.3991) | Acc: 0.9531 (avg: 0.9333) | LR: 2.38e-03
  Batch  400/1500 ( 26.7%) | Loss: 0.3702 (avg: 0.3984) | Acc: 0.9375 (avg: 0.9337) | LR: 2.39e-03
  Batch  450/1500 ( 30.0%) | Loss: 0.3151 (avg: 0.3992) | Acc: 0.9688 (avg: 0.9333) | LR: 2.40e-03
  Batch  500/1500 ( 33.3%) | Loss: 0.4684 (avg: 0.4010) | Acc: 0.9062 (avg: 0.9329) | LR: 2.40e-03
  Batch  550/1500 ( 36.7%) | Loss: 0.3435 (avg: 0.4033) | Acc: 0.9531 (avg: 0.9320) | LR: 2.41e-03
  Batch  600/1500 ( 40.0%) | Loss: 0.3203 (avg: 0.4044) | Acc: 0.9688 (avg: 0.9309) | LR: 2.42e-03
  Batch  650/1500 ( 43.3%) | Loss: 0.3616 (avg: 0.4047) | Acc: 0.9375 (avg: 0.9303) | LR: 2.42e-03
  Batch  700/1500 ( 46.7%) | Loss: 0.3955 (avg: 0.4062) | Acc: 0.9531 (avg: 0.9298) | LR: 2.43e-03
  Batch  750/1500 ( 50.0%) | Loss: 0.4214 (avg: 0.4070) | Acc: 0.9062 (avg: 0.9296) | LR: 2.44e-03
  Batch  800/1500 ( 53.3%) | Loss: 0.4152 (avg: 0.4089) | Acc: 0.9062 (avg: 0.9286) | LR: 2.44e-03
  Batch  850/1500 ( 56.7%) | Loss: 0.3222 (avg: 0.4099) | Acc: 0.9688 (avg: 0.9283) | LR: 2.45e-03
  Batch  900/1500 ( 60.0%) | Loss: 0.4050 (avg: 0.4113) | Acc: 0.9375 (avg: 0.9277) | LR: 2.45e-03
  Batch  950/1500 ( 63.3%) | Loss: 0.3859 (avg: 0.4126) | Acc: 0.9375 (avg: 0.9272) | LR: 2.45e-03
  Batch 1000/1500 ( 66.7%) | Loss: 0.3773 (avg: 0.4143) | Acc: 0.9531 (avg: 0.9265) | LR: 2.46e-03
  Batch 1050/1500 ( 70.0%) | Loss: 0.4530 (avg: 0.4151) | Acc: 0.9219 (avg: 0.9263) | LR: 2.46e-03
  Batch 1100/1500 ( 73.3%) | Loss: 0.5368 (avg: 0.4166) | Acc: 0.9219 (avg: 0.9257) | LR: 2.46e-03
  Batch 1150/1500 ( 76.7%) | Loss: 0.4398 (avg: 0.4179) | Acc: 0.9062 (avg: 0.9253) | LR: 2.47e-03
  Batch 1200/1500 ( 80.0%) | Loss: 0.4140 (avg: 0.4187) | Acc: 0.9375 (avg: 0.9250) | LR: 2.47e-03
  Batch 1250/1500 ( 83.3%) | Loss: 0.4483 (avg: 0.4184) | Acc: 0.9062 (avg: 0.9249) | LR: 2.47e-03
  Batch 1300/1500 ( 86.7%) | Loss: 0.3594 (avg: 0.4195) | Acc: 0.9531 (avg: 0.9246) | LR: 2.47e-03
  Batch 1350/1500 ( 90.0%) | Loss: 0.3761 (avg: 0.4198) | Acc: 0.9219 (avg: 0.9244) | LR: 2.47e-03
  Batch 1400/1500 ( 93.3%) | Loss: 0.4771 (avg: 0.4199) | Acc: 0.8594 (avg: 0.9244) | LR: 2.48e-03
  Batch 1450/1500 ( 96.7%) | Loss: 0.4909 (avg: 0.4204) | Acc: 0.8906 (avg: 0.9242) | LR: 2.48e-03
  Batch 1500/1500 (100.0%) | Loss: 0.5254 (avg: 0.4210) | Acc: 0.8750 (avg: 0.9238) | LR: 2.48e-03

Train Summary:
  Final Loss: 0.4210 | Final Acc: 0.9238
  Final LR: 2.48e-03
  Loss std: 0.0685 | Acc std: 0.0338

Val Epoch 6 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4640 | Final Acc: 0.9026
  Loss std: 0.0837 | Acc std: 0.0383
Epoch 06/20 | train 0.4210/0.9238 | val 0.4640/0.9026

Train Epoch 7 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.2689 (avg: 0.3713) | Acc: 1.0000 (avg: 0.9450) | LR: 2.48e-03
  Batch  100/1500 (  6.7%) | Loss: 0.4438 (avg: 0.3672) | Acc: 0.9062 (avg: 0.9480) | LR: 2.48e-03
  Batch  150/1500 ( 10.0%) | Loss: 0.3649 (avg: 0.3686) | Acc: 0.9219 (avg: 0.9463) | LR: 2.48e-03
  Batch  200/1500 ( 13.3%) | Loss: 0.3085 (avg: 0.3680) | Acc: 0.9688 (avg: 0.9462) | LR: 2.48e-03
  Batch  250/1500 ( 16.7%) | Loss: 0.2872 (avg: 0.3679) | Acc: 0.9844 (avg: 0.9455) | LR: 2.48e-03
  Batch  300/1500 ( 20.0%) | Loss: 0.3023 (avg: 0.3680) | Acc: 0.9844 (avg: 0.9456) | LR: 2.48e-03
  Batch  350/1500 ( 23.3%) | Loss: 0.3008 (avg: 0.3679) | Acc: 0.9844 (avg: 0.9460) | LR: 2.47e-03
  Batch  400/1500 ( 26.7%) | Loss: 0.3487 (avg: 0.3693) | Acc: 0.9375 (avg: 0.9454) | LR: 2.47e-03
  Batch  450/1500 ( 30.0%) | Loss: 0.3388 (avg: 0.3697) | Acc: 0.9531 (avg: 0.9448) | LR: 2.47e-03
  Batch  500/1500 ( 33.3%) | Loss: 0.3788 (avg: 0.3699) | Acc: 0.9531 (avg: 0.9444) | LR: 2.47e-03
  Batch  550/1500 ( 36.7%) | Loss: 0.3595 (avg: 0.3708) | Acc: 0.9062 (avg: 0.9439) | LR: 2.47e-03
  Batch  600/1500 ( 40.0%) | Loss: 0.3303 (avg: 0.3722) | Acc: 0.9688 (avg: 0.9434) | LR: 2.47e-03
  Batch  650/1500 ( 43.3%) | Loss: 0.4340 (avg: 0.3731) | Acc: 0.8906 (avg: 0.9431) | LR: 2.47e-03
  Batch  700/1500 ( 46.7%) | Loss: 0.4672 (avg: 0.3752) | Acc: 0.8906 (avg: 0.9424) | LR: 2.47e-03
  Batch  750/1500 ( 50.0%) | Loss: 0.3236 (avg: 0.3759) | Acc: 0.9688 (avg: 0.9421) | LR: 2.47e-03
  Batch  800/1500 ( 53.3%) | Loss: 0.3162 (avg: 0.3766) | Acc: 0.9688 (avg: 0.9420) | LR: 2.47e-03
  Batch  850/1500 ( 56.7%) | Loss: 0.4226 (avg: 0.3783) | Acc: 0.9219 (avg: 0.9413) | LR: 2.47e-03
  Batch  900/1500 ( 60.0%) | Loss: 0.4168 (avg: 0.3797) | Acc: 0.9531 (avg: 0.9407) | LR: 2.47e-03
  Batch  950/1500 ( 63.3%) | Loss: 0.3363 (avg: 0.3812) | Acc: 0.9375 (avg: 0.9399) | LR: 2.46e-03
  Batch 1000/1500 ( 66.7%) | Loss: 0.3239 (avg: 0.3816) | Acc: 0.9844 (avg: 0.9398) | LR: 2.46e-03
  Batch 1050/1500 ( 70.0%) | Loss: 0.5125 (avg: 0.3834) | Acc: 0.9219 (avg: 0.9389) | LR: 2.46e-03
  Batch 1100/1500 ( 73.3%) | Loss: 0.4669 (avg: 0.3842) | Acc: 0.9219 (avg: 0.9389) | LR: 2.46e-03
  Batch 1150/1500 ( 76.7%) | Loss: 0.3597 (avg: 0.3860) | Acc: 0.9375 (avg: 0.9379) | LR: 2.46e-03
  Batch 1200/1500 ( 80.0%) | Loss: 0.5404 (avg: 0.3873) | Acc: 0.9375 (avg: 0.9374) | LR: 2.46e-03
  Batch 1250/1500 ( 83.3%) | Loss: 0.4154 (avg: 0.3885) | Acc: 0.9375 (avg: 0.9368) | LR: 2.45e-03
  Batch 1300/1500 ( 86.7%) | Loss: 0.3533 (avg: 0.3887) | Acc: 0.9688 (avg: 0.9367) | LR: 2.45e-03
  Batch 1350/1500 ( 90.0%) | Loss: 0.3674 (avg: 0.3897) | Acc: 0.9219 (avg: 0.9362) | LR: 2.45e-03
  Batch 1400/1500 ( 93.3%) | Loss: 0.4900 (avg: 0.3911) | Acc: 0.8906 (avg: 0.9357) | LR: 2.45e-03
  Batch 1450/1500 ( 96.7%) | Loss: 0.3324 (avg: 0.3919) | Acc: 0.9688 (avg: 0.9355) | LR: 2.45e-03
  Batch 1500/1500 (100.0%) | Loss: 0.4042 (avg: 0.3928) | Acc: 0.8906 (avg: 0.9349) | LR: 2.45e-03

Train Summary:
  Final Loss: 0.3928 | Final Acc: 0.9349
  Final LR: 2.45e-03
  Loss std: 0.0626 | Acc std: 0.0320

Val Epoch 7 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4483 | Final Acc: 0.9136
  Loss std: 0.0835 | Acc std: 0.0368
Epoch 07/20 | train 0.3928/0.9349 | val 0.4483/0.9136

Train Epoch 8 - 1500 batches
--------------------------------------------------
  Batch   50/1500 (  3.3%) | Loss: 0.4438 (avg: 0.3211) | Acc: 0.9219 (avg: 0.9694) | LR: 2.44e-03
  Batch  100/1500 (  6.7%) | Loss: 0.3238 (avg: 0.3291) | Acc: 0.9688 (avg: 0.9648) | LR: 2.44e-03
  Batch  150/1500 ( 10.0%) | Loss: 0.2922 (avg: 0.3325) | Acc: 1.0000 (avg: 0.9625) | LR: 2.44e-03
  Batch  200/1500 ( 13.3%) | Loss: 0.3747 (avg: 0.3348) | Acc: 0.9219 (avg: 0.9606) | LR: 2.44e-03
  Batch  250/1500 ( 16.7%) | Loss: 0.3792 (avg: 0.3360) | Acc: 0.9062 (avg: 0.9597) | LR: 2.43e-03
  Batch  300/1500 ( 20.0%) | Loss: 0.3317 (avg: 0.3381) | Acc: 0.9688 (avg: 0.9587) | LR: 2.43e-03
  Batch  350/1500 ( 23.3%) | Loss: 0.3768 (avg: 0.3394) | Acc: 0.9375 (avg: 0.9579) | LR: 2.43e-03
  Batch  400/1500 ( 26.7%) | Loss: 0.3190 (avg: 0.3415) | Acc: 0.9844 (avg: 0.9575) | LR: 2.43e-03
  Batch  450/1500 ( 30.0%) | Loss: 0.4442 (avg: 0.3431) | Acc: 0.9062 (avg: 0.9570) | LR: 2.42e-03
  Batch  500/1500 ( 33.3%) | Loss: 0.3607 (avg: 0.3433) | Acc: 0.9375 (avg: 0.9571) | LR: 2.42e-03
  Batch  550/1500 ( 36.7%) | Loss: 0.4219 (avg: 0.3443) | Acc: 0.8906 (avg: 0.9560) | LR: 2.42e-03
  Batch  600/1500 ( 40.0%) | Loss: 0.3472 (avg: 0.3444) | Acc: 0.9375 (avg: 0.9560) | LR: 2.42e-03
  Batch  650/1500 ( 43.3%) | Loss: 0.4175 (avg: 0.3459) | Acc: 0.9219 (avg: 0.9554) | LR: 2.41e-03
  Batch  700/1500 ( 46.7%) | Loss: 0.3920 (avg: 0.3462) | Acc: 0.9219 (avg: 0.9555) | LR: 2.41e-03
  Batch  750/1500 ( 50.0%) | Loss: 0.3857 (avg: 0.3475) | Acc: 0.9219 (avg: 0.9553) | LR: 2.41e-03
  Batch  800/1500 ( 53.3%) | Loss: 0.3257 (avg: 0.3489) | Acc: 0.9688 (avg: 0.9547) | LR: 2.40e-03
  Batch  850/1500 ( 56.7%) | Loss: 0.3717 (avg: 0.3494) | Acc: 0.9531 (avg: 0.9545) | LR: 2.40e-03
  Batch  900/1500 ( 60.0%) | Loss: 0.4477 (avg: 0.3507) | Acc: 0.9531 (avg: 0.9537) | LR: 2.40e-03
  Batch  950/1500 ( 63.3%) | Loss: 0.3562 (avg: 0.3519) | Acc: 0.9531 (avg: 0.9531) | LR: 2.39e-03
  Batch 1000/1500 ( 66.7%) | Loss: 0.4042 (avg: 0.3536) | Acc: 0.9062 (avg: 0.9522) | LR: 2.39e-03
  Batch 1050/1500 ( 70.0%) | Loss: 0.4732 (avg: 0.3547) | Acc: 0.9062 (avg: 0.9518) | LR: 2.39e-03
  Batch 1100/1500 ( 73.3%) | Loss: 0.4270 (avg: 0.3551) | Acc: 0.9062 (avg: 0.9514) | LR: 2.38e-03
  Batch 1150/1500 ( 76.7%) | Loss: 0.4637 (avg: 0.3563) | Acc: 0.8906 (avg: 0.9506) | LR: 2.38e-03
  Batch 1200/1500 ( 80.0%) | Loss: 0.3730 (avg: 0.3570) | Acc: 0.9375 (avg: 0.9502) | LR: 2.38e-03
  Batch 1250/1500 ( 83.3%) | Loss: 0.5048 (avg: 0.3583) | Acc: 0.8906 (avg: 0.9497) | LR: 2.37e-03
  Batch 1300/1500 ( 86.7%) | Loss: 0.5558 (avg: 0.3595) | Acc: 0.8438 (avg: 0.9493) | LR: 2.37e-03
  Batch 1350/1500 ( 90.0%) | Loss: 0.4262 (avg: 0.3609) | Acc: 0.8750 (avg: 0.9487) | LR: 2.37e-03
  Batch 1400/1500 ( 93.3%) | Loss: 0.2807 (avg: 0.3619) | Acc: 1.0000 (avg: 0.9484) | LR: 2.36e-03
  Batch 1450/1500 ( 96.7%) | Loss: 0.5024 (avg: 0.3628) | Acc: 0.9375 (avg: 0.9480) | LR: 2.36e-03
  Batch 1500/1500 (100.0%) | Loss: 0.3395 (avg: 0.3633) | Acc: 0.9531 (avg: 0.9478) | LR: 2.35e-03

Train Summary:
  Final Loss: 0.3633 | Final Acc: 0.9478
  Final LR: 2.35e-03
  Loss std: 0.0537 | Acc std: 0.0286

Val Epoch 8 - 375 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4415 | Final Acc: 0.9151
  Loss std: 0.0836 | Acc std: 0.0357
Epoch 08/20 | train 0.3633/0.9478 | val 0.4415/0.9151
Trial pruned at epoch 8
Trial 19 failed:
wandb:
wandb: Run history:
wandb:               batch ‚ñÇ‚ñÑ‚ñá‚ñà‚ñà‚ñÇ‚ñÉ‚ñÉ‚ñà‚ñÉ‚ñÜ‚ñÜ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÅ‚ñÖ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñÉ‚ñÑ‚ñÜ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñá
wandb:          best_epoch ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñà
wandb:        best_val_acc ‚ñÅ‚ñÖ‚ñá‚ñà‚ñà
wandb:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb: final_learning_rate ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñà
wandb:       learning_rate ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:           train_acc ‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:       train_acc_std ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:       train_avg_acc ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:      train_avg_loss ‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                 +12 ...
wandb:
wandb: Run summary:
wandb:               batch 1500
wandb:          best_epoch 7
wandb:        best_val_acc 0.91358
wandb:               epoch 8
wandb:               error
wandb: final_learning_rate 0.00235
wandb:       learning_rate 0.00235
wandb:              status failed
wandb:           train_acc 0.94783
wandb:       train_acc_std 0.02864
wandb:                 +14 ...
wandb:
wandb: üöÄ View run trial_019 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/3q1u5i0w
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_074940-3q1u5i0w/logs
[I 2025-10-06 07:59:22,950] Trial 19 pruned.
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_075922-5wiuumb9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run upbeat-sound-42
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/5wiuumb9

Optimization completed!
Best trial: 3
Best validation accuracy: 0.9231
Best parameters: {'emb_dim': 200, 'channels': 256, 'kernel_sizes': (2, 3, 4, 5), 'dropout': 0.5, 'lr': 0.001982169925018568, 'weight_decay': 1.9378094205564048e-05, 'batch_size': 256, 'max_len': 200, 'grad_clip': 1.5, 'optimizer': 'adam'}

Training final model with best parameters...
wandb:
wandb: Run history:
wandb:         batch_size ‚ñÅ
wandb:  best_trial_number ‚ñÅ
wandb:  best_val_accuracy ‚ñÅ
wandb:           channels ‚ñÅ
wandb:            dropout ‚ñÅ
wandb:            emb_dim ‚ñÅ
wandb:          grad_clip ‚ñÅ
wandb:                 lr ‚ñÅ
wandb:            max_len ‚ñÅ
wandb: n_completed_trials ‚ñÅ
wandb:                 +4 ...
wandb:
wandb: Run summary:
wandb:         batch_size 64
wandb:  best_trial_number 3
wandb:  best_val_accuracy 0.92312
wandb:           channels 192
wandb:            dropout 0.5
wandb:            emb_dim 100
wandb:          grad_clip 1
wandb:                 lr 0.00248
wandb:            max_len 200
wandb: n_completed_trials 6
wandb:                 +5 ...
wandb:
wandb: üöÄ View run trial/19/upbeat-sound-42 at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/5wiuumb9
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_075922-5wiuumb9/logs
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /Users/mark/dev/cu-stuff/ag-news-classification/wandb/run-20251006_075924-wfikql5t
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run final_model
wandb: ‚≠êÔ∏è View project at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: üöÄ View run at https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/wfikql5t

Train Epoch 1 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 1.3992 (avg: 1.4881) | Acc: 0.3164 (avg: 0.2845) | LR: 8.34e-05
  Batch  100/375 ( 26.7%) | Loss: 1.2985 (avg: 1.4231) | Acc: 0.4141 (avg: 0.3288) | LR: 9.57e-05
  Batch  150/375 ( 40.0%) | Loss: 1.1592 (avg: 1.3627) | Acc: 0.5156 (avg: 0.3714) | LR: 1.16e-04
  Batch  200/375 ( 53.3%) | Loss: 1.0534 (avg: 1.3007) | Acc: 0.5898 (avg: 0.4145) | LR: 1.45e-04
  Batch  250/375 ( 66.7%) | Loss: 0.9386 (avg: 1.2428) | Acc: 0.6758 (avg: 0.4535) | LR: 1.81e-04
  Batch  300/375 ( 80.0%) | Loss: 0.8139 (avg: 1.1879) | Acc: 0.7070 (avg: 0.4899) | LR: 2.24e-04
  Batch  350/375 ( 93.3%) | Loss: 0.7889 (avg: 1.1386) | Acc: 0.6914 (avg: 0.5212) | LR: 2.74e-04

Train Summary:
  Final Loss: 1.1156 | Final Acc: 0.5350
  Final LR: 3.01e-04
  Loss std: 0.2339 | Acc std: 0.1539

Val Epoch 1 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.6727 | Final Acc: 0.8152
  Loss std: 0.0335 | Acc std: 0.0207
Epoch 01/15 | train 1.1156/0.5350 | val 0.6727/0.8152

Train Epoch 2 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.6296 (avg: 0.7214) | Acc: 0.8164 (avg: 0.7698) | LR: 3.62e-04
  Batch  100/375 ( 26.7%) | Loss: 0.6959 (avg: 0.7055) | Acc: 0.7773 (avg: 0.7771) | LR: 4.28e-04
  Batch  150/375 ( 40.0%) | Loss: 0.5774 (avg: 0.6916) | Acc: 0.8359 (avg: 0.7853) | LR: 4.99e-04
  Batch  200/375 ( 53.3%) | Loss: 0.5872 (avg: 0.6803) | Acc: 0.8477 (avg: 0.7919) | LR: 5.75e-04
  Batch  250/375 ( 66.7%) | Loss: 0.5933 (avg: 0.6710) | Acc: 0.8633 (avg: 0.7972) | LR: 6.54e-04
  Batch  300/375 ( 80.0%) | Loss: 0.6310 (avg: 0.6618) | Acc: 0.8164 (avg: 0.8025) | LR: 7.37e-04
  Batch  350/375 ( 93.3%) | Loss: 0.5718 (avg: 0.6531) | Acc: 0.8672 (avg: 0.8068) | LR: 8.23e-04

Train Summary:
  Final Loss: 0.6497 | Final Acc: 0.8085
  Final LR: 8.65e-04
  Loss std: 0.0603 | Acc std: 0.0334

Val Epoch 2 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.5273 | Final Acc: 0.8738
  Loss std: 0.0354 | Acc std: 0.0192
Epoch 02/15 | train 0.6497/0.8085 | val 0.5273/0.8738

Train Epoch 3 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.4867 (avg: 0.5488) | Acc: 0.8750 (avg: 0.8614) | LR: 9.54e-04
  Batch  100/375 ( 26.7%) | Loss: 0.5422 (avg: 0.5490) | Acc: 0.8867 (avg: 0.8611) | LR: 1.04e-03
  Batch  150/375 ( 40.0%) | Loss: 0.5820 (avg: 0.5463) | Acc: 0.8477 (avg: 0.8633) | LR: 1.13e-03
  Batch  200/375 ( 53.3%) | Loss: 0.5945 (avg: 0.5504) | Acc: 0.8633 (avg: 0.8609) | LR: 1.22e-03
  Batch  250/375 ( 66.7%) | Loss: 0.5578 (avg: 0.5504) | Acc: 0.8516 (avg: 0.8607) | LR: 1.30e-03
  Batch  300/375 ( 80.0%) | Loss: 0.5189 (avg: 0.5488) | Acc: 0.8828 (avg: 0.8613) | LR: 1.39e-03
  Batch  350/375 ( 93.3%) | Loss: 0.4687 (avg: 0.5465) | Acc: 0.8867 (avg: 0.8623) | LR: 1.47e-03

Train Summary:
  Final Loss: 0.5461 | Final Acc: 0.8626
  Final LR: 1.51e-03
  Loss std: 0.0414 | Acc std: 0.0229

Val Epoch 3 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4972 | Final Acc: 0.8862
  Loss std: 0.0365 | Acc std: 0.0185
Epoch 03/15 | train 0.5461/0.8626 | val 0.4972/0.8862

Train Epoch 4 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.4480 (avg: 0.4684) | Acc: 0.9023 (avg: 0.8969) | LR: 1.58e-03
  Batch  100/375 ( 26.7%) | Loss: 0.4325 (avg: 0.4811) | Acc: 0.8945 (avg: 0.8891) | LR: 1.65e-03
  Batch  150/375 ( 40.0%) | Loss: 0.4887 (avg: 0.4818) | Acc: 0.8711 (avg: 0.8901) | LR: 1.72e-03
  Batch  200/375 ( 53.3%) | Loss: 0.4829 (avg: 0.4810) | Acc: 0.8594 (avg: 0.8906) | LR: 1.77e-03
  Batch  250/375 ( 66.7%) | Loss: 0.5880 (avg: 0.4833) | Acc: 0.8633 (avg: 0.8903) | LR: 1.83e-03
  Batch  300/375 ( 80.0%) | Loss: 0.4415 (avg: 0.4831) | Acc: 0.8945 (avg: 0.8906) | LR: 1.87e-03
  Batch  350/375 ( 93.3%) | Loss: 0.4685 (avg: 0.4842) | Acc: 0.9102 (avg: 0.8906) | LR: 1.91e-03

Train Summary:
  Final Loss: 0.4834 | Final Acc: 0.8909
  Final LR: 1.92e-03
  Loss std: 0.0397 | Acc std: 0.0204

Val Epoch 4 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4526 | Final Acc: 0.9062
  Loss std: 0.0368 | Acc std: 0.0184
Epoch 04/15 | train 0.4834/0.8909 | val 0.4526/0.9062

Train Epoch 5 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.3866 (avg: 0.4038) | Acc: 0.9414 (avg: 0.9287) | LR: 1.95e-03
  Batch  100/375 ( 26.7%) | Loss: 0.3700 (avg: 0.4109) | Acc: 0.9258 (avg: 0.9247) | LR: 1.97e-03
  Batch  150/375 ( 40.0%) | Loss: 0.4120 (avg: 0.4127) | Acc: 0.9062 (avg: 0.9233) | LR: 1.98e-03
  Batch  200/375 ( 53.3%) | Loss: 0.4528 (avg: 0.4153) | Acc: 0.8867 (avg: 0.9219) | LR: 1.98e-03
  Batch  250/375 ( 66.7%) | Loss: 0.4291 (avg: 0.4175) | Acc: 0.9062 (avg: 0.9206) | LR: 1.98e-03
  Batch  300/375 ( 80.0%) | Loss: 0.4244 (avg: 0.4182) | Acc: 0.9297 (avg: 0.9205) | LR: 1.98e-03
  Batch  350/375 ( 93.3%) | Loss: 0.4191 (avg: 0.4192) | Acc: 0.9180 (avg: 0.9197) | LR: 1.97e-03

Train Summary:
  Final Loss: 0.4195 | Final Acc: 0.9195
  Final LR: 1.97e-03
  Loss std: 0.0321 | Acc std: 0.0180

Val Epoch 5 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4317 | Final Acc: 0.9127
  Loss std: 0.0359 | Acc std: 0.0201
Epoch 05/15 | train 0.4195/0.9195 | val 0.4317/0.9127

Train Epoch 6 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.3555 (avg: 0.3581) | Acc: 0.9414 (avg: 0.9486) | LR: 1.96e-03
  Batch  100/375 ( 26.7%) | Loss: 0.3650 (avg: 0.3616) | Acc: 0.9492 (avg: 0.9468) | LR: 1.96e-03
  Batch  150/375 ( 40.0%) | Loss: 0.3753 (avg: 0.3649) | Acc: 0.9336 (avg: 0.9452) | LR: 1.95e-03
  Batch  200/375 ( 53.3%) | Loss: 0.3185 (avg: 0.3678) | Acc: 0.9648 (avg: 0.9435) | LR: 1.93e-03
  Batch  250/375 ( 66.7%) | Loss: 0.4296 (avg: 0.3711) | Acc: 0.9102 (avg: 0.9417) | LR: 1.92e-03
  Batch  300/375 ( 80.0%) | Loss: 0.3953 (avg: 0.3751) | Acc: 0.9414 (avg: 0.9398) | LR: 1.91e-03
  Batch  350/375 ( 93.3%) | Loss: 0.3788 (avg: 0.3767) | Acc: 0.9414 (avg: 0.9387) | LR: 1.89e-03

Train Summary:
  Final Loss: 0.3782 | Final Acc: 0.9381
  Final LR: 1.88e-03
  Loss std: 0.0304 | Acc std: 0.0172

Val Epoch 6 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4276 | Final Acc: 0.9146
  Loss std: 0.0383 | Acc std: 0.0190
Epoch 06/15 | train 0.3782/0.9381 | val 0.4276/0.9146

Train Epoch 7 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.3194 (avg: 0.3297) | Acc: 0.9570 (avg: 0.9602) | LR: 1.87e-03
  Batch  100/375 ( 26.7%) | Loss: 0.3335 (avg: 0.3334) | Acc: 0.9453 (avg: 0.9585) | LR: 1.85e-03
  Batch  150/375 ( 40.0%) | Loss: 0.3366 (avg: 0.3376) | Acc: 0.9531 (avg: 0.9565) | LR: 1.83e-03
  Batch  200/375 ( 53.3%) | Loss: 0.3717 (avg: 0.3420) | Acc: 0.9375 (avg: 0.9546) | LR: 1.80e-03
  Batch  250/375 ( 66.7%) | Loss: 0.3699 (avg: 0.3452) | Acc: 0.9453 (avg: 0.9533) | LR: 1.78e-03
  Batch  300/375 ( 80.0%) | Loss: 0.3406 (avg: 0.3484) | Acc: 0.9570 (avg: 0.9519) | LR: 1.76e-03
  Batch  350/375 ( 93.3%) | Loss: 0.3665 (avg: 0.3521) | Acc: 0.9492 (avg: 0.9499) | LR: 1.73e-03

Train Summary:
  Final Loss: 0.3536 | Final Acc: 0.9493
  Final LR: 1.72e-03
  Loss std: 0.0270 | Acc std: 0.0150

Val Epoch 7 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4244 | Final Acc: 0.9177
  Loss std: 0.0385 | Acc std: 0.0176
Epoch 07/15 | train 0.3536/0.9493 | val 0.4244/0.9177

Train Epoch 8 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.3200 (avg: 0.3153) | Acc: 0.9648 (avg: 0.9671) | LR: 1.69e-03
  Batch  100/375 ( 26.7%) | Loss: 0.2897 (avg: 0.3153) | Acc: 0.9922 (avg: 0.9680) | LR: 1.66e-03
  Batch  150/375 ( 40.0%) | Loss: 0.3059 (avg: 0.3189) | Acc: 0.9648 (avg: 0.9657) | LR: 1.63e-03
  Batch  200/375 ( 53.3%) | Loss: 0.3317 (avg: 0.3217) | Acc: 0.9492 (avg: 0.9640) | LR: 1.60e-03
  Batch  250/375 ( 66.7%) | Loss: 0.3680 (avg: 0.3241) | Acc: 0.9258 (avg: 0.9630) | LR: 1.57e-03
   Batch  300/375 ( 80.0%) | Loss: 0.3811 (avg: 0.3271) | Acc: 0.9336 (avg: 0.9618) | LR: 1.54e-03
  Batch  350/375 ( 93.3%) | Loss: 0.3411 (avg: 0.3296) | Acc: 0.9453 (avg: 0.9605) | LR: 1.50e-03

Train Summary:
  Final Loss: 0.3307 | Final Acc: 0.9599
  Final LR: 1.49e-03
  Loss std: 0.0218 | Acc std: 0.0138

Val Epoch 8 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4252 | Final Acc: 0.9178
  Loss std: 0.0386 | Acc std: 0.0187
Epoch 08/15 | train 0.3307/0.9599 | val 0.4252/0.9178

Train Epoch 9 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.3018 (avg: 0.2930) | Acc: 0.9844 (avg: 0.9785) | LR: 1.45e-03
  Batch  100/375 ( 26.7%) | Loss: 0.3287 (avg: 0.2952) | Acc: 0.9648 (avg: 0.9769) | LR: 1.42e-03
  Batch  150/375 ( 40.0%) | Loss: 0.3065 (avg: 0.2963) | Acc: 0.9648 (avg: 0.9758) | LR: 1.38e-03
  Batch  200/375 ( 53.3%) | Loss: 0.3416 (avg: 0.3000) | Acc: 0.9570 (avg: 0.9738) | LR: 1.34e-03
  Batch  250/375 ( 66.7%) | Loss: 0.3326 (avg: 0.3029) | Acc: 0.9609 (avg: 0.9727) | LR: 1.31e-03
  Batch  300/375 ( 80.0%) | Loss: 0.3211 (avg: 0.3056) | Acc: 0.9648 (avg: 0.9716) | LR: 1.27e-03
  Batch  350/375 ( 93.3%) | Loss: 0.3198 (avg: 0.3074) | Acc: 0.9609 (avg: 0.9708) | LR: 1.23e-03

Train Summary:
  Final Loss: 0.3085 | Final Acc: 0.9704
  Final LR: 1.21e-03
  Loss std: 0.0192 | Acc std: 0.0120

Val Epoch 9 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4224 | Final Acc: 0.9166
  Loss std: 0.0403 | Acc std: 0.0182
Epoch 09/15 | train 0.3085/0.9704 | val 0.4224/0.9166

Train Epoch 10 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.2658 (avg: 0.2729) | Acc: 0.9922 (avg: 0.9877) | LR: 1.17e-03
  Batch  100/375 ( 26.7%) | Loss: 0.2715 (avg: 0.2740) | Acc: 0.9922 (avg: 0.9868) | LR: 1.13e-03
  Batch  150/375 ( 40.0%) | Loss: 0.2876 (avg: 0.2773) | Acc: 0.9883 (avg: 0.9854) | LR: 1.09e-03
  Batch  200/375 ( 53.3%) | Loss: 0.2813 (avg: 0.2793) | Acc: 0.9883 (avg: 0.9844) | LR: 1.05e-03
  Batch  250/375 ( 66.7%) | Loss: 0.2851 (avg: 0.2815) | Acc: 0.9766 (avg: 0.9832) | LR: 1.02e-03
  Batch  300/375 ( 80.0%) | Loss: 0.2666 (avg: 0.2832) | Acc: 0.9922 (avg: 0.9823) | LR: 9.75e-04
  Batch  350/375 ( 93.3%) | Loss: 0.3000 (avg: 0.2852) | Acc: 0.9727 (avg: 0.9814) | LR: 9.36e-04

Train Summary:
  Final Loss: 0.2863 | Final Acc: 0.9807
  Final LR: 9.17e-04
  Loss std: 0.0155 | Acc std: 0.0094

Val Epoch 10 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4193 | Final Acc: 0.9198
  Loss std: 0.0397 | Acc std: 0.0173
Epoch 10/15 | train 0.2863/0.9807 | val 0.4193/0.9198

Train Epoch 11 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.2440 (avg: 0.2617) | Acc: 1.0000 (avg: 0.9917) | LR: 8.77e-04
  Batch  100/375 ( 26.7%) | Loss: 0.2677 (avg: 0.2631) | Acc: 0.9922 (avg: 0.9913) | LR: 8.38e-04
  Batch  150/375 ( 40.0%) | Loss: 0.2558 (avg: 0.2639) | Acc: 0.9961 (avg: 0.9910) | LR: 7.99e-04
  Batch  200/375 ( 53.3%) | Loss: 0.2590 (avg: 0.2652) | Acc: 0.9922 (avg: 0.9904) | LR: 7.60e-04
  Batch  250/375 ( 66.7%) | Loss: 0.2728 (avg: 0.2661) | Acc: 0.9922 (avg: 0.9899) | LR: 7.22e-04
  Batch  300/375 ( 80.0%) | Loss: 0.2544 (avg: 0.2672) | Acc: 0.9883 (avg: 0.9894) | LR: 6.84e-04
  Batch  350/375 ( 93.3%) | Loss: 0.2751 (avg: 0.2684) | Acc: 0.9844 (avg: 0.9889) | LR: 6.47e-04

Train Summary:
  Final Loss: 0.2686 | Final Acc: 0.9888
  Final LR: 6.29e-04
  Loss std: 0.0103 | Acc std: 0.0067

Val Epoch 11 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4178 | Final Acc: 0.9208
  Loss std: 0.0411 | Acc std: 0.0173
Epoch 11/15 | train 0.2686/0.9888 | val 0.4178/0.9208

Train Epoch 12 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.2519 (avg: 0.2516) | Acc: 1.0000 (avg: 0.9954) | LR: 5.92e-04
  Batch  100/375 ( 26.7%) | Loss: 0.2662 (avg: 0.2535) | Acc: 0.9844 (avg: 0.9944) | LR: 5.56e-04
  Batch  150/375 ( 40.0%) | Loss: 0.2620 (avg: 0.2535) | Acc: 0.9922 (avg: 0.9943) | LR: 5.21e-04
  Batch  200/375 ( 53.3%) | Loss: 0.2546 (avg: 0.2545) | Acc: 0.9961 (avg: 0.9938) | LR: 4.86e-04
  Batch  250/375 ( 66.7%) | Loss: 0.2533 (avg: 0.2546) | Acc: 1.0000 (avg: 0.9940) | LR: 4.53e-04
  Batch  300/375 ( 80.0%) | Loss: 0.2532 (avg: 0.2553) | Acc: 0.9961 (avg: 0.9937) | LR: 4.20e-04
  Batch  350/375 ( 93.3%) | Loss: 0.2528 (avg: 0.2555) | Acc: 0.9961 (avg: 0.9936) | LR: 3.88e-04

Train Summary:
  Final Loss: 0.2558 | Final Acc: 0.9936
  Final LR: 3.73e-04
  Loss std: 0.0074 | Acc std: 0.0052

Val Epoch 12 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4160 | Final Acc: 0.9216
  Loss std: 0.0397 | Acc std: 0.0168
Epoch 12/15 | train 0.2558/0.9936 | val 0.4160/0.9216

Train Epoch 13 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.2448 (avg: 0.2450) | Acc: 1.0000 (avg: 0.9968) | LR: 3.42e-04
  Batch  100/375 ( 26.7%) | Loss: 0.2431 (avg: 0.2454) | Acc: 1.0000 (avg: 0.9968) | LR: 3.13e-04
  Batch  150/375 ( 40.0%) | Loss: 0.2545 (avg: 0.2460) | Acc: 0.9961 (avg: 0.9965) | LR: 2.85e-04
  Batch  200/375 ( 53.3%) | Loss: 0.2448 (avg: 0.2465) | Acc: 1.0000 (avg: 0.9965) | LR: 2.57e-04
  Batch  250/375 ( 66.7%) | Loss: 0.2515 (avg: 0.2467) | Acc: 0.9883 (avg: 0.9964) | LR: 2.31e-04
  Batch  300/375 ( 80.0%) | Loss: 0.2536 (avg: 0.2471) | Acc: 1.0000 (avg: 0.9963) | LR: 2.07e-04
  Batch  350/375 ( 93.3%) | Loss: 0.2448 (avg: 0.2473) | Acc: 1.0000 (avg: 0.9964) | LR: 1.83e-04

Train Summary:
  Final Loss: 0.2473 | Final Acc: 0.9964
  Final LR: 1.72e-04
  Loss std: 0.0057 | Acc std: 0.0037

Val Epoch 13 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4147 | Final Acc: 0.9225
  Loss std: 0.0403 | Acc std: 0.0174
Epoch 13/15 | train 0.2473/0.9964 | val 0.4147/0.9225

Train Epoch 14 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.2376 (avg: 0.2408) | Acc: 1.0000 (avg: 0.9984) | LR: 1.50e-04
  Batch  100/375 ( 26.7%) | Loss: 0.2441 (avg: 0.2407) | Acc: 1.0000 (avg: 0.9983) | LR: 1.30e-04
  Batch  150/375 ( 40.0%) | Loss: 0.2440 (avg: 0.2408) | Acc: 0.9922 (avg: 0.9982) | LR: 1.11e-04
  Batch  200/375 ( 53.3%) | Loss: 0.2449 (avg: 0.2413) | Acc: 1.0000 (avg: 0.9981) | LR: 9.36e-05
  Batch  250/375 ( 66.7%) | Loss: 0.2356 (avg: 0.2415) | Acc: 1.0000 (avg: 0.9980) | LR: 7.75e-05
  Batch  300/375 ( 80.0%) | Loss: 0.2415 (avg: 0.2417) | Acc: 1.0000 (avg: 0.9979) | LR: 6.29e-05
  Batch  350/375 ( 93.3%) | Loss: 0.2445 (avg: 0.2420) | Acc: 0.9961 (avg: 0.9977) | LR: 4.98e-05

Train Summary:
  Final Loss: 0.2422 | Final Acc: 0.9976
  Final LR: 4.40e-05
  Loss std: 0.0051 | Acc std: 0.0033

Val Epoch 14 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4141 | Final Acc: 0.9231
  Loss std: 0.0399 | Acc std: 0.0171
Epoch 14/15 | train 0.2422/0.9976 | val 0.4141/0.9231

Train Epoch 15 - 375 batches
--------------------------------------------------
  Batch   50/375 ( 13.3%) | Loss: 0.2457 (avg: 0.2403) | Acc: 1.0000 (avg: 0.9981) | LR: 3.29e-05
  Batch  100/375 ( 26.7%) | Loss: 0.2499 (avg: 0.2402) | Acc: 0.9922 (avg: 0.9980) | LR: 2.36e-05
  Batch  150/375 ( 40.0%) | Loss: 0.2422 (avg: 0.2401) | Acc: 0.9961 (avg: 0.9982) | LR: 1.58e-05
  Batch  200/375 ( 53.3%) | Loss: 0.2379 (avg: 0.2401) | Acc: 1.0000 (avg: 0.9982) | LR: 9.54e-06
  Batch  250/375 ( 66.7%) | Loss: 0.2368 (avg: 0.2402) | Acc: 1.0000 (avg: 0.9982) | LR: 4.85e-06
  Batch  300/375 ( 80.0%) | Loss: 0.2378 (avg: 0.2401) | Acc: 1.0000 (avg: 0.9982) | LR: 1.73e-06
  Batch  350/375 ( 93.3%) | Loss: 0.2360 (avg: 0.2401) | Acc: 1.0000 (avg: 0.9981) | LR: 1.90e-07

Train Summary:
  Final Loss: 0.2400 | Final Acc: 0.9982
  Final LR: 7.93e-09
  Loss std: 0.0043 | Acc std: 0.0027

Val Epoch 15 - 94 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4140 | Final Acc: 0.9230
  Loss std: 0.0399 | Acc std: 0.0175
Epoch 15/15 | train 0.2400/0.9982 | val 0.4140/0.9230

Evaluating final model on test set...

Val Epoch Test - 30 batches
--------------------------------------------------

Val Summary:
  Final Loss: 0.4874 | Final Acc: 0.8443
  Loss std: 0.0386 | Acc std: 0.0241
Final test accuracy: 0.8443
Final test loss: 0.4874
Optuna trials saved to: outputs/textcnn/optuna_trials.csv
wandb:
wandb: Run history:
wandb:               batch ‚ñÅ‚ñá‚ñÉ‚ñÇ‚ñÉ‚ñá‚ñà‚ñÅ‚ñÜ‚ñà‚ñÉ‚ñÖ‚ñà‚ñÇ‚ñÖ‚ñá‚ñà‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñà‚ñÇ‚ñÖ‚ñà‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÖ‚ñÇ‚ñÖ‚ñÜ‚ñÇ‚ñÖ‚ñÅ‚ñÖ‚ñà‚ñÇ‚ñà
wandb:          best_epoch ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà
wandb:        best_val_acc ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:    final_best_epoch ‚ñÅ
wandb:  final_best_val_acc ‚ñÅ
wandb: final_learning_rate ‚ñÇ‚ñÑ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ
wandb: final_test_accuracy ‚ñÅ
wandb:     final_test_loss ‚ñÅ
wandb:  final_val_accuracy ‚ñÅ
wandb:                 +18 ...
wandb:
wandb: Run summary:
wandb:               batch 350
wandb:          best_epoch 14
wandb:        best_val_acc 0.92312
wandb:               epoch Test
wandb:    final_best_epoch 14
wandb:  final_best_val_acc 0.92312
wandb: final_learning_rate 0.0
wandb: final_test_accuracy 0.84434
wandb:     final_test_loss 0.48738
wandb:  final_val_accuracy 0.92312
wandb:                 +19 ...
wandb:
wandb: üöÄ View run final_model at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna/runs/wfikql5t
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/devmarkpro-org/ag-news-textcnn-optuna
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20251006_075924-wfikql5t/logs
